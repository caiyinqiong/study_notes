>  å†™åœ¨å‰é¢çš„è¯ï¼š
> 
>  1ï¼‰æœ¬æ–‡çš„å†…å®¹ä¸»è¦æ‘˜è‡ªå¹³æ—¶çœ‹çš„è®ºæ–‡å¹¶æ•´ç†å¾—åˆ°ï¼Œå¦‚è¦å¼•ç”¨ï¼Œè¯·ä¸€å®šæŸ¥æ˜åŸæ–‡æ¥æºã€‚
> 
>  2ï¼‰æ•´ç†æœ¬æ–‡çš„ç›®çš„ä¸»è¦æ˜¯æƒ³åœ¨åšå®éªŒéœ€è¦æ‰¾æ•°æ®é›†æ—¶èƒ½å¤Ÿå¤§ä½“ä¸Šå¿«é€Ÿå®šä½åˆ°ä¸ªåˆ«æ•°æ®é›†ï¼Œå†åšè¿›ä¸€æ­¥æ·±å…¥è°ƒç ”ã€‚ï¼ˆå¸Œæœ›å¯¹å¤§å®¶æœ‰å¸®åŠ©ğŸ˜ï¼‰
> 
>  3ï¼‰æ‰€æœ‰å†…å®¹å¹¶æ²¡æœ‰ç»è¿‡ä¸€ä¸€æŸ¥è¯ï¼Œå¦‚æœ‰é”™è¯¯å’Œè¡¥å……ä¹‹å¤„ï¼Œå¯éšæ—¶æå‡ºï¼Œæˆ‘çº æ­£æ•´åˆåˆ°åŸè´´ã€‚



###QA

<img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191115165547284.png" alt="image-20191115165547284" style="zoom:50%;" />

<img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191107010348581.png" alt="image-20191107010348581" style="zoom:50%;" />

- **WikiQA**ï¼šYi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: a challenge dataset for open-domain question answering. In EMNLP.

  > ä¸‹è½½åœ°å€ï¼šhttps://www.microsoft.com/en-us/download/details.aspx?id=52419
  >
  > æ•°æ®é›†ç®€ä»‹ï¼šOpen-domian factoid QA dataset in which all answers were collected from the Wikipedia. 0/1æ ‡ç­¾ã€‚ 
  >
  
- **TRECQA**ï¼šMengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for QA. In EMNLP-CoNLL

  > ä¸‹è½½åœ°å€ï¼šhttps://github.com/aseveryn/deep-qa
  >
  > æ•°æ®é›†ç®€ä»‹ï¼šQA track (8-13) data of Text REtrieval Conference. Factoid question answering. å€™é€‰ç­”æ¡ˆæ˜¯å•ä¸ªå¥å­ã€‚ 0/1æ ‡ç­¾ã€‚ 
  >
  
- **Yahoo! Answers**

  > ä¸‹è½½åœ°å€ï¼šhttps://webscope.sandbox.yahoo.com/catalog.php?datatype=l
  >
  > æ•°æ®é›†ç®€ä»‹ï¼šæœ‰ä¸ªå¤§ä¸€ç‚¹çš„å’Œå°ä¸€ç‚¹çš„æ•°æ®é›†ï¼Œåˆ†åˆ«åŒ…æ‹¬44ä¸‡+å’Œ14ä¸‡+ä»Yahoo!Answerç½‘ç«™æ”¶é›†åˆ°çš„questionï¼Œæ¯ä¸ªquestionå¯¹åº”ä¸€ä¸ªæœ€ä½³ç­”æ¡ˆå’Œå‡ ä¸ªéæœ€ä¼˜çš„ç­”æ¡ˆã€‚ï¼ˆå°ä¸€ç‚¹çš„æ•°æ®é›†æ˜¯å¤§æ•°æ®é›†çš„å­é›†ï¼Œä¸»è¦æ˜¯æ ¹æ®questionä»¥â€˜Howâ€™å¼€å¤´è¿‡æ»¤å¾—åˆ°çš„ï¼‰

- **FiQAï¼š**

  > ä¸‹è½½åœ°å€ï¼šhttps://sites.google.com/view/fiqa/home
  >
  > æ•°æ®é›†ç®€ä»‹ï¼šnon-factoid QA dataset from the financial domain which has been recently released for WWW 2018 Challenges. The dataset is built by crawling Stackexchange, Reddit and StockTwits in which part of the questions are opinionated, targeting mined opinions and their respective entities, aspects, sentiment polarity and opinion holder.
  
- **InsuranceQA**ï¼šFeng, Minwei, Xiang, Bing, Glass, Michael, Wang, Lidan, and Zhou, Bowen. Applying deep learning to answer selection: A study and an open task. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2015.

  > æ•°æ®é›†ç®€ä»‹ï¼šNon-factoid QA dataset from the insurance domain. Question may have multiple correct answers and normally the questions are much shorter than the answers. The average length of questions and answers in tokens are 7 and 95, respectively. For each question in the development and test sets, there is a set of 500 candidate answers.



### MRC

- **SQuAD**ï¼šï¼ˆå•æ®µè½ä¸Šä¸‹æ–‡ã€æŠ½å–å¼QAï¼‰

  > 1.1ç‰ˆæœ¬æ•°æ®é›†ç®€ä»‹ï¼šcontains over 100, 000 passage-question pairsã€‚
  >
  > 2.0ç‰ˆæœ¬æ•°æ®é›†ç®€ä»‹ï¼šcontains about 100, 000 answerable questions and over 50, 000 crowdsourced unanswerable questions towards Wikipedia paragraphs. Moreover, for each unanswerable question, a plausible answer span is annotated.
  >
  > å¯¹æŠ—æ•°æ®é›†ç®€ä»‹ï¼šeach passage in **AddSent** contains several sentences that are similar to the question but not contradictory to the answer, while each passage in **AddOneSent** contains a human-approved random sentence that may be unrelated to the passage.

- **NewsQA**ï¼šAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A Machine Comprehension Dataset. ï¼ˆå•æ®µè½ã€æŠ½å–å¼QAï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šThe NewsQA dataset contains 100k answerable questions from a total of 120k questions.
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://datasets.maluuba.com/NewsQA/

- **HotpotQA**ï¼šZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering.ï¼ˆå¤šæ®µè½ã€supporting sentence å’Œ answer span çš„è”åˆå­¦ä¹ ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼š112,779 questions are collected by crowdsourcing based on the ï¬rst paragraphs in Wikipedia documents, 84% of which require multi-hop reasoning. The data are split into a training set (90,564 questions), a development set (7,405 questions) and a test set (7,405 questions). All questions in development and test sets are hard multi-hop cases.
  >
  > In the training set, for each question, an answer and paragraphs of 2 gold (useful) entities are provided, with multiple supporting facts, sentences containing key information for reasoning, marked out. There are also 8 unhelpful negative paragraphs for training. 
  >
  > During evaluation, only questions are offered and meanwhile supporting facts are required besides the answer.
  >
  > - distractor setting: a question-answering system reads 10 paragraphs to provide an answer to the question.
  > - fullwiki setting: a question-answering system must find the answer to a question in the scope of the entire Wikipedia. 
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://hotpotqa.github.io/

- **WikiHop**ï¼šJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics ï¼ˆå¤šæ®µè½ã€æä¾›å€™é€‰ç­”æ¡ˆé›†åˆçš„MRCï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šThe query of WikiHop is constructed with entities and relations from WikiData, while supporting documents are from WIKIREADING. A bipartite graph connecting entities and documents is ï¬rst built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, WIKIHOP is a multi-choice style reading comprehension data setã€‚ã€‚ã€‚ã€‚ã€‚There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set.ã€‚ã€‚ã€‚ã€‚The task is to predict the correct answer given a query and multiple supporting documents.ã€‚ã€‚ã€‚ã€‚Each question is associated with an average of 13.7 supporting Wikipedia passages, each with 36.4 tokens on average.

- **ReCoRD**ï¼šSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension.ï¼ˆå¤šæ®µè½ã€æŠ½å–å¼QAã€éœ€è¦å¸¸è¯†æ¨ç†ã€é’ˆå¯¹å®ä½“ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šReading Comprehension with Commonsense Reasoning Dataset is a large-scale MRC dataset requiring commonsense reasoning. It consists of passage-question-answer tuples, collected from CNN and Daily Mail news articles. In each tuple, the passage is formed by the ï¬rst few paragraphs of a news article, with named entities recognized and marked. The question is a sentence from the rest of the article, with a missing entity speciï¬ed as the golden answer. The goal is to ï¬nd the golden answer among the entities marked in the passage, which can be deemed as an extractive MRC task.
  
- **MARCO Q&A + Natural Language Generation**ï¼šï¼ˆå¤šæ®µè½ã€ç”Ÿæˆå¼QAï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šMS-MARCO dataset deï¬nes the task as answering a question from multiple passages and the words in the answer are not necessary in the passages. There are several related passages for each question in the MS-MARCO dataset. In addition to annotating the answer, MS-MARCO also annotates which passage is correct.
  >
  > The questions are user queries issued to the Bing search engine and the contextual passages are from real web documents. The data has been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers).
  >
  
- **NarrativeQA**ï¼šï¼ˆå•è·³ã€ç”Ÿæˆå¼çš„QAï¼‰

<img src="../images/image-20200111213236074.png" alt="image-20200111213236074" style="zoom:50%;" />



### Conversational MRC

- **CoQA**ï¼šSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. TACL, 2019. ï¼ˆå•æ®µè½ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šIt consists of 127k questions with answers collected from 8k conversations over text passages. It covers seven diverse domains (ï¬ve of them are in-domain and two are out-of-domain). The out-of-domain passages only appear in the test set. The average number of question answering turns is more than 15 per passage.  CoQA contains text passages from diverse domains, conversational questions and answers developed for each passage, as well as rationales (i.e., text spans extracted from given passages) to support answers.
  >
  > <img src="../images/image-20191217190530921.png" alt="image-20191217190530921" style="zoom:50%;" />
  >
  > ![image-20191219161354572](../images/image-20191219161354572.png)

- **QuAC**ï¼šEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC : Question answering in context. In EMNLP, 2018.

- **QBLink**ï¼šAhmed Elgohary, Chen Zhao, and Jordan Boyd-Graber. A dataset and baselines for sequential open-domain question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 1077â€“1083, 2018. ï¼ˆæ‰¾ä¸åˆ°åœ¨å“ªå„¿çœ‹çš„äº† ğŸ¤¦â€â™‚ï¸ï¼‰



### Open-domain QA

<img src="../images/image-20191205191716536.png" alt="image-20191205191716536" style="zoom:50%;" />

<img src="../images/image-20200525112410881.png" alt="image-20200525112410881" style="zoom:50%;" />

<img src="../images/image-20200603214631635.png" alt="image-20200603214631635" style="zoom:50%;" />

- **Quasar-T**ï¼šBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. QUASAR: Datasets for question answering by search and reading. arXiv preprint arXiv:1707.03904, 2017b.

  > æ•°æ®é›†ç®€ä»‹ï¼šThe data set makes use of the â€œLucene indexâ€ on the ClueWeb09 corpus. For each question, 100 unique sentence-level passages were collected. These supporting passages are given in an order rankedï¼ˆå¸¦æœ‰floatç›¸å…³åº¦ï¼‰ by a search engine. Supporting passageçš„å¹³å‡é•¿åº¦ä¸º24ã€‚ç­”æ¡ˆæ˜¯ä¸€ä¸ªanswer spanï¼Œä¸”å¯èƒ½å‡ºç°åœ¨å¤šä¸ªsupporting passageä¸­ï¼ˆ#p(truth)=14.8ï¼‰ã€‚ã€‚æ¯ä¸ªsupporting passageé™¤äº†æä¾›äº†å†…å®¹ï¼Œè¿˜æä¾›äº†floatç±»å‹çš„ç›¸å…³æ€§åˆ†å€¼ã€‚
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://github.com/bdhingra/quasar

- **SearchQA**ï¼šMatthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.

  > æ•°æ®é›†ç®€ä»‹ï¼šA dataset of complex questions extracted from Jeopardy! and uses Google to collect about 50 web page snippets as passages for each question. Each snippet is 37.3Â±11.7 tokens long on average.
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://github.com/nyu-dl/SearchQA
  
- **TriviaQA**ï¼šMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2017.

  > æ•°æ®é›†ç®€ä»‹ï¼šcollected trivia questions coming from 14 trivia and quiz-league websites, and makes use of the Bing Web search API to collect the top 50 webpages most related to the questions.
  >
  > å…ˆæ”¶é›†é—®ç­”å¯¹ï¼Œå†æ”¶é›†è¯æ®æ–‡æœ¬ã€‚Gathered 95k question-answer pairs from trivia and quiz-league websites and collected textual evidence which contained the answer from either Web search results or Wikipedia pages corresponding to the entities which are mentioned in the question. Totally collected 650k (passage, question, answer) triples in total. The passages used in this dataset are mostly long documents, the average document length is 2,895 words. ã€‚ã€‚ã€‚ä½†è¿™ç§åŠè‡ªåŠ¨çš„æ•°æ®é›†æ„é€ æ–¹å¼ä¹Ÿå­˜åœ¨ç¼ºé™·ï¼Œthere is no guarantee that the passage really provides the answer to the question and this inï¬‚uences the quality of the training data.
  >
  > æ•°æ®é›†ç‰ˆæœ¬ï¼š1ï¼‰TriviaQA_wikiï¼šæ¯ä¸ªé—®ç­”å¯¹å¯¹åº”ä¸€ä¸ªå¤šä¸ªWikipedia articleï¼Œç›¸å½“äºä¸æ˜¯å¼€æ”¾åŸŸçš„ã€‚
  >
  > 1ï¼‰TriviaQA_unfilterï¼šå¼€æ”¾åŸŸçš„ï¼Œä½¿ç”¨ Bing Web search API to collect the top 50 webpages
  >
  > ä¸‹è½½åœ°å€ï¼šhttp://nlp.cs.washington.edu/triviaqa/
  >
  > http://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz

- **SQuAD-open**

- **SQuAD-document**

- **WebQuestions**ï¼šJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.

  > æ•°æ®é›†ç®€ä»‹ï¼šThis dataset is built to answer questions from the Freebase KB. It was created by crawling questions through the GOOGLE SUGGEST API, and then obtaining answers using Amazon Mechanical Turkã€‚ å«3Kå·¦å³ä¸ªtrain questionï¼Œå’Œ2Kå·¦å³ä¸ªtest questionã€‚åªå«é—®ç­”å¯¹ï¼Œä¸å«passageã€‚
  >
  > https://github.com/brmson/dataset-factoid-webquestions

- **Natural Questions**ï¼šTom Kwiatkowski, Jennimaria Palomaki, Olivia Redï¬eld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research.

  > æ•°æ®é›†ç®€ä»‹ï¼ša dataset of questions from web queries, each of which is accompanied by a Wikipedia article that contains the answer.

- **CuratedTREC**ï¼šPetr BaudiË‡s and Jan Sediv`y. 2015. Modeling of the question answering task in the YodaQA system.

  > æ•°æ®é›†ç®€ä»‹ï¼šContains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002. All the answers are written in regular expressions.  åªå«é—®ç­”å¯¹ï¼Œä¸å«passageã€‚
  >
  > ä¸‹è½½ï¼šhttps://github.com/brmson/dataset-factoid-curated.
  >
  > https://github.com/brmson/dataset-factoid-curated/tree/master/trec

- **WikiMovies**ï¼šAlexander Miller, Adam Fisch, Jesse Dodge, AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.

  > æ•°æ®é›†ç®€ä»‹ï¼šåªå«é—®ç­”å¯¹ï¼Œä¸å«å¯¹åº”çš„æ­£ä¾‹passageã€‚ ä¸€å…±ï¼ˆFullç‰ˆæœ¬ï¼‰consists of 200K questions about movies, along with 18K Wikipedia articles for extracting the answers. æ¯ä¸ªWikipedia articleså…³äºä¸€ä¸ªç”µå½±ï¼Œå¯èƒ½ä¼šæ¯”è¾ƒé•¿ã€‚The dataset includes a list of entities: movie titles, actor names, genres etc. Answers to all the questions are in the entity list.
  >
  > Wikipediaç‰ˆæœ¬å»æ‰äº†ä¸€äº›éš¾ä»¥å•ç‹¬ç”¨Wikipediaæ–‡ç« å›ç­”çš„é—®é¢˜ï¼Œcontains 96k question-answer pairs in the domain of moviesã€‚
  >
  > <img src="../images/image-20200525113612324.png" alt="image-20200525113612324" style="zoom:50%;" />
  >
  > ä¸‹è½½ï¼šhttp://fb.ai/babi

- **HotpotQA**ï¼šZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering.ï¼ˆå¤šæ®µè½ã€supporting sentence å’Œ answer span çš„è”åˆå­¦ä¹ ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼š112,779 questions are collected by crowdsourcing based on the ï¬rst paragraphs in Wikipedia documents, 84% of which require multi-hop reasoning. The data are split into a training set (90,564 questions), a development set (7,405 questions) and a test set (7,405 questions). All questions in development and test sets are hard multi-hop cases.
  >
  > In the training set, for each question, an answer and paragraphs of 2 gold (useful) entities are provided, with multiple supporting facts, sentences containing key information for reasoning, marked out. There are also 8 unhelpful negative paragraphs for training. 
  >
  > During evaluation, only questions are offered and meanwhile supporting facts are required besides the answer.
  >
  > - distractor setting: a question-answering system reads 10 paragraphs to provide an answer to the question.
  > - fullwiki setting: a question-answering system must find the answer to a question in the scope of the entire Wikipedia. 
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://hotpotqa.github.io/



### Dialogue

<img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191115104207757.png" alt="image-20191115104207757" style="zoom:50%;" />

<img src="../images/image-20200226113223805.png" alt="image-20200226113223805" style="zoom:50%;" />

- **UDC**ï¼ˆUbuntu Dialog Corpusï¼‰ï¼šR. Lowe, N. Pow, I. Serban, and J. Pineau. 2015. The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. CoRR abs/1506.08909 (2015).  ï¼ˆåŸºäºæ£€ç´¢çš„ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šmulti-turn technical support conversation data collected from the chat logs of the Freenode Internet Relay Chat (IRC) network. It consists of 1 million context-response pairs for training, 0.5 million pairs for validation and 0.5 million pairs for testing. In all the three sets, positive responses are human responses, while negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in the training set, and 1:9 in both the validation set and the test set.
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu%20data.zip?dl=0 ï¼ˆä¸€ä¸ªå¤„ç†åçš„ç‰ˆæœ¬ï¼Œnumbers, urls and paths are replaced by special placeholdersï¼‰
  
- **Douban**ï¼šYu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhoujun Li. 2017. Sequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.  ï¼ˆåŸºäºæ£€ç´¢çš„ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šconsists of multiturn Chinese conversations collected from Douban group. There are 1 million context-response pairs for training, 50 thousand pairs for validation, and 6, 670 pairs for testing. In the training set and the validation set, the last turn of each conversation is taken as a positive response and a negative response is randomly sampled. For each context in the test set, 10 response candidates are retrieved from an index and their appropriateness regarding to the context is annotated by human labelers.
  
- **E-commerce Dialogue Corpus**ï¼šZhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao, and Gongshen Liu. 2018b. Modeling multiturn conversation with deep utterance aggregation. In Proceedings of the 27th International Conference on Computational Linguistics,  ï¼ˆåŸºäºæ£€ç´¢çš„ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šThe data consists of multi-turn realworld conversations between customers and customer service staff in Taobao. It contains 1 million context-response pairs for training, and 10 thousand pairs for validation and test. Positive responses in this data are real human responses, and negative candidates are automatically constructed by ranking the response corpus based on conversation history augmented messages using Apache Lucene. The ratio of the positive and the negative is 1:1 in training and validation, and 1:9 in test.

- **JDC**ï¼šï¼ˆäº¬ä¸œå‘å¸ƒçš„æ¯”èµ›æ•°æ®é›†ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šconsists of 515,686 conversational context-response pairs published by the JD contest.
  >
  > ä¸‹è½½åœ°å€ï¼šhttps://www.jddc.jd.com

<img src="../images/image-20200114113705629.png" alt="image-20200114113705629" style="zoom:50%;" />

- **Neurips ConvAI2**ï¼šSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.

  > æ•°æ®é›†ç®€ä»‹ï¼šbased on the Persona-Chat dataset. For each response, the model has to pick the correct annotated utterance from a set of 20 choices, where the remaining 19 were other randomly chosen utterances from the evaluation set. Note that in a ï¬nal system however, one would retrieve from the entire training set of over 100k utterances.

- **DSTC7ï¼ŒTrack1**ï¼š

  > æ•°æ®é›†ç®€ä»‹ï¼šconsists of conversations extracted from Ubuntu chat logs.

- **MSDialog**

  >æ•°æ®é›†ç®€ä»‹ï¼šMSDialog dataset is a labeled dialog dataset of question answering (QA) interactions between information seekers and providers from an online forum on Microsoft products. 
  >
  >è¯¥æ•°æ®é›†åŒ…å«ä¸€ä¸ªæ— æ ‡ç­¾çš„æ€»é›†åˆ  å’Œ æœ‰ç”¨æˆ·æ„å›¾æ ‡ç­¾ï¼ˆ12ç§æ„å›¾ï¼‰çš„å­é›†åˆã€‚
  >
  >æ— æ ‡ç­¾æ€»é›†åˆçš„ç»Ÿè®¡ä¿¡æ¯ï¼š
  >
  ><img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191121163454524.png" alt="image-20191121163454524" style="zoom:50%;" />
  >
  >![image-20191121162806729](/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191121162806729.png)
  >
  >The dataset contains more than 2,199 multi-turn QA dialogs with 10,020 utterances that are annotated with user intent on the utterance level. å¹³å‡æ¯ä¸ªè¡¨è¾¾æœ‰1.83ä¸ªæ ‡ç­¾ã€‚
  >
  ><img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191121162950923.png" alt="image-20191121162950923" style="zoom:25%;" />
  >
  ><img src="/Users/caiyinqiong/Library/Application Support/typora-user-images/image-20191121162708793.png" alt="image-20191121162708793" style="zoom:50%;" />
  >
  >ä¸‹è½½åœ°å€ï¼šhttps://ciir.cs.umass.edu/downloads/msdialog
  >
  
- **bAbI dialog**ï¼šAntoine Bordes and Jason Weston. 2016.Learning end-to-end goal-oriented dialog. CoRR.

- **Persona Chat**ï¼šSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too?

  > æ•°æ®é›†ç®€ä»‹ï¼šan open domain dataset with multi-turn chit-chat conversations between turkers who are each assigned a â€œpersonaâ€ at random. It comprises of 10.9k dialogs with an average of 14.8 turns per dialog.

- **Dailydialog**ï¼šYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset.

  > æ•°æ®é›†ç®€ä»‹ï¼šan open domain dataset which consists of dialogs that resemble day-to-day conversations across multiple topics. It comprises of 13k dialogs with an average of 7.9 turns per dialog.

- **MutualFriends**ï¼šH. He, A. Balakrishnan, M. Eric, and P. Liang. 2017. Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.

  > æ•°æ®é›†ç®€ä»‹ï¼ša multi-turn goal-oriented dataset where two agents must discover which friend of theirs is mutual based on the friendsâ€™ attributes. It contains 11k dialogs with an average of 11.41 utterances per dialog.
  
- **MultiWOZ**ï¼šPaweÅ‚ Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, IËœnigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica GaË‡siÂ´c. 2018. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. ï¼ˆåŸºäºæ£€ç´¢çš„ï¼‰

  > æ•°æ®é›†ç®€ä»‹ï¼šMultiWOZ contains task-oriented conversations between a tourist and a Wizard-of-Oz.   MultiWOZ contains 8422 dialogs for training, 1000 for validation and 1000 for testing. There are 20 candidate responses for each dialog context.



### IR

<img src="../images/image-20191111162751237-2207984.png" alt="image-20191111162751237" style="zoom:50%;" />

- **Robust**

- **ClueWeb**

- **AOL search log**ï¼šGreg Pass, Abdur Chowdhury, and Cayley Torgeson. A picture of search. In InfoScale, volume 152, pp. 1, 2006.

  > æ•°æ®é›†ç®€ä»‹ï¼šThe queries in this dataset were sampled between 1 March, 2006 and 31 May, 2006. In total there are 16,946,938 queries submitted by 657,426 unique users.
  >
  > åˆ«äººå¯¹è¯¥æ•°æ®é›†çš„å¤„ç†æ–¹æ³•ï¼š
  >
  > We removed all non-alphanumeric characters from the queries, applied word segmentation and lowercasing. We deï¬ne a session by a 30-minute window of inactive time, and ï¬ltered sessions by their lengths (minimum 2, maximum 10). We only kept the most frequent |V | = 100k words and mapped all other words to an <unk> token when constructing the vocabulary. We randomly selected 1,032,459 sessions for training, 129,053 sessions for development and 91,108 sessions for testing, with no overlapping.

- **Wikipedia Article Search**ï¼šLedell Yu Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston. Starspace: Embed all the things! In Thirty-Second AAAI Conference on Artiï¬cial Intelligence, 2018.

  > æ•°æ®é›†ç®€ä»‹ï¼šUsing the 2016-12-21 dump of English Wikipedia (âˆ¼5M articles), the task is given a sentence from an article as a search query, ï¬nd the article it came from. Evaluation ranks the true article (minus the sentence) against 10,000 other articles using retrieval metrics.
  >
  > <img src="file:///Users/caiyinqiong/Desktop/notes/images/image-20200114114310548.png?lastModify=1582207867" alt="image-20200114114310548" style="zoom:50%;" />



### è¯­ä¹‰åŒ¹é…

- **SICKï¼ˆSentences Involving Compositional Knowledgeï¼‰**

  > æ•°æ®é›†ç®€ä»‹ï¼šthe similarity degree is denoted by a real number in the range of [1, 5]. SICK comprises 9,927 sentence pairs with 4,500/500/4,927 instances for training/dev/test sets.
  >
  > ä¸‹è½½åœ°å€ï¼šhttp://clic.cimec.unitn.it/composes/sick.html

- **STS-B**

  > æ•°æ®é›†ç®€ä»‹ï¼šæ–°é—»æ ‡é¢˜ã€è§†é¢‘åŠå›¾ç‰‡æ ‡é¢˜ç­‰å¥å­å¯¹çš„ç›¸ä¼¼åº¦åˆ¤æ–­ï¼ˆ äº”åˆ†ç±»ï¼‰
  >
  > GLUEé‡Œé¢çš„ã€‚



### NLI

- **SciTail**ï¼šTushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTail: A textual entailment dataset from science question answering. In AAAI. 

  > æ•°æ®é›†ç®€ä»‹ï¼šThe task involves assessing whether a given premise entails a given hypothesis. The hypotheses in SciTail are created from science questions while the corresponding answer candidates and premises come from relevant web sentences retrieved from a large corpus.

- **QNLI**

  > æ•°æ®é›†ç®€ä»‹ï¼šæ˜¯ç”±SQuADæ•°æ®å¤„ç†å¾—åˆ°çš„ï¼Œå¯¹äºquestion-passage pairï¼Œå°†å…¶æ‹†åˆ†æˆå¤šä¸ªquestion-sentence pairï¼Œåˆ¤æ–­sentenceä¸­æ˜¯å¦å«æœ‰ç­”æ¡ˆï¼ˆäºŒåˆ†ç±»ï¼‰ã€‚
  >
  > GLUEé‡Œé¢çš„ã€‚

- **RTE**

  > æ•°æ®é›†ç®€ä»‹ï¼šç”±RTE1ã€RTE2ã€RTE3ã€RTE5ç»„æˆï¼ˆäºŒåˆ†ç±»ï¼‰
  >
  > GLUEé‡Œé¢çš„ã€‚

- **SNLIã€MNLI**



### Paraphrase Identification

- **MSRP**ï¼ˆMicrosoft Research Paraphrase corpusï¼‰ï¼šBill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In COLING.

  > æ•°æ®é›†ç®€ä»‹ï¼šconstructed from a large corpus of temporally and topically clustered news articles. 
  >
  > The training set contains 2753 true and 1323 false paraphrase pairs; the test set contains 1147 and 578 pairsï¼Œno development set is provided. 
  >
  > 0/1æ ‡ç­¾

- **Wiki Answer**ï¼ˆé—®é¢˜å¤è¿°æ•°æ®é›†ï¼‰ï¼šAnthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.

  > æ•°æ®é›†ç®€ä»‹ï¼šWikiAnswers æ˜¯ä¸€ä¸ªè‹±è¯­è‡ªç„¶è¯­è¨€é—®é¢˜æ•°æ®é›†ï¼Œä»WikiAnswers ç½‘ç«™ä¸ŠæŠ“å–çš„ç›¸åŒé—®é¢˜çš„ä¸åŒè¡¨è¿°å½¢å¼ï¼ŒåŒ…æ‹¬ 250 ä¸‡ä¸ªä¸åŒé—®é¢˜å’Œ 1800 ä¸‡ä¸ªé—®é¢˜å¤è¿°å¯¹ã€‚ã€‚ã€‚æ¯ä¸ªé—®é¢˜åŒ…æ‹¬å¤šä¸ªå¤è¿°é—®é¢˜ã€‚
  >
  > ä¸‹è½½åœ°å€ï¼šhttp://dataju.cn/Dataju/publishing/accessContentPublishingDescription/1135

- **PARAGRAM**ï¼šä¸€ä¸ªé’ˆå¯¹PIä»»åŠ¡é¢„è®­ç»ƒçš„è¯å‘é‡

  > ä¸‹è½½åœ°å€ï¼šhttp://ttic.uchicago.edu/wieting/
  >
  > æ•°æ®é›†ç®€ä»‹ï¼š25-dimensional word vectors that are developed for paraphrase tasks.

- **QQP**



### Knowledge base

- **subset of Freebase (FB5M 2 )**

  > ä¸‹è½½åœ°å€ï¼šhttps://research.facebook.com/researchers/1543934539189348
  >
  > æ•°æ®é›†ç®€ä»‹ï¼šincludes 4,904,397 entities, 7,523 relations, and 22,441,880 facts.

- **ConceptNet**ï¼šRobert Speer, Joshua Chin, and Catherine Havasi. 2016. Conceptnet 5.5: An open multilingual graph of general knowledge. In AAAI Conference on Artiï¬cial Intelligence.

  > It is a semantic network representing words and phrases as well as the commonsense relationships between them.



### æƒ…æ„Ÿåˆ†ç±»

- **MR**ï¼šPang, Bo and Lee, Lillian. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.

  > ä¸‹è½½åœ°å€ï¼šhttps://www.cs.cornell.edu/people/pabo/movie-review-data/
  >
  > æ­£ã€è´Ÿæƒ…ç»ªã€‚ç”µå½±è¯„è®º
  >
  
- **SSTï¼ˆStanford Sentiment Treebankï¼‰**ï¼šSocher, Richard, Perelygin, Alex, Wu, Jean Y., Chuang, Jason, Manning, Christopher D., Ng, Andrew Y., and Potts, Christopher. Recursive deep models for semantic compositionality over a sentiment treebank.

  > SST-1ï¼šan extension of MR but with train/dev/test splits provided and ï¬ne-grained labels (very positive, positive, neutral, negative, very negative), re-labeled by Socher et al. It has 11855 sentences taken from the movie review site Rotten Tomatoes. 8544 sentences for training, 2210 sentences for test and 1101 sentences for validation. æ¯ä¸ªæ ·æœ¬æ˜¯å•ä¸ªå¥å­ã€‚
  >
  > ä¸‹è½½åœ°å€ï¼šhttp://nlp.stanford.edu/sentiment/ 
  >
  > SST-2ï¼šSame as SST-1 but with neutral reviews removed and binary labels.

- **IMDB**ï¼šMaas, Andrew L., Daly, Raymond E., Pham, Peter T., Huang, Dan, Ng, Andrew Y., and Potts, Christopher. Learning word vectors for sentiment analysis.

  > æ•°æ®é›†ç®€ä»‹ï¼šThe dataset consists of 100,000 movie reviewsï¼ˆ25,000 labeled training instances, 25,000 labeled test instances and 50,000 unlabeled training instancesï¼‰taken from IMDBã€‚ã€‚æ¯ä¸ªæ ·æœ¬åŒ…å«å¤šä¸ªå¥å­ã€‚æ­£ã€è´Ÿä¸¤ç±»æ ‡ç­¾ã€‚
  >
  > ä¸‹è½½åœ°å€ï¼šhttp://ai.Stanford.edu/amaas/data/sentiment/index.html



### æ‘˜è¦

æ‘˜è¦çš„æ•°æ®é›†ï¼šhttps://github.com/mathsyouth/awesome-text-summarization

<img src="../images/image-20191219145644180.png" alt="image-20191219145644180" style="zoom:33%;" />



###å…¶ä»–

**Stanford Parser**: http://nlp.stanford.edu/software/lex-parser.shtml

**stop word** list: http://snowball.tartarus.org/algorithms/english/stop.txt




