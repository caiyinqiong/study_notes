其他

- [x] Rethinking Query Expansion for BERT Reranking（ECIR2020）

  > BERT reranker更适合于自然语言句子的长query，所以传统模型的问题扩展方法可能并不太适用于BERT-based reranker。本文探究了新的问题扩展方法，发现结合扩展【结构词】和【概念词】的方法最有效。

- [x] 【Transformer-kernel】Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking（ECAI2020）

  > 网络结构：对query和document分别采用三层Transformer，对contextual word embedding结果计算交互矩阵，再进行kernel-pooling。
  > 卖点：1）相比BERT来说效率更高，相比非BERT的模型性能更好，所以达到effective和efficiency的平衡。2）采用交互矩阵和kernel-pooling，具有更好的解释性，因为可以看到document每个term的得分。
  > 实验数据集：MARCO-passge、MARCO-document、TREC-CAR2017
  > 亮点：评估效率的方式，比较了在相同的时间限制下，每个模型的MRR、NDCG、Recall性能。而不是像其他工作中仅比较处理每个document/query所需的时间。
  
- [x] 【WMD：word mover‘s distance】From Word Embeddings To Document Distances（2015，ICML）

  > 定义一种Word mover distance，由word embedding之间的距离得到docuemnt之间的距离。主要用于文本分类任务，原论文中不涉及检索任务。
  
- [x] Reqa: an evaluation for end-to-end answer retrieval models（2019）

  > 本文把SQUAD和NQ数据集改编成ReQA任务，即introduce Retrieval QuestionAnswering (ReQA) as a new benchmark for evaluating end-to-end answer retrieval models.
  >
  > ReQA任务：The task assesses how well models are able to retrieve relevant sentence-level answers to queries from a large corpus.

- [x] PARADE: Passage representation aggregation for document reranking

  > 提出了三种由passage-level representation得到document-level representation，进而得到document打分的方法：PARADE_Max、PARADE_Attn、PARADE_Transformer。
  
- [ ] Learning a Beer Negative Sampling Policy with Deep Neural Networks for Search（ICTIR2019）

  > we establish that using reinforcement learning to optimize a policy over a set of sampling functions.
  >









# （完成）SIGIR2018（共录取86篇，录取率21%）

- [x] （IR匹配）Modeling Diverse Relevance Patterns in Ad-hoc Retrieval（范意兴师兄）

- [x] ！！（排序）Learning a Deep Listwise Context Model for Ranking Refinement

- [x] ！！（点击模型）A Click Sequence Model for Web Search

- [x] ！！（多跳问答匹配）Multihop Attention Networks for Question Answer Matching

- [x] ！！（对话系统的response排序）Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems

- [x] ！！（句子相似度建模、对抗学习）CAN: Enhancing Sentence Similarity Modeling with Collaborative and Adversarial Network

- [x] ！！（CQA的相似问题识别）Related or Duplicate: Distinguishing Similar CQA Questions via Convolutional Neural Networks

- [x] ！！（QA排序、知识库）Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs

- [x] ！！（融合知识库）Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling

- [x] （TREC-CAR任务，对question path进行分段处理）Characterizing Question Facets for Complex Answer Retrieval

- [x] （无监督的QA任务的baseline）Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering

- [x] ！！（IR）Multi-level Abstraction Convolutional Model with Weak Supervision for Information Retrieval

- [ ] ！！（点击相关性数据集）**Sogou-QCL: A New Dataset with Click Relevance Label**

- [ ] ！！（QA数据集）**WikiPassageQA: A Benchmark Collection for Research on Non-factoid Answer Passage Retrieval**

- [x] ！！（对话和用户意图分析的数据集）Analyzing and Characterizing User Intent in Information-seeking Conversations

  

- [ ] ！！（文本生成平台）Texygen: A Benchmarking Platform for Text Generation Models

- [ ] XXX（IR）Modeling multidimensional user relevance in IR using vector spaces

- [ ]  XXX（IR）Are we on the Right Track? Integrating Theoretical and Empirical Methodologies for Information Retrieval

- [ ]  XXX（概率信息检索模型）A New Term Frequency Normalization Model for Probabilistic Information Retrieval

- [ ] XXX（L2R）Universal Approximation Functions for Fast Learning to Rank

- [ ] XXX (搜索的点击模型) Constructing Click Models for Mobile Search

- [ ] XXX（电商搜索中的问题分类）A taxonomy of queries for e-commerce search

- [ ] XXX（问答）Characterizing and Supporting Question Answering in Human-to-Human Communication

- [ ] XXX (搜索结果的多样性) From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks （徐君老师）

- [ ] Predicting User Knowledge Gain in Informational Search Sessions

- [ ] （排序的鲁棒性）Ranking Robustness under Adversarial Document Manipulations

- [ ] ？？（一个信息检索框架）An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings

- [ ] ？？Deep Semantic Text Hashing with Weak Supervision

- [ ] ？？（排序中的公平性）Equity of Attention: Amortizing Individual Fairness in Rankings

- [ ] ？？Generating Better Queries for Systematic Reviews

- [ ] ？？Neural Compatibility Modeling with Attentive Knowledge Distillation

- [ ] ？？Measuring the Utility of Search Engine Result Pages

- [ ] ？？Ranking Documents by Answer-Passage Quality

- [ ] ？？（相关性的理论）What Can Rationales behind Relevance Judgments Tell Us About Assessor Disagreement?

- [ ] ？？（相关性的理论）Testing the Cluster Hypothesis with Focused and Graded Relevance Judgment

- [ ] ？？Neural Query Performance Prediction using Weak Supervision from Multiple Signals

- [ ] ？？Query Performance Prediction using Passage Information

- [ ] ？？Query Performance Prediction Focused on Summarized Letor Features

- [ ] ？？Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin

- [ ] ？？Query Variation Performance Prediction for Systematic Reviews

- [ ] ？？Efficient Exploration of Gradient Space for Online Learning to Rank

- [ ] ？？Selective Gradient Boosting for Effective Learning to Rank

- [ ] ？？Item Retrieval as Utility Estimation

- [ ] ？？Attention-based Hierarchical Neural Query Suggestion

- [ ] ？？Optimizing Query Evaluations using Reinforcement Learning for Web Search

- [ ] （network embedding）BiNE: Bipartite Network Embedding

- [ ] Translating Representations of Knowledge Graphs with Neighbors





# （完成）SIGIR2019（不包括openQA相关）

- [x] （CQA中的重复问题检测）Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection

  > 动机：paired answers can provide effective information for duplicate question detection while they may simultaneously introduce noise to the detection.
  >
  > 方法：We propose an adaptive multi-attention network (AMAN), an effective method integrating external knowledge from answers for duplicate identification and filtering out the noise introduced by paired answers adaptively.

- [x] Controlling Risk of Web Question Answering

- [ ] （人类行为）Human Behavior Inspired Machine Reading Comprehension

- [x] （人类行为）Teach Machine How to Read: Reading Behavior Inspired Relevance Estimation

  > 由人类阅读的6条启发式规则设计检索模型。

- [x] （人类行为）Investigating Passage-level Relevance and Its Role in Document-level Relevance Judgment

  > 如何由passage-level的相关性得到document-level的相关性。

- [x] CEDR: Contextualized Embeddings for Document Ranking

  > 把BERT得到的contextual embedding 融合到现有的neural ranking models（KNRM、DRMM、PACRR）

- [x] Content-Based Weak Supervision for Ad-Hoc Re-Ranking

  > We presented an approach for employing content-based sources of pseudo relevance for training neural IR models.
  >
  > We also showed that performance can be boosted using two ﬁltering techniques: one heuristic-based and one that re-purposes a neural ranker

- [x] Deeper Text Understanding for IR with Contextual Neural Language Modeling

  > 用bing search log数据微调Bert，再在Robust和CluWeb上进行rerank BoW top-100的实验。
  >
  > 使用Bert时把document切分成passage，再使用BERT-FirstP、BERT-MaxP、BERT-SumP三种组合方式。

- [x] FAQ Retrieval Using Attentive Matching

  > 本文提出了多种方法解决FAQ检索。We compared various possible aggregation methods to e￿ectively represent query, question and answer information, and observed that answers in FAQs can provide valuable and bene￿cial information for retrieval models, if properly aggregated.
  > 表明了在FAQ检索中也可以使用answer的信息。

- [x] FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance

  > This paper presented a method for using TSUBAKI-based query-question similarity and BERT-based query-answer relevance in a FAQ retrieval task.

- [x] History Modeling for Conversational Question Answering

  > 提出了一个模型 for 对话式QA。

- [x] Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval

  > presented two approaches(noise-aware model 和 influence-aware model) to reduce the amount of weak data needed to surpass the performance of the unsupervised method that generates the training data.

- [x] 【MMN】Multi-level Matching Networks for Text Matching

  > 动机：A major limitation of existing works is that only high level contextualized word representations are utilized to obtain word level matching results without considering other levels of word representations, thus resulting in incorrect matching decisions for cases where two words with different meanings are very close in high level contextualized word representation space.
  >
  > 方法：instead of making decisions utilizing single level word representations, a multi-level matching network (MMN) is proposed in this paper for text matching, which utilizes multiple levels of word representations to obtain multiple word level matching results for final text level matching decision.

- [x] Document Gated Reader for Open Domain Question Answering（计算每个文档与query的相似度时考虑其他文档的信息；将文档的相似度与answer span的概率相乘，在全局进行normalize；用bootstrapping数据生成机制解决远程监督带来的假正例问题）

- [x] An axiomatic approach to regularizing neural ranking models

  > 使用一些公理对document进行扰动，使得扰动后的文本比原文本更相关或者更不相关，从而得到更多的监督信号，帮助模型的训练。

- [ ] 



# （完成）SIGIR2020

- [x] （TKL）Local Self-Attention over Long Text for Efficient Document Retrieval

  > 对Transformer-kernel的改进，主要是针对Transformer-kernel用于长文本时。之前的基于Transformer的模型用于长文本时通常的做法是直接截断。本文在Transformer-kernel模型的基础上提出了local self-attention。并且计算出每个区域的相关度之后再使用top-local-max机制组合得到全局相关度。
  > 数据集：MARCO的document retrieval数据集

- [x] **Training Curricula for Open Domain Answer Re-Ranking**

  > for passage re-ranking 阶段。
  >
  > 提出课程学习策略，改进BERT和Conv-KNRM的训练过程，刚开始给容易的passage更大的权重，给难的passage更小的权重，后期给相同的权重。

- [x] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT

- [x] 【PreTTR】Efficient Document Re-Ranking for Transformers by Precomputing Term Representations

- [ ] SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval

- [x] Open-Retrieval Conversational Question Answering

  > 提出了一个开放域对话式QA数据集：OR-QuAC。
  >
  > 提出了一个baseline，包括retriever、re-ranker，reader。

- [x] Match$^2$: A Matching over Matching Model for Similar Question Identification

- [x] **MarkedBERT: Integrating Traditional IR cues in pre-trained language models for passage retrieval**

  > We proposed MarkedBERT that incorporates Exact Matching signals via a simple yet effective marking technique that only modifies the model input. 

- [x] Context-Aware Term Weighting For First-Stage Passage Retrieval

- [x] Learning Term Discrimination

- [x] Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation

  > 对BERT类的上下文预训练模型提出针对多轮对话任务的两种改进策略：1）Speaker Segmentation；2）Dialogue Augmentation

- [x] Large-scale Image Retrieval with Sparse Binary Projections

- [x] 【EPIC】**Expansion via Prediction of Importance with Contextualization**

  > 把query表示成稀疏向量，document表示成dense向量，计算点积。

- [x] Efficiency Implications of Term Re-Weighting for Passage Retrieval

- [x] DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding

- [x] Improving Matching Models with Hierarchical Contextualized Representations for Multi-turn Response Selection

- [x] **Distilling Knowledge for fast retrieval-based chat-bots**

  > we introduced an enhanced BERT cross-encoder architecture modiﬁed for the task of response retrieval. Alongside that, we utilized knowledge distillation to compress the complex BERT cross-encoder network as a teacher model into the student BERT bi-encoder model. This increases the BERT bi-encoders prediction quality without aﬀecting its inference speed.

- [x] **Having Your Cake and Eating it Too: Training Neural Retrieval for Language Inference without Losing Lexical Match**

  > We presented a simple approach to infuse lexical matching using unsupervised IR methods into a state-of-the-art transformer method - RoBERTa. We show that infusing lexical-matching improves the performance on simpler retrieval based question and the (justification) retrieval task itself。

- [x] An analysis of BERT in document ranking

  > What Does BERT Look At? An Analysis of BERT’s Attention.
  >
  > How Contextual are Contextualized Word Representations.
  >
  > Understanding the Behaviors of BERT in Ranking（刘知远）

- [x] Read, Attend, and Exclude:  Multi-Choice Reading Comprehension by Mimicking Human Reasoning Process

  > 提出了一个针对多选型阅读理解任务的模型。

- [x] Unsupervised Text Summarization with Sentence Graph Compression

  > 对多篇文档生成摘要。



# ACL2018

- [x] Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval



# （完成）ACL2019

- [x] RankQA: Neural Question Answering with Answer Re-Ranking 

- [x] Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data 

- [x] Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension

- [x] Latent Retrieval for Weakly Supervised Open Domain Question Answering

- [x] Multi-Hop Paragraph Retrieval for Open-Domain Question Answering

- [x] Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index

- [x] A cross-sentence latent variable model for semi-supervised sequence matching

- [x] Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction 

- [x] Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems

- [x] Matching Article Pairs with Graphical Decomposition and Convolutions

- [x] RE2: Simple and Effective Text Matching with Richer Alignment Features

- [x] A Lightweight Recurrent Network for Sequence Modeling

- [x] Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification

- [x] Pretraining Methods for Dialog Context Representation Learning

- [x] Relational Word Embeddings

- [x] Training Neural Response Selection for Task-Oriented Dialogue Systems

- [x] Learning Transferable Feature Representations Using Neural Networks

- [x] Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems

  > In this paper, instead of conﬁguring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. we consider four negative sampling strategies, namely minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling.
  >
  > In the ﬁrst two strategies, a response candidate that corresponds to the minimal or the maximal matching score at the current step is picked from a pool as a negative example for the next step; and in the latter two strategies, we select negative examples by considering how hard they are to the current matching models. The semi-hard sampling prefers candidates with moderate difﬁculty to avoid both false negatives and trivial true negatives, and the decay-hard sampling gradually increases the difﬁculty of negative samples with the training process going on.




第一次阅读列表

- [x] ！！（多段落MRC）**Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs**

- [x] ！！（生成式MRC）**Multi-style Generative Reading Comprehension** 【基于transformer】

- [x] **（NLI数据集中的bias问题）Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference 【给定假设和label，预测前提。。。方法新颖，但没看懂帖子】**

- [ ] （MRC、推理）Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text 【模型很复杂，，主要针对多步推理】

  


第二次论文列表（20篇）

- [x] ！！（MRC，外部知识）**Explicit Utilization of General Knowledge in Machine Reading Comprehension**【提出一种使用外部知识WordNet的方式】
- [x] ！！（MRC）**Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension**【提出词级别的动态self-attention机制用于多段落MRC】
- [x] ！！（多任务学习）**Multi-Task Deep Neural Networks for Natural Language Understanding** 【在BERT的基础上对GLUE的所有任务联合学习】
- [x] （无监督MRC数据集生成）Unsupervised Question Answering by Cloze Translation
- [x] ！！（BERT+知识图谱）ERNIE: Enhanced Language Representation with Informative Entities
- [x] ！！（问题生成）**Learning to Ask Unanswerable Questions for Machine Reading Comprehension** 【论坛上的帖子没太看懂具体方法，但感觉有价值】
- [x] ！！（QA对生成）**Generating Question-Answer Hierarchies** 【论坛上的帖子没太看懂具体方法，但感觉有价值】
- [x] ！！（MRC）**Exploiting Explicit Paths for Multi-hop Reading Comprehension**
- [x] ！！（self-attention）**Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned**
- [x] ！！（对话）**One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues**  【论坛上的帖子没太看懂具体方法，但感觉有价值】



第三次论文列表

- [x] ！！（QA模型的鲁棒性）**Improving the Robustness of Question Answering Systems to Question Paraphrasing**
- [x] **！！（对话的问题生成）Reinforced Dynamic Reasoning for Conversational Question Generation**
- [x] **Transformer-XL: Attentive Language Models beyond a Fixed-Length Context   **【循环机制 + 相对位置编码】
- [x] ！！！（多跳MRC）**Cognitive Graph for Multi-Hop Reading Comprehension at Scale** 【用两个system解决多跳MRC的推理】
- [x] ！！！（开放域QA）**Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader**【为了弥补KB无法提供开放域QA所需的全部知识，因此考虑加入一些非结构化文本知识】
- [x] ！！（多跳MRC）Multi-hop Reading Comprehension through Question Decomposition and Rescoring
- [x] ！！（MRC）Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension 【在BERT上加入知识库】
- [x] **（对话式QA）MCˆ2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension**
- [x] MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension
- [x] ！！（对话）**Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References**【建模并利用多个valid response之间的关系】
- [x] ！！（对话）**Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading**【融合外部知识，帮助对话系统生成response。。同时提供了一个包括外部知识的新数据集】



- [x] ！！（对话）Improving Multi-turn Dialogue Modelling with Utterance ReWriter

- [x] （对话）Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study

- [x] ！！（对话系统）Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension

- [x] （对话，response生成）Incremental Transformer with Deliberation Decoder for Document Grounded Conversations

- [x] （对话，response选择）Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection

- [x] （对话生成）ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation

- [x] （response 生成）Retrieval-Enhanced Adversarial Training for Neural Response Generation

- [x] （response 生成）Learning to Abstract for Memory-augmented Conversational Response Generation

- [x] （response 生成）Neural Response Generation with Meta-words

- [x] （对话，元学习）Domain Adaptive Dialog Generation via Meta Learning

- [x] ！！（不可回答的问题生成）Self-Attention Architectures for Answer-Agnostic Neural Question Generation

- [x] （图网络、多跳推理）Dynamically Fused Graph Network for Multi-hop Reasoning

- [x] ！！（多步推理）Compositional Questions Do Not Necessitate Multi-hop Reasoning

- [x] （MRC）Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives 

- [x] （MRC）Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension

- [x] ！！（预训练，迁移学习，自然语言生成）Large-Scale Transfer Learning for Natural Language Generation

- [x] Learning Compressed Sentence Representations for On-Device Text Processing

- [x] （BERT可解释性）What Does BERT Learn about the Structure of Language

- [x] Dual Supervised Learning for Natural Language Understanding and Generation

- [x] （梯度反转，域适应）Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks

- [x] （知识蒸馏，多任务学习）BAM! Born-Again Multi-Task Networks for Natural Language Understanding





# （完成）ACL2020（571 篇长文和 208 篇短文）

- [x] （效率）DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering（解耦低层的BERT）

- [x] （效率）DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference

  > 本文提出假设：对有些样本而言，不需要通过BERT/RoBERT的所有层就可以得到正确的预测结果，即有些层是冗余的。基于此假设，本文提出的方法是在预训练的BERT/RoBERT的基础上，在每层Transformer后面加一个分类层，然后进行两阶段的参数微调，最后在推断阶段时，如果哪层的分类结果达到置信值，就提前退出。实验部分在GLUE的所有task上进行评估，可以在性能稍降的基础上明显提高效率。

- [x] （效率、蒸馏）FastBERT: a Self-distilling BERT with Adaptive Inference Time

  > 整体模型结构和motivation类似DeeBERT，只是在对每个分类层的训练方法上不太一样，本文使用self-distilling的方式，可以利用无限的unlabel数据，最后来看效果会比DeeBERT更好。实验在6个中文数据集和6个英文数据集上进行，可以在性能几乎不降的情况下3-5倍增加效率。

- [ ] （效率）HAT: Hardware-Aware Transformers for Efficient Natural Language Processing

- [ ] （效率、模型压缩）MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices

- [x] （效率、稀疏表示）Contextualized Sparse Representations for Real-Time Open-Domain Question Answering

  > 对DenSPI的稀疏表示部分的改进，增加可学习的稀疏表示，使用核函数思想。

- [x] A Mixture of h−1 Heads is Better than h Heads

- [x] Highway Transformer: Self-Gating Enhanced Self-Attentive Networks

  > 对Transformer结构进行改进，加入类似highway的思想，从而加快收敛速度，同时取得更好的性能。主要的motivation是：Transformer中multi-head attention的结构只保留了词之间的关系，没有保留词本身的特征。

- [ ] How Does Selective Mechanism Improve Self-Attention Networks?

- [ ] Combining Subword Representations into Word-level Representations in the Transformer Architecture

- [x] （可解释性）Roles and Utilization of Attention Heads in Transformer-based Neural Language Models

  > 1) we suggest an analysis method which helps understand where linguistic properties are learned and represented along attention heads in transformer architectures and 2) we show that using analysis results, attention heads can be maximally utilized for performance gains during the ﬁne-tuning process on the downstream tasks and for capturing linguistic properties.

- [x] （预训练）BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

  > 主要是针对生成式任务（翻译、摘要、QA、对话），同时保证分类任务表现也不差。
  >
  > 论文中也对比了不同预训练目标函数和噪声干扰函数的影响。

- [ ] BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance

- [ ] ？？schuBERT: Optimizing Elements of BERT

- [ ] SenseBERT: Driving Some Sense into BERT

- [ ] （可解释性）Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT

- [x] （预训练）Span Selection Pre-training for Question Answering

- [x] （表示学习）Attentive Pooling with Learnable Norms for Text Representation

  > we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. we propose two methods to ensure the numerical stability of the model training: The ﬁrst one is scale limiting, which limits the scale of input representations to ensure their non-negativity and avoid potential exponential explosion. The second one is re-formulation, which decomposes the exponent operation into several safe atomic operations to avoid computing the real-valued powers of input features with less computational cost.
  >
  > 评估任务是文本（情感）分类。

- [x] 【MRC】Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension

  > We model documents at different levels of granularity to learn the hierarchical nature of the document. On the Natural Questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers.

- [x] （表示学习）Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning

  > we propose a new learning objective named language autoencoding (LAE) for obtaining fully contextualized language representations without repetition. To learn the proposed LAE, we develop a diagonal masking operation and an input isolation mechanism inside the T-TA.

- [x] QUASE: Question-Answer Driven Sentence Encoding

- [x] ReasoningOver Semantic-Level Graph for Fact Checking

  > The main contribution of this work is the graphbased reasoning approach for claim veriﬁcation.

- [x] （表示学习）SPECTER: Document-level Representation Learning using Citation-informed Transformers（利用学术文章之间的引用关系更好的学习学术文章的document表示）

- [x] （课程学习）Curriculum Learning for Natural Language Understanding

  > We explore and demonstrate the effectiveness of CL in the context of ﬁnetuning LM on NLU tasks. We propose a novel CL framework that consists of a Difﬁculty Review method and a Curriculum Arrangement algorithm.

- [x] （问题生成）How to Ask Good Questions? Try to Leverage Paraphrases

- [x] （问题生成）Semantic Graphs for Generating Deep Questions

  > Deep Questions Generation: 生成需要多文本片段推理的复杂问题。
  >
  > we propose a novel framework which ﬁrst constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards, we fuse the document-level and graphlevel representations to perform joint training of content selection and question decoding.

- [x] （商品检索）Learning Robust Models for e-Commerce Product Search（通过匹配的query-item pair生成不匹配的query，以便学习更鲁棒的分类界面，更好地识别不匹配的query-item pair。）

- [x] （FAQ）Unsupervised FAQ Retrieval with Question Generation and BERT

  > We presented a fully unsupervised method for FAQ retrieval. The method is based on an initial retrieval of FAQ candidates followed by three rerankers. The ﬁrst one is based on an IR passage retrieval approach, and the others two are independent BERT models that are ﬁne-tuned to predict query-to-answer and query-to-question matching.

- [x] The Cascade Transformer: an Application for Efficient Answer Sentence Selection

  >This work introduces CT, a variant of the traditional transformer model designed to improve inference throughput.  Our approach leverages classiﬁers placed at different encoding stages to prune candidates in a batch and improve model throughput.

- [x] （短文本匹配、图网络）Neural Graph Matching Networks for Chinese Short Text Matching

  > we propose a neural graph matching model for Chinese short text matching.

- [x] （语义匹配）tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection

  > we proposed a ﬂexible framework for combining topic models with BERT.

- [x] （自然语言生成）A Generative Model for Joint Natural Language Understanding and Generation



 



# EMNLP2018

- [x] Phrase indexed question answering: A new challenge for scalable document comprehension.（学习独立的document phrase表示和question表示）
- [x] Ranking paragraphs for improving answer recall in open-domain question answering.（对top ranked documents进行段落排序，选择topM个段落进行RC。以便经过retrieval之后可以接触更多的段落，提高answer的召回率）
- [x]  Adaptive Document Retrieval for Deep Question Answering.（探究了retriever和reader之间的关系，对于不同的question，取不同top数目的document用于RC）





# EMNLP2019

- [x] （对话）Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots

- [x] （对话）Multi-Granularity Representations of Dialog

- [x] （文本分类、表示学习）Enhancing Local Feature Extraction with Global Representation for Neural Text Classification

- [x] （表示学习）Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations

- [x] （表示学习）Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

- [x] ~~（句子表示学习）Parameter-free Sentence Embedding via Orthogonal Basis~~

- [x] （机器翻译）Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation

- [ ] （文本匹配）Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching

- [ ] （文本匹配）Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching

- [ ] （语义匹配）Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling

- [ ] （NLI）Asynchronous Deep Interaction Network for Natural Language Inference

- [x] （对上下文词向量的分析）How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings

  > 1）In all layers of all three models, the contextualized word representations of all words are not isotropic（即各向异性的）: they are not uniformly distributed with respect to direction. Instead, they are anisotropic, occupying a narrow cone in the vector space.
  >
  > 2）While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-speciﬁc representations.
  >
  > 3）In ELMo, representations of words in the same sentence grow more similar to each other as context-speciﬁcity increases in upper layers; in BERT, they become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average; in GPT-2, however, words in the same sentence are no more similar to each other than two randomly chosen words.
  >
  > 4）In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justiﬁcation for the success of contextualized representations.

- [ ] （对transformer的改进）Tree Transformer: Integrating Tree Structures into Self-Attention

- [ ] （对bert的改进）Fine-tune BERT with Sparse Self-Attention Mechanism

- [ ] （bert的可解释性）Visualizing and Understanding the Effectiveness of BERT

- [ ] ！！（相关性匹配、语义匹配）Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling

- [ ] （QA的鲁棒性）QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization

- [x] 【Birch】（长文本的ad-hoc检索）Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval

  > 用BERT sentence-level的得分得到document-level的得分。
  >
  > Our results demonstrate two surprising ﬁndings: ﬁrst, that relevance models can be transferred quite straightforwardly across domains by BERT, and second, that effective document retrieval requires only “paying attention” to a small number of “top sentences” in each document.

- [ ] （基于异质memory网络的对话系统）Task-Oriented Conversation Generation Using Heterogeneous Memory Networks

- [ ] （QA的采样策略）Improving Answer Selection and Answer Triggering using Hard Negatives

- [ ] （answer selection）A Gated Self-attention Memory Network for Answer Selection

- [ ] MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering

- [x] （外部知识，生成式QA）Incorporating External Knowledge into Machine Reading for Generative Question Answering

- [x] ！！**PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text**

- [x] ！！**Ranking and Sampling in Open-Domain Question Answering**

- [x] ！！Revealing the Importance of Semantic Retrieval for Machine Reading at Scale

- [x] （多跳问答）Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering

  > We introduced ROCC, a simple unsupervised approach for selecting justiﬁcation sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. We coupled this method with a state-of-the-art BERT-based supervised question answering system, and achieved a new state-of-the-art on the MultiRC and ARC datasets among approaches that do not use external resources during training.

- [x] （开放域多跳QA）Answering Complex Open-domain Questions Through Iterative Query Generation

  > In the very ﬁrst hop of reasoning, GOLDEN Retriever is presented the original question q, from which it generates a search query q1 that retrieves supporting document d1. Then for each of the subsequent reasoning steps (k = 2, . . . , S), GOLDEN Retriever generates a query q_k from the question and the available context, (q, d_1 , . . . , d_k−1 ). This formulation allows the model to generate queries based on information revealed in the supporting facts.

- [x]  （开放域QA）Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering（提出要跨段落进行global normalization，并使用基于BERT的passage reranker）



# （完成）EMNLP2020

- [ ] *~~AmbigQA: Answering Ambiguous Open-domain Questions~~*

  > 研究开放域问答中问题的歧义性，找到歧义性问题对应的多个答案，并针对每一个答案对问题进行重写以实现消歧。
  >
  > 提出了一个对应的任务和数据集，以及baseline。

- [x] 【**句子表示学习**】*An Unsupervised Sentence Embedding Method by Mutual Information Maximization*

  > <img src="../images/image-20201010154450654.png" alt="image-20201010154450654" style="zoom:33%;" />
  >
  > 句子输入到 BERT 后被编码，其输出的 token embeddings 通过多个不同 kernel size 的一维卷积神经网络得到多个 n-gram 特征。我们把每一个 n-gram 特征当成局部表征（Local representation）， 将平均池化后的局部表征称为全局表征（Global representation）。最后，我们用一个基于互信息的损失函数来学习最终的句向量。该损失函数的出发点是最大化句子的全局表征（句向量）与局部表征之间的平均互信息值，因为对于一个好的全局句向量，它与所对应的局部表征之间的 MI 应该是很高的， 相反，它与其他句子的局部表征间的 MI 应该是很低的。
  >
  > 这样的任务类似 contrastive learning，可以鼓励编码器更好地捕捉句子的局部表征，并且更好地区分不同句子之间的表征。

- [x] 【**模型的可解释性**】*Analyzing Individual Neurons in Pre-trained Language Models*

  > 分析模型：ELMo，T-ELMo，BERT，XLNet
  >
  > 分析方式：从neuron而不是layer的角度进行探测。
  >
  > 一些分析结论：1）it is possible to extract a very small subset of neurons to predict a linguistic task with the same accuracy as using the entire network. Low level tasks such as predicting morphology require fewer neurons compared to high level tasks such as predicting syntax. 2）ELMo generally needed fewer neurons while T-ELMo required more neurons compared to the other models to achieve oracle performance. 说明knowledge of lexical semantics in T-ELMo is distributed in more neurons。3）lexical tasks such as learning morphology (POS tagging) and word semantics (SEM tagging) are dominantly captured by the neurons at lower layers, whereas the more complicated task of modeling syntax (CCG supertagging) is taken care of at the ﬁnal layer. An exception to this overall pattern is the BERT model. Top neurons in BERT spread across all the layers, unlike other models where top neurons (for a particular task) are contributed by fewer layers. 4）BERT is the most distributed model with respect to all properties while XLNet exhibits focus with the most disjoint set of neurons and layers designated for different linguistic properties. 5）Some phenomena (e.g. Verbs) are distributed across many neurons while others (e.g. Interjections) are localized in a fewer neurons。

- [x] *Analyzing Redundancy in Pretrained Transformer Models*

  > 结论：1）相邻层的冗余度高，除了最后两层。2）85%的神经元可以被remove，而且没有性能的损失。大部分冗余的神经元来自于同一层或者相邻层。3）high layer-level redundancy for sequence labeling tasks, The amount of redundancy is substantially lower for sequence classiﬁcation tasks. all the sequence classiﬁcation tasks are learned at higher layers and none of the lower layers were found to be redundant. 4）XLNet is more redundant than BERT。5）Complex core language tasks require more neurons。Less task-speciﬁc redundancy for core linguistic tasks compared to higher-level tasks

- [ ] **

- [ ] *Context-Aware Answer Extraction in Question Answering**.* （没找到）

- [x] *Dense Passage Retrieval for Open-Domain Question Answering**.*

- [ ] *Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering**.* （没找到）

- [x] *Masking as an Efficient Alternative to Finetuning for Pretrained Language Models**.* 

  > 实验方法：使用预训练模型到下游任务时，不是采用微调所有参数的方法，而是选择一组weights，mask掉其他的参数，最后可以取得和微调差不多的性能。
  > 实验结果：1）Intrinsic evaluations show that masked language models extract valid representations for downstream tasks。2）the representations are generalizable。3）the minima obtained by ﬁnetuning and masking can be easily connected by a line segment。

- [ ] *Modularized Transfomer-based Ranking Framework**.*（没找到）

- [x] *Multi-Step Inference for Reasoning Over Paragraphs*

  > 提出推理模型，分为select+chain+predict三个模块。

- [x] *MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale**.*

  > 结论：1）neither domain similarity nor training data size are suitable for predicting the best models。

- [x] *Multilevel Text Alignment with Cross-Document Attention*

  > 在hierarchical attention 的双塔模型中加入cross-attention。
  >
  > 提出一个doc-doc和sentence-doc对齐的benchmark。

- [ ] *On the Sentence Embeddings from BERT for Semantic Textual Similarity**.*（没找到）

- [ ] *Probing Pretrained Language Models for Lexical Semantics*(找不到)

- [x] *Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting*

  > 在微调阶段使用多任务学习的思想，联合学习预训练任务，但是需要Pretraining Simulation mechanism解决预训练任务不可得的困难，而且多任务学习时要使用Objective Shifting mechanism逐渐偏重下游任务。

- [ ] *Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2**.* (找不到)

- [ ] *Towards Better Context-aware Lexical Semantics: Adjusting Contextualized Representations through Static Anchors* (找不到)

- [ ] FastFormers: Highly Efficient Transformer Models for Natural Language Understanding

- [ ] A Little Bit Is Worse Than None: Ranking with Limited Training Data

- [ ] Early Exiting BERT for Efficient Document Ranking

- [ ] Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering

- [ ] Exploring the Boundaries of Low-Resource BERT Distillation

- [ ] 





# （完成）ICLR2018

（2.3%的口头展示，31.4%的poster接受，9%的workshop）

- [x] ！！！！（DIIN）Natural Language Inference over Interaction Space

- [x] ！！！！（IR，多任务学习）Multi-Task Learning for Document Ranking and Query Suggestion 

- [x] ！！！！（表达学习）An efficient framework for learning sentence representations

- [x] ！！（表达学习，多任务学习）Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning

- [x] ！！！！（序列建模）Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling

- [x] ！！！！（开放域QA）Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering

- [x] ！！！（QA，强化学习）Ask the Right Questions: Active Question Reformulation with Reinforcement Learning

- [x] ！！（MRC）QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension

- [x] ！！（MRC）DCN+: Mixed Objective And Deep Residual Coattention for Question Answering

- [x] ！！（MRC）FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension

  

- [x] （MRC）Multi-Mention Learning for Reading Comprehension with Neural Cascades 

- [x] （摘要）Generating Wikipedia by Summarizing Long Sequences 

- [x] （表达学习）A New Method of Region Embedding for Text Classification 

  

- [ ] （语言模型）breaking_the_softmax_bottleneck：a_high_rank_rnn_language_model

- [ ] （语言模型）Neural Language Modeling by Jointly Learning Syntax and Lexicon

- [ ] （文本生成，GAN）MaskGAN: Better Text Generation via Filling in the _______ 

- [ ] （摘要）A Deep Reinforced Model for Abstractive Summarization

- [ ] （LSTM的可解释性，情感分析）Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs

- [ ] （迁移学习）Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation

- [ ] （图卷积+self-attention）Graph Attention Networks

- [ ] （多任务学习）Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning 

- [ ] All-but-the-Top: Simple and Effective Postprocessing for Word Representations

- [ ] ！！（一个逻辑蕴含的数据集）Can Neural Networks Understand Logical Entailment? 

- [ ] Spherical CNNs

- [ ] （多任务学习）Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering

- [ ] （transformer-based）Non-Autoregressive Neural Machine Translation



# （完成）ICLR2019（可能需要再滤一遍）

- [x] （面向任务型对话系统）Global-to-local Memory Pointer Networks for Task-Oriented Dialogue
- [x] 方法过于数学（对话系统的问题生成）Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation
- [x] （对话系统）Wizard of Wikipedia: Knowledge-Powered Conversational Agents
- [x] （NLU）GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding 
- [x] （多文档QA）Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering
- [x] （开放域QA）Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering
- [x] 论文写得很不清晰（QA）Generative Question Answering: Learning to Answer the Whole Question
- [x] （对话式QA）flowQA




- [ ] （对话系统）Detecting Egregious Responses in Neural Sequence-to-sequence Models 
- [ ] （信息检索）textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR

- [ ] Posterior Attention Models for Sequence to Sequence Learning

- [ ] Universal Transformers 
- [ ] Trellis Networks for Sequence Modeling
- [ ] 



# （完成）ICLR2020

第一次阅读列表：

- [x] ！！！**NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension**【提出一个比较好用的 answerability 判定模块，可借鉴到qq匹配任务上】
- [x] ！！！（对话、外部知识）**Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue**【对话任务，如何更好地选择知识】
- [x] **TinyBERT: Distilling BERT for Natural Language Understanding**
- [x] **StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding**
- [x] **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**【对bert预训练任务的改进】
- [x] Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension
- [x] （和我的研究好像很像）Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring
- [x] （对BERT的改进）ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
- [x] （对BERT的改进）RoBERTa: A Robustly Optimized BERT Pretraining Approach
- [x] （对话生成，低资源）Low-Resource Knowledge-Grounded Dialogue Generation 【论坛上写的很清晰，方法简明，不用再看了】



第二次阅读列表：

- [x] **！！！！（多跳QA）Transformer-XH: Multi-hop question answering with eXtra Hop attention**
- [x] **！！！！（问题生成）Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation**
- [x] （QA、推理）**Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering**
- [x] ！！！（BERT、机器翻译）**Incorporating BERT into Neural Machine Translation**【不直接使用BERT初始化模型，本文是把BERT的输出融入到SEQ2SEQ模型的每一层，用在每个self-attention之中。此外，本文还提出drop-net——随机丢弃BERT表示或模型本身的表示，充分利用两个方面的信息。实验结果还不错。】
- [x] ！！！（互信息最大化、表达学习）**A Mutual Information Maximization Perspective of Language Representation Learning**



- [x] **（基于知识的预训练语言模型）Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model** 【论坛上写的很清楚】
- [ ] （新的阅读理解数据集）ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning
- [ ] ！！（对transformer的理解）Are Transformers universal approximators of sequence-to-sequence functions?
- [ ] ！！（Transformer的可解释性）Robustness Verification for Transformers 
- [ ] ！！（Transformer）On the Relationship between Self-Attention and Convolutional Layers【好像不少数学 推理】
- [x] **（预训练的检索模型）GOING BEYOND TOKEN-LEVEL PRE-TRAINING FOR EMBEDDING-BASED LARGE-SCALE RETRIEVAL** 【论坛上写的很清楚】
- [ ] （学习word embedding）DeFINE: Deep Factorized Input Word Embeddings for Neural Sequence Modeling
- [ ] （MRC、推理）**Neural Module Networks for Reasoning over Text** 

- [ ] ！！Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models

- [ ] （文本生成、对抗学习）Self-Adversarial Learning with Comparative Discrimination for Text Generation



# （完成）ICLR2021

- [x] Universal Sentence Representations Learning with Conditional Masked Language Model 

  > A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora (either in monolingual and multilingual). CMLM通过以相邻句子的编码向量为条件，将句子表示学习整合到MLM训练中。 

- [x] Synthesizer: Rethinking Self-Attention for Transformer Models 

  > This paper seeks to develop a deeper understanding of the role that the dot product self-attention mechanism plays in Transformer models.
  >
  > This paper proposes SYNTHESIZER, a new model that learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products.

- [x] Multi-Head Attention: Collaborate Instead of Concatenate

  > As the multiple heads are inherently solving similar tasks, they can collaborate instead of being independent. Collaborative MHA introduces weight sharing across the key/query projections and decreases the number of parameters and FLOPS.

- [x] Deepening Hidden Representations from Pre-trained Language Models 

  > We argue that only taking single layer’s output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the ﬁnal layer.

- [x] Contextual Knowledge Distillation for Transformer Compression

  > we present a new knowledge distillation strategy for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. WR is proposed to capture the knowledge of relationships between word representations and LTR deﬁnes how each word representation changes as it passes through the network layers.

- [ ] A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks

- [x] JAKET: Joint Pre-training of Knowledge Graph and Language Understanding

  > These Pre-trained language models models struggle to grasp world knowledge about entities and relations. we propose JAKET, a Joint pre-trAining framework for KnowledgE graph and Text. Under our framework, the knowledge module and language module both provide essential information for each other.

- [x] Optimizing Transformers with Approximate Computing for Faster, Smaller and more Accurate NLP Models 

  > We observe that for a given downstream task, some parts of the pre-trained Transformer are more signiﬁcant to obtain good accuracy, while other parts are less important or unimportant. In order to exploit this observation in a principled manner, we introduce a framework to introduce approximations while ﬁne-tuning a pre-trained Transformer network, optimizing for either size, latency, or accuracy of the ﬁnal network.

- [ ] Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth 

  > In this paper, we study these core questions, through detailed analysis of a family of ResNet models with varying depths and widths trained on CIFAR-10, CIFAR-100 and ImageNet.

- [x] 【MDR】**Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval** 

  > We propose a simple and efﬁcient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.
  >
  > Our method iteratively encodes the question and previously retrieved documents as a query vector and retrieves the next relevant documents using efﬁcient MIPS methods.
  >
  > It improves DPR by sampling negatives from a memory bank (Wu et al., 2018) — in which the representations of negative candidates are frozen so more candidates can be stored.

- [x] Cluster-Former: Clustering-based Sparse Transformer for Question Answering

  > self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically
  >
  > we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Cluster-Former consists of two types of encoding layer. The ﬁrst one (noted as SlidingWindow Layer) focuses on extracting local information within a sliding window. The other one (noted as Cluster-Former Layer) learns to encode global information beyond the initial chunked sequences. we stack these two types of layer interchangeably to capture both global and local context efﬁciently.

- [x] Block Skim Transformer for Efficient Question Answering

  > We then propose Block Skim Transformer (BST), a plug-and-play module to the transformer-based models, to accelerate transformer-based models on QA tasks. 
  >
  > By handling the attention weight matrices as feature maps, the CNN-based Block Skim module extracts information from the attention mechanism to make a skim decision. With the predicted block mask, BST skips irrelevant context blocks, which do not enter subsequent layers’ computation.

- [x] Distilling Knowledge from Reader to Retriever for Question Answering

  > In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.
  >
  > Relevance-guided supervision for openqa with colbert

- [ ] When Do Curricula Work?

- [ ] Rethinking Embedding Coupling in Pre-trained Language Models

- [x] SEED: Self-supervised Distillation For Visual Representation

  > we ﬁnd that existing techniques like contrastive learning do not work well on small networks.
  >
  > Instead of directly learning from unlabeled data, we proposed SEED as a novel self-supervised learning paradigm, which learns representation by self-supervised distillation from a bigger SSL pre-trained model.

- [x] Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval 

  > In this paper, we ﬁrst theoretically show the learning bottleneck of dense retrieval is the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. 
  >
  > We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.

- [ ] PMI-Masking: Principled masking of correlated spans

  > 不只是mask一个subtoken，而且mask一个span。

- [ ] Decomposing Mutual Information for Representation Learning

- [ ] Semantic Hashing with Locality Sensitive Embeddings

  > we propose a method for learning continuous representations in which the optimized similarity is the angular similarity.  

- [x] Memory Representation in Transformer 

  > Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model.
  >
  > In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer.

- [x] Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model

  > In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. To reduce the variance due to the sampling of masks, we proposed a fully-explored masking strategy, where a text sequence is divided into multiple non-overlapping segments. During training, all tokens in one segment are masked out, and the model is asked to predict them with the other segments as the context. It was demonstrated theoretically that the gradients obtained with such a novel masking strategy have a smaller variance, thus enabling more efﬁcient pre-training.
  >
  > 现有的BERT等模型往往采用masked language model进行自监督学习，但是其往往采用随机的方法确定mask的word或者span；本文提出不合适的mask会导致梯度方差变大，并影响模型的效果，并分析原因在于同时mask的word之间具有一定的相似度；故本文提出一种特殊的mask机制，其考虑增大被mask的word之间的差异，进而削弱梯度方差大带来的影响。

- [x] Deep Retrieval: An End-to-End Structure Model for Large-Scale Recommendations

  > we have proposed Deep Retrieval, an end-to-end learnable structure model for largescale recommender systems. DR uses an EM-style algorithm to learn the model parameters and paths of items jointly. Experiments have shown that DR performs well compared with brute-force baselines in two public recommendation datasets.

# 



# ICML2018

# ICML2019





# IJCAI2018

# IJCAI2019





# NIPS2018

# NIPS2019

# NIPS2020







# AAAI2018

# AAAI2019

# AAAI2020

- [x] 【NLI、PI】Multi-level Head-wise Match and Aggregation in Transformer for Textual Sequence Matching

  > Multi-level Head-wise Match.

- [x] 【QA】Knowledge and Cross-Pair Pattern Guided Semantic Matching for Question Answering

  > we propose a novel system named KCG for AS, which considers both knowledge and pattern conditions. KCG applies both intra-pair and cross-pair learning. cross-pair learning使用GCN网络，intra-pair learning使用cross attention。最终是cross-pair 和 intra-pair得分的加权和。
  >
  > WikiQA、TrecQA

- [ ] Scalable attentive sentence-pair modeling via distilled sentence embedding

 

# NAACL2018

- [x] Contextualized word representations for reading comprehension.（NAACL2018）

# NAACL2019





# CIKM2018

- [x] From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing
- [x] Retrieve-and-read: Multi-task learning of information retrieval and reading comprehension.
- [ ] 



# CIKM2019

- [ ] （表示学习）Beyond word2vec: Distance-graph Tensor Factorization for Word and Document Embeddings
- [ ] （dialogue，response选择，捕捉多粒度信息）Multi-Turn Response Selection in Retrieval-Based Chatbots with Iterated Attentive Convolution Matching Network
- [ ] （对话）A Hybrid Retrieval-Generation Neural Conversation Model
- [ ] （对话）Attentive History Selection for Conversational Question Answering
- [ ] （知识图谱QA、用户反馈）An Interactive Mechanism to Improve Question Answering Systems via Feedback
- [ ] （构建适合语义检索的稠密向量搜索引擎）GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine （不用看）

------ 短文

- [ ] ~~Machine Reading Comprehension: Matching and Orders~~
- [ ] ~~Cluster-Based Focused Retrieval~~
- [ ] （dialogue，response选择）Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] （learn2rank的训练策略、鲁棒性）Analysis of Adaptive Training for Learning to Rank in Information Retrieval



# ！CIKM2020

- [x] 【SMITH】（长文本匹配）Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching

  > 两层次的 dual Transformer架构，第一层建模句子块内文本，第二层由句子块建模文本表达。
  >
  > 提出预训练+微调的方法，其中预训练任务在BERT MLM的基础上加入sentence block mask language model。

- [x] Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems

- [x] Dual Head-wise Coattention Network for Machine Comprehension with Multiple-Choice Questions

- [ ] Robust Retrievability based Document Selection for Relevance Feedback with Automatically Generated Query Variants（找不到）

- [ ] Transformer Model Compression via Joint Structured Pruning and Knowledge Distillation（找不到）

- [ ] TABLE: A Task-Adaptive BERT-based ListwisE Ranking Model for Document Retrieval（找不到）

- [x] CGTR: Convolution Graph Topology Representation for Document Ranking

  > we propose a GCN based method to encode global structure in contextual embeddings to improve ad-hoc document retrieval.

- [x] 【DMIN】Deep Multi-Interest Network for Click-through Rate Prediction

  > Distant Supervision in BERT-based Adhoc Document Retrieval
  >
  > Multi-Interest Network with Dynamic Routing for Recommendation at Tmall

- [x] Distant supervision in BERT-based Ad-hoc Document Retrieval

  > We find that direct transfer of relevance labels from documents to passage introduces label noise and it can not leverage information from large amount of unlabelled documents.
  >
  > We show that our distantly supervised models(Qa-DocRank and QA-Full-DOCRANK) perform better than the baselines. Extracting relevant passages from the unlabelled documents (QA-Full-DOCRANK) helps in solving the scarcity problem.
  >
  > Qa-DocRank：先用一个QA_model对passage进行打分，根据设定阈值得到query-passage的相关性label，再用这些labeled pair微调BERT模型。推断时是先得到每个passage的得分，再根据FirstP/MaxP/SumP/AvgP等策略得到最终得分。
  >
  > QA-Full-DOCRANK：在Qa-DocRank的基础上，用一个QA_model对无标签的document的passage进行打分，根据设定阈值得到query-passage的相关性label，再用这些labeled pair微调BERT模型。



# ！WSDM2018（共录取84篇，录取率16%）

- [x] （匹配，迁移学习）Modelling Domain Relationships for Transfer Learning on Chatbot-based Question Answering Systems
- [x] ！！（ConvKNRM）Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search
- [x] （HyperQA）Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering
- [ ] （XXX）Query Driven Algorithm Selection in Early Stage Retrieval
- [x] ！！（Co-PACRR）ERM-PACRR: A Neural IR model with Enhanced Relevance Matching
- [x] Neural Ranking Models with Multiple Document Fields



# ！WSDM2019

- [ ] （xxxx,ranking）WassRank: Listwise Document Ranking Using Optimal Transport Theory
- [ ] （xxxx,ranking）Joint Optimization of Cascade Ranking Models
- [x] （匹配，迁移学习，强化学习）Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching
- [x] （QA）Learning to Transform, Combine, and Reason in Open-Domain Question Answering
- [x] （对话）Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] （XXX,表达学习）SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.
- [x] （电商）Weakly Supervised Co-Training Query Rewriting and Semantic Matching for E-Commerce
- [x] （推荐）Gated Attentive-Autoencoder for Content-Aware Recommendation
- [x] （推荐）Product-Aware Answer Generation in E-Commerce Question-Answering



# ！WSDM2020

- [x] 【TMKD】Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System

  > we propose a novel Two-stage Multi-teacher Knowledge Distillation approach for model compression:1) a Q&A multi-teacher distillation task is proposed for student model pre-training,2) a multi-teacher paradigm is designed to jointly learn from multiple teacher models (m-o-1) for more generalized knowledge distillation on downstream specific tasks.



# WWW2019

- [ ] （IR）Adversarial Sampling and Training for Semi-Supervised Information Retrieval
- [ ] What We Vote for? Answer Selection from User Expertise View in Community Question Answering
- [ ] （匹配模型）Semantic Text Matching for Long-Form Documents
- [ ] （表示学习）Learning Graph Pooling and Hybrid Convolutional Operations for Text Representations

- [ ] （问答）HAR: A Hierarchical Attention Retrieval Model for Healthcare Question Answering
- [ ] （匹配模型）Learning Fast Matching Models from Weak Annotations
- [ ] （表示学习）Semantic Hilbert Space for Text Representation Learning
- [ ] （问答）Focusing Attention Network for Answer Ranking
- [ ] （问题生成）Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to Attention Network





# SIGKDD

# ICDM

# COLING

# CoNLL







# 模型压缩

##### 剪枝

- [ ] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.（2019）
- [ ] Are sixteen heads really better than one?（NIPS2019）

##### Quantization 方法

- [ ] Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.（ICLR2015）
- [ ] Deep learning with limited numerical precision.（ICML2015）
- [ ] Training and inference with integers in deep neural networks.（ICLR2018）
- [ ] Efficient Weights Quantization of Convolutional Neural Networks Using Kernel Density Estimation based Non-uniform Quantizer.（2019）

##### binarized networks

- [ ] Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1.（2016）

##### 蒸馏

- [ ] 【知识蒸馏】Model compression（2006，Buciluˇa）

- [ ] 【知识蒸馏】Do deep nets really need to be deep?（2014）

- [ ] 【知识蒸馏】Distilling the knowledge in a neural network.（2015，Hinton）

- [x] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.（NIPS2019，将BERT-base通过预训练阶段蒸馏到6层。loss=MLM + cosine embedding loss（？？） + 预测层输出类别分布蒸馏loss。实验是GLUE、SQuAD）

- [x] TinyBERT: Distilling BERT for Natural Language Understanding.（2019，将BERT-base通过预训练阶段蒸馏到少层。loss=embedding层蒸馏 + trm attention matrix蒸馏loss + trm hidden state蒸馏loss + 预测层输出类别分布蒸馏loss。实验是GLUE）

- [ ] Well-read students learn better: The impact of student initialization on knowledge distillation.（2019）

- [x] 【蒸馏成dual encoder】**Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**.（2019，distilling the **BERT** model into a two-tower model with **one-layer BiLSTM** as encoders。使用数据增强得到无标签数据集进一步帮助知识蒸馏。实验是SST-2、QQP、MNLI）

- [x] 【蒸馏成dual encoder】**Scalable attentive sentence-pair modeling via distilled sentence embedding**.（AAAI2020，Given a cross-attentive teacher model (e.g. a fine-tuned **BERT-large**), we train a sentence embedding based student model（**BERT-large**） to reconstruct the sentence-pair scores obtained by the teacher model。loss=预测层输出蒸馏loss + ground truth loss。实验是GLUE中的5个sentence-pair任务）

- [x] 【TMKD】Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System（WSDM2020，预训练蒸馏+微调蒸馏，m-to-1模式，student模型是BiLSTM或者BERT。实验是QA、MNLI、SNLI、RTE）  

- [x] 【蒸馏成dual encoder】Distilling Knowledge for fast retrieval-based chat-bots（SIGIR2020。student model是BiLSTM或者BERT）

- [x] 【蒸馏成dual encoder】DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling（2020）

- [ ] Understanding bert rankers under distillation.

  



