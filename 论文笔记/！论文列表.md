å…¶ä»–

- [x] Rethinking Query Expansion for BERT Rerankingï¼ˆECIR2020ï¼‰

  > BERT rerankeræ›´é€‚åˆäºè‡ªç„¶è¯­è¨€å¥å­çš„é•¿queryï¼Œæ‰€ä»¥ä¼ ç»Ÿæ¨¡å‹çš„é—®é¢˜æ‰©å±•æ–¹æ³•å¯èƒ½å¹¶ä¸å¤ªé€‚ç”¨äºBERT-based rerankerã€‚æœ¬æ–‡æ¢ç©¶äº†æ–°çš„é—®é¢˜æ‰©å±•æ–¹æ³•ï¼Œå‘ç°ç»“åˆæ‰©å±•ã€ç»“æ„è¯ã€‘å’Œã€æ¦‚å¿µè¯ã€‘çš„æ–¹æ³•æœ€æœ‰æ•ˆã€‚

- [x] ã€Transformer-kernelã€‘Interpretable & Time-Budget-Constrained Contextualization for Re-Rankingï¼ˆECAI2020ï¼‰

  > ç½‘ç»œç»“æ„ï¼šå¯¹queryå’Œdocumentåˆ†åˆ«é‡‡ç”¨ä¸‰å±‚Transformerï¼Œå¯¹contextual word embeddingç»“æœè®¡ç®—äº¤äº’çŸ©é˜µï¼Œå†è¿›è¡Œkernel-poolingã€‚
  > å–ç‚¹ï¼š1ï¼‰ç›¸æ¯”BERTæ¥è¯´æ•ˆç‡æ›´é«˜ï¼Œç›¸æ¯”éBERTçš„æ¨¡å‹æ€§èƒ½æ›´å¥½ï¼Œæ‰€ä»¥è¾¾åˆ°effectiveå’Œefficiencyçš„å¹³è¡¡ã€‚2ï¼‰é‡‡ç”¨äº¤äº’çŸ©é˜µå’Œkernel-poolingï¼Œå…·æœ‰æ›´å¥½çš„è§£é‡Šæ€§ï¼Œå› ä¸ºå¯ä»¥çœ‹åˆ°documentæ¯ä¸ªtermçš„å¾—åˆ†ã€‚
  > å®éªŒæ•°æ®é›†ï¼šMARCO-passgeã€MARCO-documentã€TREC-CAR2017
  > äº®ç‚¹ï¼šè¯„ä¼°æ•ˆç‡çš„æ–¹å¼ï¼Œæ¯”è¾ƒäº†åœ¨ç›¸åŒçš„æ—¶é—´é™åˆ¶ä¸‹ï¼Œæ¯ä¸ªæ¨¡å‹çš„MRRã€NDCGã€Recallæ€§èƒ½ã€‚è€Œä¸æ˜¯åƒå…¶ä»–å·¥ä½œä¸­ä»…æ¯”è¾ƒå¤„ç†æ¯ä¸ªdocument/queryæ‰€éœ€çš„æ—¶é—´ã€‚
  
- [x] ã€WMDï¼šword moverâ€˜s distanceã€‘From Word Embeddings To Document Distancesï¼ˆ2015ï¼ŒICMLï¼‰

  > å®šä¹‰ä¸€ç§Word mover distanceï¼Œç”±word embeddingä¹‹é—´çš„è·ç¦»å¾—åˆ°docuemntä¹‹é—´çš„è·ç¦»ã€‚ä¸»è¦ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ŒåŸè®ºæ–‡ä¸­ä¸æ¶‰åŠæ£€ç´¢ä»»åŠ¡ã€‚
  
- [x] Reqa: an evaluation for end-to-end answer retrieval modelsï¼ˆ2019ï¼‰

  > æœ¬æ–‡æŠŠSQUADå’ŒNQæ•°æ®é›†æ”¹ç¼–æˆReQAä»»åŠ¡ï¼Œå³introduce Retrieval QuestionAnswering (ReQA) as a new benchmark for evaluating end-to-end answer retrieval models.
  >
  > ReQAä»»åŠ¡ï¼šThe task assesses how well models are able to retrieve relevant sentence-level answers to queries from a large corpus.

- [x] PARADE: Passage representation aggregation for document reranking

  > æå‡ºäº†ä¸‰ç§ç”±passage-level representationå¾—åˆ°document-level representationï¼Œè¿›è€Œå¾—åˆ°documentæ‰“åˆ†çš„æ–¹æ³•ï¼šPARADE_Maxã€PARADE_Attnã€PARADE_Transformerã€‚
  
- [ ] Learning a Beer Negative Sampling Policy with Deep Neural Networks for Searchï¼ˆICTIR2019ï¼‰

  > we establish that using reinforcement learning to optimize a policy over a set of sampling functions.
  
- [x] BERT-QE: Contextualized Query Expansion for Document Re-ranking

  > this paper proposes a novel query expansion model that leverages the strength of the BERT model to select relevant document chunks for expansion



# 2020

- [x] Neural Passage Retrieval with Improved Negative Contrast

  > We explore four negative sampling strategies: 1) coarse semantic similarity, 2) fine semantic similarity, 3) BM25 negative, 4) context negative.
  >
  > We train the dual encoder models in two stages: pre-training with synthetic data and ï¬ne tuning with domain-speciï¬c data. We apply negative sampling to both stages.
  >
  > it is not evident that there is one single sampling strategy that works best in all the tasks. mixing the negatives from different strategies achieve performance on par with the best performing strategy in all tasks.

- [x] Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation

  > we explore a supervised data augmentation approach leveraging a complex classiï¬cation model with cross-attention between questionanswer pairs.

- [x] A White Box Analysis of ColBERT

  > our analysis reveals that ColBERT (i) is able to capture a notion of term importance; (ii) relies on exact matches for important terms.
  >
  > 1ï¼‰IDFè¶Šå¤§çš„query termï¼Œå¯¹æ•´ä¸ªç›¸å…³æ€§çš„è´¡çŒ®æ›´å¤§ã€‚ä½†IDFå¤§åˆ°ä¸€å®šç¨‹åº¦åï¼Œè¿™ç§æ­£å‘æ€§ä¸å†æ˜æ˜¾ã€‚
  >
  > 2ï¼‰IDFè¶Šå¤§çš„query termï¼Œåˆ™å¯¹è¯¥è¯å­˜åœ¨ç²¾ç¡®åŒ¹é…ä¿¡å·å¾ˆé‡è¦ã€‚å¾®è°ƒå¯ä»¥åŠ å‰§è¿™ç§æ­£å‘æ€§ã€‚
  >
  > 3ï¼‰IDFä½çš„è¯æ›´å®¹æ˜“å¸æ”¶ä¸Šä¸‹æ–‡ä¸­å…¶ä»–è¯çš„è¯ä¹‰ã€‚

- [x] Diagnosing BERT with Retrieval Heuristics

  > ä½œè€…è®¾è®¡äº†9ä¸ªdiagnostic datasetså»æ¢ç©¶BERTï¼ˆå¾®è°ƒon TREC DLï¼‰æ»¡è¶³çš„retrieval axiomã€‚
  > ç»“è®ºï¼šåœ¨ç°æœ‰çš„å®éªŒè®¾ç½®ä¸Šï¼Œå‘ç°BERT, while signiï¬cantly better than traditional models for ad-hoc retrieval, does not fulï¬l most retrieval heuristicsã€‚è¯´æ˜the axioms are not suitable to analyse BERT and it is an open question what type of axioms would be able to capture some performance aspects of BERT and related models.

- [x] 



# 2021

- [x] Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching

  > è§£å†³é•¿æ–‡æœ¬re-rankçš„é—®é¢˜ã€‚

- [x] Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipelineï¼ˆECIRï¼‰

  > éšç€æ·±åº¦è¯­è¨€æ¨¡å‹åœ¨ç¬¬ä¸€é˜¶æ®µæ£€ç´¢ä¸­çš„åº”ç”¨ï¼ˆå¦‚doc2queryï¼ŒDeepCTï¼ŒHDCTï¼‰ï¼Œä½¿å¾—ç¬¬ä¸€é˜¶æ®µæ£€ç´¢å¾—åˆ°çš„ç»“æœæœ‰æ›´å¤šçš„hard negativeã€‚ä¹‹å‰ç”¨äºrerankingé˜¶æ®µçš„BERTæ¨¡å‹é€šå¸¸ä½¿ç”¨binary cross entropyç›®æ ‡å‡½æ•°ï¼Œä½†åœ¨å¼ºretrieverçš„èƒŒæ™¯ä¸‹ï¼Œè¿™ä¸å†æ˜¯æœ€ä¼˜çš„æ¨¡å‹è®­ç»ƒæ–¹å¼ã€‚å¯¹æ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Localized Contrastive Estimationç›®æ ‡å‡½æ•°ã€‚

- [x] Optimizing Dense Retrieval Model Training with Hard Negatives

  > æå‡ºäº†ä¸¤ç§dense retrieval modelè®­ç»ƒç­–ç•¥ï¼š
  >
  > 1ï¼‰STARï¼šstatic hard negative samlingæ–¹æ³•çš„è®­ç»ƒä¸å¤ªç¨³å®šã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå¼•å…¥random negativesã€‚è€Œä¸”ä¸ºäº†åœ¨å¼•å…¥random negativesçš„åŒæ—¶ä¸å¢åŠ è®¡ç®—ä»£ä»·ï¼Œé‡‡ç”¨in-batchç­–ç•¥ã€‚
  >
  > 2ï¼‰ADOREï¼šè®­ç»ƒè¿‡ç¨‹å›ºå®šdoc embeddingï¼Œåªè®­ç»ƒquery embeddingã€‚ä»è€Œå¯ä»¥æ—¶æ—¶å¾—åˆ°çœŸæ­£çš„dynamic hard negativesã€‚

- [x] OpenMatch: An Open-Source Package for Information Retrieval

  > æ¸…åå¼€æºçš„NeuIRå·¥å…·åŒ…ã€‚
  >
  > <img src="../images/image-20210310150321343.png" alt="image-20210310150321343" style="zoom:33%;" />

- [x] Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation

  > Following our observation that different architectures converge to different scoring ranges, we proposed to optimize not the raw scores, but rather the margin between a pair of relevant and non-relevant passages with a Margin-MSE loss.

- [x] Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation

  > we propose TRMD, a method enabling a bi-encoder model to learn from cross-encoder and bi-encoder teachers by applying multi-teacher knowledge distillation.

- [x] Composite Re-Ranking for Efficient Document Search with BERT

  > With token encoding, BECR effectively approximates the query representations and makes a proper use of deep contextual and lexical matching features, allowing for strong ad-hoc ranking performance to be achieved on TREC datasets.

- [x] TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for EÂ€icient Retrieval

- [x] Towards Robust Neural Retrieval Models with Synthetic Pre-Training

  > æœ¬æ–‡æå‡ºDPRæ¨¡å‹ï¼ˆç”¨NQæ•°æ®é›†å¾®è°ƒè®­ç»ƒçš„ï¼‰çš„zero-shotè¿ç§»èƒ½åŠ›ä¸å¼ºï¼Œå°¤å…¶æ˜¯é¢†åŸŸå·®å¼‚è¾ƒå¤§æ—¶ã€‚æœ¬æ–‡æå‡ºç”¨åˆæˆçš„æ•°æ®æ¥å…ˆå¯¹DPRè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†ç”¨NQæ•°æ®é›†è¿›è¡ŒDPRçš„å¾®è°ƒï¼Œè¿™æ ·å°±èƒ½è·å¾—è¾ƒå¥½zero-shotæ€§èƒ½ã€‚å…¶ä¸­åˆæˆçš„æ•°æ®çš„æŒ‡ï¼šå…ˆç”¨NQæ•°æ®é›†ï¼ˆquestion+passageï¼‰å¾®è°ƒBARTç”Ÿæˆå™¨ï¼Œç„¶åç”¨è®­ç»ƒå¥½çš„BARTç”Ÿæˆå™¨å¯¹Wikipediaçš„æ¯ä¸ªpassageç”Ÿæˆquestionï¼Œæ‹¿ç”Ÿæˆçš„question-passageé¢„è®­ç»ƒDPQæ¨¡å‹ã€‚

- [x] Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling

  > We proposed to improve dense passage retrieval training with a cost-neutral topic aware (query) and balanced margin (passage pairs) sampling strategy, called TAS-Balanced.
  >
  > è€Œä¸”ä½¿ç”¨BERT_CATå’ŒColBERTä¸¤ä¸ªæ¨¡å‹è¿›è¡Œè’¸é¦ã€‚

- [x] COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List

  > å¯¹dual encoderå¾—åˆ°çš„ä¸Šä¸‹æ–‡è¯åµŒå…¥ï¼ˆé™ç»´åï¼‰è¿›è¡Œå€’æ’ç´¢å¼•ã€‚è®¡ç®—ç›¸ä¼¼åº¦æ—¶åªçœ‹docä¸­å­˜åœ¨çš„query termä¸ query termçš„ç›¸ä¼¼åº¦ï¼Œå†æŠŠæ‰€æœ‰query termçš„ç›¸ä¼¼åº¦ç›¸åŠ ã€‚

- [x] UHD-BERT: Bucketed Ultra-High Dimensional Sparse Representations for Full Rankingï¼ˆEMNLP2021ï¼‰

  > åœ¨åŒå¡”BERTçš„åŸºç¡€ä¸Šå­¦ä¹ é«˜ç»´ç¨€ç–è¡¨ç¤ºã€‚

- [x] Learning Passage Impacts for Inverted Indexes

  > å…ˆç”¨DocT5Queryå¯¹æ–‡æ¡£è¿›è¡Œæ‰©å±•ï¼Œç„¶åå¯¹åŸæ–‡æ¡£çš„è¯å’Œæ‰©å±•è¯è¿›è¡Œterm impactçš„è®¡ç®—ã€‚

- [x] Pre-trained Language Model based Ranking in Baidu Search

  > we perform relevance-oriented pre-training using large-scale user behavioral data and design a tree-based calibration model to refine the noisy and biased clicking data.

- [x] GLOW : Global Weighted Self-Attention Network for Web Search

  > It learns semantic representation for both queries and documents by integrating global weight into attention score calculation.

- [x] Context-Aware Learning to Rank with Self-Attention

  > In this paper, we propose a context-aware neural network model that learns item scores by applying a self-attention mechanism.

- [x] **Distilling Knowledge from Reader to Retriever for Question Answering**

  > readerä¸­çš„attentionåˆ†å¸ƒä½œä¸ºretrieverçš„ç›‘ç£ä¿¡å·ã€‚

- [x] **End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering**

  > openQAçš„retrieverå’Œreaderè”åˆè®­ç»ƒã€‚

- [x] **Is Retriever Merely an Approximator of Reader?**

  > given that the retriever and the reader are complementary to each other, we propose to distill the reader into the retriever to combine the best of both worlds. Our distillation method signiï¬cantly enhances the recall rate of an existing retriever, which also translates into a non-trivial improvement in the end-to-end QA accuracy.

- [x] Efï¬cient Passage Retrieval with Hashing for Open-domain Question Answering

- [x] Whitening Sentence Representations for Better Semantics and Faster Retrieval

  > the whitening operation in traditional machine learning can similarly enhance the isotropy of sentence representations and achieve competitive results.

- [x] Learnt Sparsity for Effective and Interpretable Document Ranking

  > å¯¹document rankingï¼Œå…ˆé€‰å¥å­ï¼Œå†è®¡ç®—ç›¸å…³æ€§ã€‚

- [x] Pre-trained Language Model for Web-scale Retrieval in Baidu Searchï¼ˆKDDï¼‰

  > The system employs 1) an ERNIE-based retrieval model, 2) a multi-stage training paradigm and 3) a unified workflow for the retrieval system.

- [x] ã€coCondenserã€‘Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval

  > identify and address two underlying problems of dense retrievers: i) fragility to training data noise and ii) requiring large batches to robustly learn the embedding space.
  >
  > 1. ä½¿ç”¨Condenseræ¨¡å‹åšé¢„è®­ç»ƒï¼ˆé¢„è®­ç»ƒè¿‡ç¨‹ï¼šBERTåˆå§‹åŒ– + Condenser çš„é€šç”¨é¢„è®­ç»ƒ[MLMç›®æ ‡] + coCondenser çš„è¯­æ–™é¢„è®­ç»ƒ[MLM+å¯¹æ¯”å­¦ä¹ çš„ç›®æ ‡]ï¼‰ã€‚
  > 2. æå‡ºä¸€ç§æ˜¾å­˜å ç”¨é«˜æ•ˆçš„cross-batchè®­ç»ƒæ–¹æ³•ã€‚

- [x] ã€Condenserã€‘Is Your Language Model Ready for Dense Representation Fine-tuning? ï¼ˆEMNLP 2021ï¼‰

  > æå‡ºCondenseræ¨¡å‹ï¼ŒåŒ…æ‹¬earlyã€lateã€headä¸‰ä¸ªéƒ¨åˆ†ã€‚é¢„è®­ç»ƒç»“æŸåæŠŠheadéƒ¨åˆ†ä¸¢å¼ƒã€‚
  >
  > åœ¨STSã€openQAã€MARCO passageä¸‰ä¸ªä»»åŠ¡ä¸Šåšå®éªŒã€‚

- [x] On Single and Multiple Representations in Dense Passage Retrieval

  > æ¯”è¾ƒäº†ä¸€äº›å•è¡¨è¾¾ï¼ˆANCEï¼‰å’Œå¤šè¡¨è¾¾æ¨¡å‹ï¼ˆColBERTï¼‰ï¼Œå‘ç°å¯¹äºéš¾çš„queryï¼ˆhardest for BM25ï¼Œdefinitional queriesï¼Œcomplex information needs.ï¼‰ï¼Œå¤šè¡¨è¾¾æ¨¡å‹æ›´æœ‰ç”¨ã€‚

- [x] Domain-matched Pre-training Tasks for Dense Retrieval

  > ä½¿ç”¨in-batch lossåœ¨ä¸€äº›ä»»åŠ¡ç±»å‹éå¸¸æ¥è¿‘çš„å¤§æ•°æ®é›†ï¼ˆPAQã€Redditï¼‰ä¸Šç»§ç»­é¢„è®­ç»ƒï¼Œç„¶åå†ä¸‹æ¸¸ä»»åŠ¡ï¼ˆMARCOã€OpenQAã€dialogueï¼‰å¾®è°ƒã€‚

- [x] ã€Pre-Rankã€‘Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm

  > åŒæ—¶è€ƒè™‘user viewï¼ˆç”¨click logè¿›è¡Œé¢„è®­ç»ƒï¼‰å’Œsystem viewï¼ˆç”¨ä¸“å®¶æ ‡æ³¨çš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼‰çš„ç›¸å…³æ€§ã€‚

- [x] ã€ST5ã€‘Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models

  > We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model.

- [x] Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls

  > we address this gap by investigating methods for integrating PRF signals into **rerankers** and dense **retrievers** based on deep language models. 
  >
  > We consider **text-based** and **vector-based** PRF approaches, and investigate different ways of combining and scoring relevance signals.
  >
  > Text-based PRFä¼šå¸¦æ¥å¤ªå¤§çš„æ•ˆç‡æŸå¤±ï¼Œvector-based PRFåœ¨æ•ˆç‡æ–¹é¢ä¸ä¼šæœ‰å¤ªå¤§ä»£ä»·ã€‚

- [x] Dealing with Typos for BERT-based Passage Retrieval and Ranking

  > MS MARCO passage rankingæ•°æ®é›†ä¸­å­˜åœ¨æ‹¼å†™é”™è¯¯ã€‚

- [x] Shallow pooling for sparse labels

- [x] **SEED-Encoder: Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder**ï¼ˆEMNLP2021ï¼‰

  > we propose a self-learning task that employs an auxiliary decoder that learns to reconstruct the input from the pre-training encoderâ€™s [CLS] embeddings. we deliberately make the decoder weaker by restricting its parameter capacity and attention ï¬‚exibility.
  >
  > SEED-Encoder indeed obtains more encoding power by allocating less capacity to the decoder.

- [x] Exploiting Sentence-Level Representations for Passage Ranking

  > we explicitly model the sentence-level representations by using Dynamic Memory Networks (DMNs) and conduct empirical evaluation to show improvements in passage re-ranking over fine-tuned vanilla BERT models by memory-enhanced explicit sentence modelling on a diverse set of open-domain QA datasets.

- [x] Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers

  > 1. the shape of models plays an important role on how it performs on the target task and the performance is not merely a function of parameter size.
  > 2. pretraining perplexity is not indicative of transfer performance

- [x] Serverless BM25 Search and BERT Reranking

  > we introduce a serverless prototype of the retrieveâ€“rerank pipeline for search using Amazon Web Services (AWS), comprised of BM25 for first-stage retrieval using Lucene followed by reranking with the monoBERT model using Hugging Face Transformers. 

- [x] SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval

  > we build on SPLADE and propose several significant improvements in terms of effectiveness and/or efficiency. More specifically, we modify the pooling mechanism, benchmark a model solely based on document expansion, and introduce models trained with distillation.

- [x] Unsupervised Open-Domain Question Answering

  > we ï¬rst propose the task of Unsupervised Open-domain Question Answering, and explore to what extent it can perform based on our suggested data construction methods.

- [x] Combining Lexical and Dense Retrieval for Computationally Efï¬cient Multi-hop Question Answering

  > we provided insights on the performance of state-of-the-art dense retrieval for multihop questions. We showed that Rerank+DPR 2 (our hybrid model) outperforms MDR (the state-of-the-art multi-hop dense retrieval model) in the low resource setting,

- [x] YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement Ranker

  > we propose a ï¬ne-tuning strategy for document ranking, namely Self-Involvement Ranker (SIR), to dynamically select hard negative samples to construct high-quality semantic space for training a high-quality ranking model.

- [x] Relation-Guided Pre-Training for Open-Domain Question Answering

  > we propose a simple yet effective pre-training framework RGPT-QA. We leverage both the Wikipedia hyperlinks and Wikidata relation triplets to construct Grounded Relational WikiGraph, based on which we generate relational QA dataset. We then pre-train a QA model to infer the latent relation from the question, and then conduct extractive QA to get the target answer entity.

- [x] A Semantic Indexing Structure for Image Retrieval

  > The SIS provide a new way to combine high-level semantic information and low-level texture information. SIS solves some problems other ANN searching methods bring, which result from the dimension limitation of other Index.

- [x] Keyword Extraction for Improved Document Retrieval in Conversational Search

- [x] Born Again Neural Rankers

  > We propose Born Again neural Rankers (BAR) and further push the limits of neural ranker eï¬€ectiveness without sacriï¬cing eï¬ƒciency. We show that the key success factors of BAR lie in a proper teacher score transform and a listwise distillation approach speciï¬cally designed for ranking problems

- [x] A Proposed Conceptual Framework for a Representational Approach to Information Retrieval

  > The conceptual framework for text retrieval presented in this paper allow us to â€œtie togetherâ€ recent work in dense and sparse retrieval, along with multi-stage ranking architectures, and to understand them not as disjoint innovations, but different aspects of the same underlying research program that has remained hidden until now.
  
-  [x] Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question Answering
  
  > we examine different combinations of DPRâ€™s question encoders and passage encoders trained on ï¬ve different QA datasets, which we refer to as â€œencoder adaptationâ€. we observe that the passage encoder affects the generalization lower bound more while the question encoder seems to play a more vital role in the generalization upper bound.
  
- [x] How Does BERT Rerank Passages? An Attribution Analysis with Information Bottlenecks

  > we leverage information bottleneck for attribution (IBA) method to examine BERT for reranking. 
  >
  > 1) We compare ranking mechanisms between BM25 and BERT, ï¬nding that BERT still values token matching, and it also learns deeper relevance between queries and passages. 
  > 2) We further analyze special tokens across layers and demonstrate patterns that [CLS] aggregate evidence. 
  > 3) We then investigate the robustness of top-ranked passages. 
  > 4) Finally, we ï¬nd that BERT ï¬ne-tuned on MSMARCO has positional bias towards the start of the passage.

- [x] Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval

  > we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efï¬ciency, we propose a simple unsupervised compression pipeline that consists of principal component analysis (PCA), product quantization, and hybrid search.

- [x] **Adversarial Retriever-Ranker for dense text retrieval**

  > we present Adversarial RetrieverRanker (AR2), which consists of a dual-encoder retriever plus a cross-encoder ranker. The two models are jointly optimized according to a minimax adversarial objective: the retriever learns to retrieve negative documents to cheat the ranker, while the ranker learns to rank a collection of candidates including both the ground-truth and the retrieved ones, as well as providing progressive direct feedback to the dual-encoder retriever.

- [x] Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answeringï¼ˆEMNLP 2021 findingï¼‰

  > we propose to train individual dense passage retrievers (DPR) for different tasks and aggregate their predictions during test time, where we use uncertainty estimation as weights to indicate how probable a speciï¬c query belongs to each expertâ€™s expertise.

- [x] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

- [x] Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation

  > we propose an Unsupervised Document Expansion with Generation (UDEG) framework with a pretrained language model, which generates diverse supplementary sentences for the original document without using labels on querydocument pairs for training. 
  >
  > For generating sentences, we further stochastically perturb their embeddings to generate more diverse sentences for document expansion.

- [x] Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations

  > we aim to improve the generalization ability of DR models from source training domains with rich supervision signals to target domains without any relevant labels, in the zero-shot setting.
  >
  > we propose Momentum adversarial Domain Invariant Representation learning (MoDIR), which introduces a momentum method in the DR training process to train a domain classiï¬er distinguishing source versus target, and then adversarially updates the DR encoder to learn domain invariant representations.
  
- [x] Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?

  > we show that it is indeed possible to mimic a given sparse retriever (e.g., BM25 or UniCOIL) with a dense model Î›, and we build the SPAR model by combining Î› with a standard dense retriever(e.g., DPR or ANCE).
  >
  > åˆ©ç”¨BM25æ£€ç´¢çš„topç»“æœæ„é€ æ­£è´Ÿä¾‹å¯¹ï¼Œè®­ç»ƒdense retrievalï¼Œæ¥è¾¾åˆ°æ¨¡ä»¿çš„æ•ˆæœã€‚

- [x] Embracing Structure in Data for Billion-Scale Semantic Product Search

  > we model dyadic data as a bipartite graph with edges between positively associated pairs. We partition the nodes of this graph into balanced clusters by approximately minimizing edge-cuts. Given this partitioned graph, we can speed up training and inference.

- [x] Fast Forward Indexes for Efficient Document Ranking

  > Using interpolation, we observe increased performance compared to hybrid retrieval and up to 75% improvements in query processing latency and memory footprint due to our optimization techniques, sequential coalescing and early stopping.

- [x] Representation Decoupling for Open-Domain Passage Retrieval

  > The cause of the Contrastive Conï¬‚icts lies in the fact that each passage is always composed of several sentences, while these sentences may not always stick to the same topic.
  >
  > we propose to decouple the passage representation into contextual sentence-level ones for better retrieval performance. We reï¬ne the original CL framework by creating sentence-aware positive and negative samples.

- [x] BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models

  > æå‡ºäº†ä¸€ä¸ªä¿¡æ¯æ£€ç´¢benchmarkç”¨äºzrro-shotè¯„ä¼°ï¼ŒåŒ…æ‹¬10ä¸ªæ•°æ®é›†ã€‚
  >
  > æ¯”è¾ƒäº†BM25ã€sparseã€denseï¼ŒLate-interaction, rerankæ¨¡å‹ï¼ˆå…±10ä¸ªmodelï¼‰çš„è¡¨ç°ã€‚
  >
  > åˆ†æäº†ç›®å‰çš„æ•°æ®é›†å­˜åœ¨çš„lexical bias

- [x] FiD: Leveraging passage retrieval with generative models for open domain question answering

  > building generative seq2seq model T5 on top of DPR retrieved passages.
  >
  > The key beneï¬t of FiD is that the answer decoder can efï¬ciently fuse the information from multiple passages. In detail, it separately encodes retrieved passages, each appended with question, and then concatenates all resulting passage token embeddings to send to the decoder.

- [x] KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering

  >Our reranking model reuses the passage representation generated by the reader encoder and apply graph neural networks to compute reranking scores. We propose to use the intermediate layer of encoder to reduce computation cost while maintaining good performance.

- [ ] ~~ï¼ˆæ¨¡å‹ä¸å¯çŸ¥çš„æ£€ç´¢ç»“æœçš„å¯è§£é‡Šæ€§ï¼‰Explaining Documentsâ€™ Relevance to Search Queries~~

  > we propose GenEx, an approach that generates terse explanations on the level of noun phrases â€“ for search results describing what aspect of the query is covered by a retrieved document.
  >
  > we cast the task of search result explanation as a sequence transduction problem, where an attention-based encoder-decoder architecture first provides a query-focused contextual representation of a document and then generates desired explanations.

- [x] Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey

- [x] Knowledge Inheritance for Pre-trained Language Models

  > loss = è‡ªå­¦ä¹ loss + çŸ¥è¯†ç»§æ‰¿lossï¼ˆè’¸é¦ï¼‰
  >
  > ä½¿ç”¨åŠ¨æ€çš„ç»§æ‰¿æ¯”ä¾‹ï¼ˆä¸¤ä¸ªlossä¹‹é—´çš„æ¯”ä¾‹è¶…å‚æ•°éšè®­ç»ƒæ­¥æ•°å‘ç”Ÿå˜åŒ–ï¼‰

- [x] Pre-Trained Models: Past, Present and Future

Ë™


# ï¼ˆå®Œæˆï¼‰SIGIR2018

- [x] ï¼ˆIRåŒ¹é…ï¼‰Modeling Diverse Relevance Patterns in Ad-hoc Retrievalï¼ˆèŒƒæ„å…´å¸ˆå…„ï¼‰

- [x] ï¼ï¼ï¼ˆæ’åºï¼‰Learning a Deep Listwise Context Model for Ranking Refinement

- [x] ï¼ï¼ï¼ˆç‚¹å‡»æ¨¡å‹ï¼‰A Click Sequence Model for Web Search

- [x] ï¼ï¼ï¼ˆå¤šè·³é—®ç­”åŒ¹é…ï¼‰Multihop Attention Networks for Question Answer Matching

- [x] ï¼ï¼ï¼ˆå¯¹è¯ç³»ç»Ÿçš„responseæ’åºï¼‰Response Ranking with Deep Matching Networks and External Knowledge in Information-seeking Conversation Systems

- [x] ï¼ï¼ï¼ˆå¥å­ç›¸ä¼¼åº¦å»ºæ¨¡ã€å¯¹æŠ—å­¦ä¹ ï¼‰CAN: Enhancing Sentence Similarity Modeling with Collaborative and Adversarial Network

- [x] ï¼ï¼ï¼ˆCQAçš„ç›¸ä¼¼é—®é¢˜è¯†åˆ«ï¼‰Related or Duplicate: Distinguishing Similar CQA Questions via Convolutional Neural Networks

- [x] ï¼ï¼ï¼ˆQAæ’åºã€çŸ¥è¯†åº“ï¼‰Knowledge-aware Attentive Neural Network for Ranking Question Answer Pairs

- [x] ï¼ï¼ï¼ˆèåˆçŸ¥è¯†åº“ï¼‰Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling

- [x] ï¼ˆTREC-CARä»»åŠ¡ï¼Œå¯¹question pathè¿›è¡Œåˆ†æ®µå¤„ç†ï¼‰Characterizing Question Facets for Complex Answer Retrieval

- [x] ï¼ˆæ— ç›‘ç£çš„QAä»»åŠ¡çš„baselineï¼‰Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering

- [x] ï¼ï¼ï¼ˆIRï¼‰Multi-level Abstraction Convolutional Model with Weak Supervision for Information Retrieval

- [ ] ï¼ï¼ï¼ˆç‚¹å‡»ç›¸å…³æ€§æ•°æ®é›†ï¼‰**Sogou-QCL: A New Dataset with Click Relevance Label**

- [ ] ï¼ï¼ï¼ˆQAæ•°æ®é›†ï¼‰**WikiPassageQA: A Benchmark Collection for Research on Non-factoid Answer Passage Retrieval**

- [x] ï¼ï¼ï¼ˆå¯¹è¯å’Œç”¨æˆ·æ„å›¾åˆ†æçš„æ•°æ®é›†ï¼‰Analyzing and Characterizing User Intent in Information-seeking Conversations

  

- [ ] ï¼ï¼ï¼ˆæ–‡æœ¬ç”Ÿæˆå¹³å°ï¼‰Texygen: A Benchmarking Platform for Text Generation Models

- [ ] XXXï¼ˆIRï¼‰Modeling multidimensional user relevance in IR using vector spaces

- [ ]  XXXï¼ˆIRï¼‰Are we on the Right Track? Integrating Theoretical and Empirical Methodologies for Information Retrieval

- [ ]  XXXï¼ˆæ¦‚ç‡ä¿¡æ¯æ£€ç´¢æ¨¡å‹ï¼‰A New Term Frequency Normalization Model for Probabilistic Information Retrieval

- [ ] XXXï¼ˆL2Rï¼‰Universal Approximation Functions for Fast Learning to Rank

- [ ] XXX (æœç´¢çš„ç‚¹å‡»æ¨¡å‹) Constructing Click Models for Mobile Search

- [ ] XXXï¼ˆç”µå•†æœç´¢ä¸­çš„é—®é¢˜åˆ†ç±»ï¼‰A taxonomy of queries for e-commerce search

- [ ] XXXï¼ˆé—®ç­”ï¼‰Characterizing and Supporting Question Answering in Human-to-Human Communication

- [ ] XXX (æœç´¢ç»“æœçš„å¤šæ ·æ€§) From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks ï¼ˆå¾å›è€å¸ˆï¼‰

- [ ] Predicting User Knowledge Gain in Informational Search Sessions

- [ ] ï¼ˆæ’åºçš„é²æ£’æ€§ï¼‰Ranking Robustness under Adversarial Document Manipulations

- [ ] ï¼Ÿï¼Ÿï¼ˆä¸€ä¸ªä¿¡æ¯æ£€ç´¢æ¡†æ¶ï¼‰An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings

- [ ] ï¼Ÿï¼ŸDeep Semantic Text Hashing with Weak Supervision

- [ ] ï¼Ÿï¼Ÿï¼ˆæ’åºä¸­çš„å…¬å¹³æ€§ï¼‰Equity of Attention: Amortizing Individual Fairness in Rankings

- [ ] ï¼Ÿï¼ŸGenerating Better Queries for Systematic Reviews

- [ ] ï¼Ÿï¼ŸNeural Compatibility Modeling with Attentive Knowledge Distillation

- [ ] ï¼Ÿï¼ŸMeasuring the Utility of Search Engine Result Pages

- [ ] ï¼Ÿï¼ŸRanking Documents by Answer-Passage Quality

- [ ] ï¼Ÿï¼Ÿï¼ˆç›¸å…³æ€§çš„ç†è®ºï¼‰What Can Rationales behind Relevance Judgments Tell Us About Assessor Disagreement?

- [ ] ï¼Ÿï¼Ÿï¼ˆç›¸å…³æ€§çš„ç†è®ºï¼‰Testing the Cluster Hypothesis with Focused and Graded Relevance Judgment

- [ ] ï¼Ÿï¼ŸNeural Query Performance Prediction using Weak Supervision from Multiple Signals

- [ ] ï¼Ÿï¼ŸQuery Performance Prediction using Passage Information

- [ ] ï¼Ÿï¼ŸQuery Performance Prediction Focused on Summarized Letor Features

- [ ] ï¼Ÿï¼ŸQuery Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin

- [ ] ï¼Ÿï¼ŸQuery Variation Performance Prediction for Systematic Reviews

- [ ] ï¼Ÿï¼ŸEfficient Exploration of Gradient Space for Online Learning to Rank

- [ ] ï¼Ÿï¼ŸSelective Gradient Boosting for Effective Learning to Rank

- [ ] ï¼Ÿï¼ŸItem Retrieval as Utility Estimation

- [ ] ï¼Ÿï¼ŸAttention-based Hierarchical Neural Query Suggestion

- [ ] ï¼Ÿï¼ŸOptimizing Query Evaluations using Reinforcement Learning for Web Search

- [ ] ï¼ˆnetwork embeddingï¼‰BiNE: Bipartite Network Embedding

- [ ] Translating Representations of Knowledge Graphs with Neighbors





# ï¼ˆå®Œæˆï¼‰SIGIR2019

- [x] ï¼ˆCQAä¸­çš„é‡å¤é—®é¢˜æ£€æµ‹ï¼‰Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection

  > åŠ¨æœºï¼špaired answers can provide effective information for duplicate question detection while they may simultaneously introduce noise to the detection.
  >
  > æ–¹æ³•ï¼šWe propose an adaptive multi-attention network (AMAN), an effective method integrating external knowledge from answers for duplicate identification and filtering out the noise introduced by paired answers adaptively.

- [x] Controlling Risk of Web Question Answering

- [ ] ï¼ˆäººç±»è¡Œä¸ºï¼‰Human Behavior Inspired Machine Reading Comprehension

- [x] ï¼ˆäººç±»è¡Œä¸ºï¼‰Teach Machine How to Read: Reading Behavior Inspired Relevance Estimation

  > ç”±äººç±»é˜…è¯»çš„6æ¡å¯å‘å¼è§„åˆ™è®¾è®¡æ£€ç´¢æ¨¡å‹ã€‚

- [x] ï¼ˆäººç±»è¡Œä¸ºï¼‰Investigating Passage-level Relevance and Its Role in Document-level Relevance Judgment

  > å¦‚ä½•ç”±passage-levelçš„ç›¸å…³æ€§å¾—åˆ°document-levelçš„ç›¸å…³æ€§ã€‚

- [x] CEDR: Contextualized Embeddings for Document Ranking

  > æŠŠBERTå¾—åˆ°çš„contextual embedding èåˆåˆ°ç°æœ‰çš„neural ranking modelsï¼ˆKNRMã€DRMMã€PACRRï¼‰

- [x] Content-Based Weak Supervision for Ad-Hoc Re-Ranking

  > We presented an approach for employing content-based sources of pseudo relevance for training neural IR models.
  >
  > We also showed that performance can be boosted using two ï¬ltering techniques: one heuristic-based and one that re-purposes a neural ranker

- [x] Deeper Text Understanding for IR with Contextual Neural Language Modeling

  > ç”¨bing search logæ•°æ®å¾®è°ƒBertï¼Œå†åœ¨Robustå’ŒCluWebä¸Šè¿›è¡Œrerank BoW top-100çš„å®éªŒã€‚
  >
  > ä½¿ç”¨Bertæ—¶æŠŠdocumentåˆ‡åˆ†æˆpassageï¼Œå†ä½¿ç”¨BERT-FirstPã€BERT-MaxPã€BERT-SumPä¸‰ç§ç»„åˆæ–¹å¼ã€‚

- [x] FAQ Retrieval Using Attentive Matching

  > æœ¬æ–‡æå‡ºäº†å¤šç§æ–¹æ³•è§£å†³FAQæ£€ç´¢ã€‚We compared various possible aggregation methods to eï¿¿ectively represent query, question and answer information, and observed that answers in FAQs can provide valuable and beneï¿¿cial information for retrieval models, if properly aggregated.
  > è¡¨æ˜äº†åœ¨FAQæ£€ç´¢ä¸­ä¹Ÿå¯ä»¥ä½¿ç”¨answerçš„ä¿¡æ¯ã€‚

- [x] FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance

  > This paper presented a method for using TSUBAKI-based query-question similarity and BERT-based query-answer relevance in a FAQ retrieval task.

- [x] History Modeling for Conversational Question Answering

  > æå‡ºäº†ä¸€ä¸ªæ¨¡å‹ for å¯¹è¯å¼QAã€‚

- [x] Learning More From Less: Towards Strengthening Weak Supervision for Ad-Hoc Retrieval

  > presented two approaches(noise-aware model å’Œ influence-aware model) to reduce the amount of weak data needed to surpass the performance of the unsupervised method that generates the training data.

- [x] ã€MMNã€‘Multi-level Matching Networks for Text Matching

  > åŠ¨æœºï¼šA major limitation of existing works is that only high level contextualized word representations are utilized to obtain word level matching results without considering other levels of word representations, thus resulting in incorrect matching decisions for cases where two words with different meanings are very close in high level contextualized word representation space.
  >
  > æ–¹æ³•ï¼šinstead of making decisions utilizing single level word representations, a multi-level matching network (MMN) is proposed in this paper for text matching, which utilizes multiple levels of word representations to obtain multiple word level matching results for final text level matching decision.

- [x] Document Gated Reader for Open Domain Question Answeringï¼ˆè®¡ç®—æ¯ä¸ªæ–‡æ¡£ä¸queryçš„ç›¸ä¼¼åº¦æ—¶è€ƒè™‘å…¶ä»–æ–‡æ¡£çš„ä¿¡æ¯ï¼›å°†æ–‡æ¡£çš„ç›¸ä¼¼åº¦ä¸answer spançš„æ¦‚ç‡ç›¸ä¹˜ï¼Œåœ¨å…¨å±€è¿›è¡Œnormalizeï¼›ç”¨bootstrappingæ•°æ®ç”Ÿæˆæœºåˆ¶è§£å†³è¿œç¨‹ç›‘ç£å¸¦æ¥çš„å‡æ­£ä¾‹é—®é¢˜ï¼‰

- [x] An axiomatic approach to regularizing neural ranking models

  > ä½¿ç”¨ä¸€äº›å…¬ç†å¯¹documentè¿›è¡Œæ‰°åŠ¨ï¼Œä½¿å¾—æ‰°åŠ¨åçš„æ–‡æœ¬æ¯”åŸæ–‡æœ¬æ›´ç›¸å…³æˆ–è€…æ›´ä¸ç›¸å…³ï¼Œä»è€Œå¾—åˆ°æ›´å¤šçš„ç›‘ç£ä¿¡å·ï¼Œå¸®åŠ©æ¨¡å‹çš„è®­ç»ƒã€‚

- [x] Report on the First HIPstIR Workshop on the Future of Information Retrieval

  > ä¸€ä¸ªworkshopï¼Œé‡Œé¢æé«˜å’Œâ€œæ£€ç´¢æ•ˆç‡â€ç›¸å…³çš„å‡ ç‚¹åŒ…æ‹¬ï¼š1. æ¢ç´¢å¦‚ä½•æŠŠbertç”¨äºé«˜æ•ˆçš„fullrankingï¼›2. æ„å»ºå’Œæ£€ç´¢æ•ˆç‡æœ‰å…³çš„benchmarkï¼›3. æ¢ç´¢æ–°çš„æ•°æ®ç»“æ„ï¼Œ e.g. å¼•å…¥ç‹¬ç«‹æ€§å‡è®¾é‚£ç¯‡æ–‡ç« ï¼ˆå¯ä»¥ç»§ç»­åšçš„å·¥ä½œåŒ…æ‹¬ï¼Œ1ï¼‰æ„å»ºå¤šå±‚æ¬¡çš„ç´¢å¼•ï¼Œtermã€phraseã€queryï¼›2ï¼‰åŸºäºå¼ºåŒ–å­¦ä¹ çš„index scanç­–ç•¥ï¼‰ï¼›4. åœ¨è®¾è®¡deep modelæ—¶è€ƒè™‘å¯ä»¥æ”¯æŒé«˜æ•ˆæ£€ç´¢çš„æ•°æ®ç»“æ„ï¼Œ e.g. SNRM

- [ ] Letâ€™s measure run time! Extending the IR replicability infrastructure to include performance aspects.

  >æå‡ºåº”è¯¥é‡è§†NeuIRçš„æ•ˆç‡é—®é¢˜ï¼Œå¹¶æå‡ºä¸¤ä¸ªbenchmark/dockerå¯ä»¥å…¬å¹³å¯¹æ¯”ä¸åŒçš„æ¨¡å‹ï¼ŒOpen-Source IR Replicability Challengeï¼‰

- [ ] 

  

  

# ï¼ˆå®Œæˆï¼‰SIGIR2020

- [x] ï¼ˆTKLï¼‰Local Self-Attention over Long Text for Efficient Document Retrieval

  > å¯¹Transformer-kernelçš„æ”¹è¿›ï¼Œä¸»è¦æ˜¯é’ˆå¯¹Transformer-kernelç”¨äºé•¿æ–‡æœ¬æ—¶ã€‚ä¹‹å‰çš„åŸºäºTransformerçš„æ¨¡å‹ç”¨äºé•¿æ–‡æœ¬æ—¶é€šå¸¸çš„åšæ³•æ˜¯ç›´æ¥æˆªæ–­ã€‚æœ¬æ–‡åœ¨Transformer-kernelæ¨¡å‹çš„åŸºç¡€ä¸Šæå‡ºäº†local self-attentionã€‚å¹¶ä¸”è®¡ç®—å‡ºæ¯ä¸ªåŒºåŸŸçš„ç›¸å…³åº¦ä¹‹åå†ä½¿ç”¨top-local-maxæœºåˆ¶ç»„åˆå¾—åˆ°å…¨å±€ç›¸å…³åº¦ã€‚
  > æ•°æ®é›†ï¼šMARCOçš„document retrievalæ•°æ®é›†

- [x] **Training Curricula for Open Domain Answer Re-Ranking**

  > for passage re-ranking é˜¶æ®µã€‚
  >
  > æå‡ºè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼Œæ”¹è¿›BERTå’ŒConv-KNRMçš„è®­ç»ƒè¿‡ç¨‹ï¼Œåˆšå¼€å§‹ç»™å®¹æ˜“çš„passageæ›´å¤§çš„æƒé‡ï¼Œç»™éš¾çš„passageæ›´å°çš„æƒé‡ï¼ŒåæœŸç»™ç›¸åŒçš„æƒé‡ã€‚

- [x] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT

- [x] ã€PreTTRã€‘Efficient Document Re-Ranking for Transformers by Precomputing Term Representations

- [ ] SetRank: Learning a Permutation-Invariant Ranking Model for Information Retrieval

- [x] Open-Retrieval Conversational Question Answering

  > æå‡ºäº†ä¸€ä¸ªå¼€æ”¾åŸŸå¯¹è¯å¼QAæ•°æ®é›†ï¼šOR-QuACã€‚
  >
  > æå‡ºäº†ä¸€ä¸ªbaselineï¼ŒåŒ…æ‹¬retrieverã€re-rankerï¼Œreaderã€‚

- [x] Match$^2$: A Matching over Matching Model for Similar Question Identification

- [x] **MarkedBERT: Integrating Traditional IR cues in pre-trained language models for passage retrieval**

  > We proposed MarkedBERT that incorporates Exact Matching signals via a simple yet effective marking technique that only modifies the model input. 

- [x] Context-Aware Term Weighting For First-Stage Passage Retrieval

- [x] Learning Term Discrimination

- [x] Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation

  > å¯¹BERTç±»çš„ä¸Šä¸‹æ–‡é¢„è®­ç»ƒæ¨¡å‹æå‡ºé’ˆå¯¹å¤šè½®å¯¹è¯ä»»åŠ¡çš„ä¸¤ç§æ”¹è¿›ç­–ç•¥ï¼š1ï¼‰Speaker Segmentationï¼›2ï¼‰Dialogue Augmentation

- [x] Large-scale Image Retrieval with Sparse Binary Projections

- [x] ã€EPICã€‘**Expansion via Prediction of Importance with Contextualization**

  > æŠŠqueryè¡¨ç¤ºæˆç¨€ç–å‘é‡ï¼Œdocumentè¡¨ç¤ºæˆdenseå‘é‡ï¼Œè®¡ç®—ç‚¹ç§¯ã€‚

- [x] Efficiency Implications of Term Re-Weighting for Passage Retrieval

- [x] DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding

- [x] Improving Matching Models with Hierarchical Contextualized Representations for Multi-turn Response Selection

- [x] **Distilling Knowledge for fast retrieval-based chat-bots**

  > we introduced an enhanced BERT cross-encoder architecture modiï¬ed for the task of response retrieval. Alongside that, we utilized knowledge distillation to compress the complex BERT cross-encoder network as a teacher model into the student BERT bi-encoder model. This increases the BERT bi-encoders prediction quality without aï¬€ecting its inference speed.

- [x] **Having Your Cake and Eating it Too: Training Neural Retrieval for Language Inference without Losing Lexical Match**

  > We presented a simple approach to infuse lexical matching using unsupervised IR methods into a state-of-the-art transformer method - RoBERTa. We show that infusing lexical-matching improves the performance on simpler retrieval based question and the (justification) retrieval task itselfã€‚

- [x] An analysis of BERT in document ranking

  > What Does BERT Look At? An Analysis of BERTâ€™s Attention.
  >
  > How Contextual are Contextualized Word Representations.
  >
  > Understanding the Behaviors of BERT in Rankingï¼ˆåˆ˜çŸ¥è¿œï¼‰

- [x] Read, Attend, and Exclude:  Multi-Choice Reading Comprehension by Mimicking Human Reasoning Process

  > æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å¤šé€‰å‹é˜…è¯»ç†è§£ä»»åŠ¡çš„æ¨¡å‹ã€‚

- [x] Unsupervised Text Summarization with Sentence Graph Compression

  > å¯¹å¤šç¯‡æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ã€‚



# ï¼ˆå®Œæˆï¼‰SIGIR2021

- [ ] ~~Optimizing Training of Pre-trained Rerankers in Multi-Stage Dense Retrievalï¼ˆæœªå½•ç”¨ï¼‰~~

  > æå‡ºã€åŠ¨æ€å¤šç²’åº¦ã€‘çš„è®­ç»ƒæ–¹æ³•ï¼Œè§£å†³ä¸å……åˆ†çš„æ­£ä¾‹æ ‡æ³¨é—®é¢˜ã€‚

- [x] ~~Dense Representation for Few-sample Knowledge Retrievalï¼ˆæœªå½•ç”¨ï¼‰~~

  > æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®å¯¹è®­ç»ƒdense retrieverçš„é—®é¢˜ã€‚

- [x] Learning Robust Dense Retrieval Models from Incomplete Relevance Labels

  > è§£å†³è´Ÿæ ·æœ¬æ ‡æ³¨ä¸å……åˆ†çš„é—®é¢˜ï¼Œä½¿ç”¨å…·æœ‰å®Œæ•´ç›¸å…³æ€§æ ‡æ³¨çš„å¼€å‘é›†ï¼ˆe.g., TREC DLï¼‰æ¥è¯„ä¼°è´Ÿæ ·æœ¬åˆ†å¸ƒï¼Œä»è€Œå¸®åŠ©dense retrievalæ¨¡å‹çš„è®­ç»ƒã€‚

- [ ] ~~Analysing Dense Passage Retrieval for Multi-Hop Questionsï¼ˆæœªå½•ç”¨ï¼‰~~

  > åˆ†ædense retrievalæ¨¡å‹åœ¨å¤šè·³é—®ç­”ä¸ŠæˆåŠŸçš„åŸå› ã€‚

- [x] SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking

  > åœ¨SparTermçš„åŸºç¡€ä¸Šï¼Œæå‡ºexplicit sparsity regularization and a log-saturation effect on term weightsã€‚
  
- [ ] ã€æ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ã€é£é™©æ€§ã€‘Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Modelsï¼ˆæ•°å­¦å…¬å¼ï¼Œçœ‹ä¸å¤ªæ‡‚ï¼‰

  > we introduced an efficient Bayesian framework to estimate epistemic and aleatoric uncertainty in retrieval models.
  >
  > The performance of these stochastic models stays reasonably close to their deterministic versions while offering substantially more information per document score. Furthermore, the actual scores themselves are better calibrated with each other allowing for a more accurate comparison between documents.

- [x] Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking

  > we adopt an intra-document cascading strategy, which prunes passages of a candidate document using a less expensive model, called ESM, before running a scoring model that is more expensive and effective, called ETM. We found it best to train ESM (short for Efficient Student Model) via knowledge distillation from the ETM (short for Effective Teacher Model) e.g., BERT.

- [x] Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling

- [x] B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval

- [x] Few-Shot Conversational Dense Retrieval

- [x] Optimizing Dense Retrieval Model Training with Hard Negatives

- [x] **Binary Neural Network Hashing for Image Retrieval**

  > we propose a novel deep hashing method, called Binary Neural Network Hashing (BNNH) for fast image retrieval.

- [ ] ~~Answer Complex Questions: Path Ranker Is All You Need~~ï¼ˆå¤šè·³é—®ç­”ï¼Œæœªçœ‹ï¼‰

- [ ] ~~Answering Any-hop Open-domain Questions with Iterative Document Reranking~~ï¼ˆä»»æ„è·³ï¼ˆä¸€è·³å’Œå¤šè·³ï¼‰çš„é—®ç­”ï¼Œæœªçœ‹ï¼‰

- [x] TILDE: Term Independent Likelihood moDEl for Passage Re-ranking

  > åœ¨åŸºäºBERTçš„æŸ¥è¯¢ä¼¼ç„¶è¯­è¨€æ¨¡å‹ä¸­å¼•å…¥æŸ¥è¯¢è¯çš„ç‹¬ç«‹æ€§å‡è®¾ã€‚
  >
  > æŠŠdocumentè¾“å…¥åˆ°BERTï¼Œé€šè¿‡CLSè®¡ç®—ç”Ÿæˆqueryä¸­å‡ºç°çš„è¯çš„æ¦‚ç‡ï¼Œå¹¶é€šè¿‡æœ€å¤§åŒ–è¯¥æ¦‚ç‡æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ä»è€Œå¯ä»¥ç”¨documentè®¡ç®—ç”Ÿæˆæ¯ä¸ªè¯çš„æ¦‚ç‡ï¼Œæå‰å­˜å‚¨ï¼ŒèŠ‚çœåœ¨çº¿æ£€ç´¢æ—¶é—´ã€‚
  >
  > åç»­å·¥ä½œï¼šFast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansionï¼ˆå…ˆç”¨TILDEæ¨¡å‹è¿›è¡Œpassage expansionï¼Œç„¶åå¯¹queryä½¿ç”¨TFè¡¨ç¤ºï¼Œå¯¹docä¸­æ¯ä¸ªè¯çš„BERT embeddingæ˜ å°„åˆ°å•ä¸ªå€¼ï¼Œä¸queryå‘é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼‰


- [x] Learning Early Exit Strategies for Additive Ranking Ensembles

  > åœ¨LTRåœºæ™¯ä¸­åº”ç”¨æ—©é€€æœºåˆ¶ã€‚åŸºç¡€æ¨¡å‹æ˜¯é›†æˆæ ‘çš„ğœ†-Martã€‚

- [x] Contextualized Offline Relevance Weighting for Efficient and Effective Neural Retrieval

  > the neighbour documents of selected seed documents are quickly re-ranked using pre-computed relevance and pseudo-queries.

- [x] Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation

- [x] Text-to-Text Multi-view Learning for Passage Re-ranking

  > åœ¨T5çš„åŸºç¡€ä¸Šï¼ŒåŒæ—¶è®­ç»ƒpassage rerankingå’Œquery generationã€‚ä»è€Œæå‡passage rerankingçš„æ€§èƒ½ã€‚ï¼ˆæå‡å¾ˆå°ï¼‰

- [x] ã€CCSAã€‘Composite Code Sparse Autoencoders for First Stage Retrieval

  > åœ¨Dense retrieverçš„åŸºç¡€ä¸Šå†åŠ ä¸€ä¸ªè‡ªç¼–ç å™¨ï¼Œä»è€Œå°†ç¨ å¯†è¡¨è¾¾è½¬åŒ–æˆç¨€ç–ç¼–ç ã€‚ç›®çš„æ˜¯æé«˜cpu-basedçš„æœç´¢å»¶è¿Ÿã€‚
  >
  > å…³é”®ç»„ä»¶ï¼šComposite code + Gumbel-softmax activation + Uniformity regularizer

- [x] ã€KeyBLDã€‘KeyBLD: Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval

  > è§£å†³é•¿æ–‡æœ¬æ£€ç´¢çš„é—®é¢˜ã€‚
  >
  > one first selects key blocks with local pre-ranking using classical IR models. A joint representation of queries and key blocks is then learned using a standard BERT model. A final relevance score is calculated from the representation obtained, which can be regarded as an aggregation of local relevance informationã€‚

- [ ] ~~Significant Improvements over the State of the Art? A Case Study of the MS MARCO Document Ranking Leaderboard~~

- [x] Learning Passage Impacts for Inverted Indexes

- [x] Improving Transformer-Kernel Ranking Model Using Conformer and Query Term Independence

  > åŸè®ºæ–‡åï¼šConformer-kernel with query term independence for document retrieval

- [x] A Systematic Evaluation of Transfer Learning and Pseudo-labeling with BERT-based Ranking Models

  > è¿ç§»å­¦ä¹ çš„æ¨¡å‹æ€§èƒ½é€šå¸¸æ¯”ä¼ªæ ‡ç­¾ï¼ˆBM25 scoreå¼±ç›‘ç£ï¼‰è®­ç»ƒçš„æ¨¡å‹å¥½ã€‚

- [x] Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index

- [x] Passage Retrieval for Outside-Knowledge Visual Question Answering

- [x] MS MARCO: Benchmarking Ranking Models in the Large-Data Regime

- [x] ã€CBNSã€‘Cross-Batch Negative Sampling for Training Two-Tower Recommenders

  > analyse the embedding stabilityï¼Œæå‡ºcross-batchè´Ÿé‡‡æ ·ç­–ç•¥ã€‚
  
- [ ] ~~Semantic Query Labeling Through Synthetic Query Generation~~

- [x] Towards a Better Tradeoff between Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based Approach

  > æå‡ºä¸€ä¸ªåœ¨ç”µå•†æ¨èç³»ç»Ÿçš„å¤šçº§æ£€ç´¢æ¶æ„ä¸­çš„pre-rankingæ¨¡å‹ã€‚æœ¬æ¥pre-rankingæ¨¡å‹åªèƒ½ç”¨åŒå¡”æ¶æ„ï¼Œä½†æœ¬æ–‡æå‡ºä¸€ä¸ªåŸºäºäº¤äº’çš„æ¨¡å‹ï¼Œå…¶ä¸­åº”ç”¨ç‰¹å¾é€‰æ‹©æ¥ä¿è¯æ•ˆç‡å’Œæ€§èƒ½çš„å¹³è¡¡ã€‚

- [x] ã€Adapted DPRã€‘Synthetic Target Domain Supervision for Open Retrieval QA

  > åŠ¨æœºï¼šç›´æ¥æŠŠDPRç”¨äºCOVIDæ•°æ®æ—¶ï¼Œæ£€ç´¢æ•ˆæœä¸å¥½ã€‚
  >
  > æœ¬æ–‡æå‡ºç”¨SQuADæ•°æ®é›†å¾®è°ƒBARTæ¨¡å‹ï¼Œç”±passageç”Ÿæˆqueryå’Œanswerã€‚å¾®è°ƒååœ¨ç›®æ ‡åŸŸï¼ˆCOVIDï¼‰çš„æ–‡æœ¬é›†åˆä¸Šç”Ÿæˆè®­ç»ƒæ ·æœ¬å¯¹ï¼Œå†ç”¨è¿™äº›ç”Ÿæˆçš„æ•°æ®å¾®è°ƒDPRã€‚
  
- [x] **Rethinking Search: Making Domain Experts out of Dilettantes**

  > a so-called model-based information retrieval framework is proposed that breaks away from the traditional index-retrieve-then-rank paradigm.




# ACL2018

- [x] Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval



# ï¼ˆå®Œæˆï¼‰ACL2019

- [x] RankQA: Neural Question Answering with Answer Re-Ranking 

- [x] Episodic Memory Reader: Learning What to Remember for Question Answering from Streaming Data 

- [x] Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension

- [x] Latent Retrieval for Weakly Supervised Open Domain Question Answering

- [x] Multi-Hop Paragraph Retrieval for Open-Domain Question Answering

- [x] Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index

- [x] A cross-sentence latent variable model for semi-supervised sequence matching

- [x] Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction 

- [x] Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems

- [x] Matching Article Pairs with Graphical Decomposition and Convolutions

- [x] RE2: Simple and Effective Text Matching with Richer Alignment Features

- [x] A Lightweight Recurrent Network for Sequence Modeling

- [x] Encouraging Paragraph Embeddings to Remember Sentence Identity Improves Classification

- [x] Pretraining Methods for Dialog Context Representation Learning

- [x] Relational Word Embeddings

- [x] Training Neural Response Selection for Task-Oriented Dialogue Systems

- [x] Learning Transferable Feature Representations Using Neural Networks

- [x] Sampling Matters! An Empirical Study of Negative Sampling Strategies for Learning of Matching Models in Retrieval-based Dialogue Systems

  > In this paper, instead of conï¬guring new architectures, we investigate how to improve the performance of existing matching models with a better learning method. we consider four negative sampling strategies, namely minimum sampling, maximum sampling, semi-hard sampling, and decay-hard sampling.
  >
  > In the ï¬rst two strategies, a response candidate that corresponds to the minimal or the maximal matching score at the current step is picked from a pool as a negative example for the next step; and in the latter two strategies, we select negative examples by considering how hard they are to the current matching models. The semi-hard sampling prefers candidates with moderate difï¬culty to avoid both false negatives and trivial true negatives, and the decay-hard sampling gradually increases the difï¬culty of negative samples with the training process going on.




ç¬¬ä¸€æ¬¡é˜…è¯»åˆ—è¡¨

- [x] ï¼ï¼ï¼ˆå¤šæ®µè½MRCï¼‰**Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs**

- [x] ï¼ï¼ï¼ˆç”Ÿæˆå¼MRCï¼‰**Multi-style Generative Reading Comprehension** ã€åŸºäºtransformerã€‘

- [x] **ï¼ˆNLIæ•°æ®é›†ä¸­çš„biasé—®é¢˜ï¼‰Donâ€™t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference ã€ç»™å®šå‡è®¾å’Œlabelï¼Œé¢„æµ‹å‰æã€‚ã€‚ã€‚æ–¹æ³•æ–°é¢–ï¼Œä½†æ²¡çœ‹æ‡‚å¸–å­ã€‘**

- [ ] ï¼ˆMRCã€æ¨ç†ï¼‰Inferential Machine Comprehension: Answering Questions by Recursively Deducing the Evidence Chain from Text ã€æ¨¡å‹å¾ˆå¤æ‚ï¼Œï¼Œä¸»è¦é’ˆå¯¹å¤šæ­¥æ¨ç†ã€‘

  


ç¬¬äºŒæ¬¡è®ºæ–‡åˆ—è¡¨ï¼ˆ20ç¯‡ï¼‰

- [x] ï¼ï¼ï¼ˆMRCï¼Œå¤–éƒ¨çŸ¥è¯†ï¼‰**Explicit Utilization of General Knowledge in Machine Reading Comprehension**ã€æå‡ºä¸€ç§ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†WordNetçš„æ–¹å¼ã€‘
- [x] ï¼ï¼ï¼ˆMRCï¼‰**Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension**ã€æå‡ºè¯çº§åˆ«çš„åŠ¨æ€self-attentionæœºåˆ¶ç”¨äºå¤šæ®µè½MRCã€‘
- [x] ï¼ï¼ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰**Multi-Task Deep Neural Networks for Natural Language Understanding** ã€åœ¨BERTçš„åŸºç¡€ä¸Šå¯¹GLUEçš„æ‰€æœ‰ä»»åŠ¡è”åˆå­¦ä¹ ã€‘
- [x] ï¼ˆæ— ç›‘ç£MRCæ•°æ®é›†ç”Ÿæˆï¼‰Unsupervised Question Answering by Cloze Translation
- [x] ï¼ï¼ï¼ˆBERT+çŸ¥è¯†å›¾è°±ï¼‰ERNIE: Enhanced Language Representation with Informative Entities
- [x] ï¼ï¼ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰**Learning to Ask Unanswerable Questions for Machine Reading Comprehension** ã€è®ºå›ä¸Šçš„å¸–å­æ²¡å¤ªçœ‹æ‡‚å…·ä½“æ–¹æ³•ï¼Œä½†æ„Ÿè§‰æœ‰ä»·å€¼ã€‘
- [x] ï¼ï¼ï¼ˆQAå¯¹ç”Ÿæˆï¼‰**Generating Question-Answer Hierarchies** ã€è®ºå›ä¸Šçš„å¸–å­æ²¡å¤ªçœ‹æ‡‚å…·ä½“æ–¹æ³•ï¼Œä½†æ„Ÿè§‰æœ‰ä»·å€¼ã€‘
- [x] ï¼ï¼ï¼ˆMRCï¼‰**Exploiting Explicit Paths for Multi-hop Reading Comprehension**
- [x] ï¼ï¼ï¼ˆself-attentionï¼‰**Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned**
- [x] ï¼ï¼ï¼ˆå¯¹è¯ï¼‰**One Time of Interaction May Not Be Enough: Go Deep with an Interaction-over-Interaction Network for Response Selection in Dialogues**  ã€è®ºå›ä¸Šçš„å¸–å­æ²¡å¤ªçœ‹æ‡‚å…·ä½“æ–¹æ³•ï¼Œä½†æ„Ÿè§‰æœ‰ä»·å€¼ã€‘



ç¬¬ä¸‰æ¬¡è®ºæ–‡åˆ—è¡¨

- [x] ï¼ï¼ï¼ˆQAæ¨¡å‹çš„é²æ£’æ€§ï¼‰**Improving the Robustness of Question Answering Systems to Question Paraphrasing**
- [x] **ï¼ï¼ï¼ˆå¯¹è¯çš„é—®é¢˜ç”Ÿæˆï¼‰Reinforced Dynamic Reasoning for Conversational Question Generation**
- [x] **Transformer-XL: Attentive Language Models beyond a Fixed-Length Context   **ã€å¾ªç¯æœºåˆ¶ + ç›¸å¯¹ä½ç½®ç¼–ç ã€‘
- [x] ï¼ï¼ï¼ï¼ˆå¤šè·³MRCï¼‰**Cognitive Graph for Multi-Hop Reading Comprehension at Scale** ã€ç”¨ä¸¤ä¸ªsystemè§£å†³å¤šè·³MRCçš„æ¨ç†ã€‘
- [x] ï¼ï¼ï¼ï¼ˆå¼€æ”¾åŸŸQAï¼‰**Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader**ã€ä¸ºäº†å¼¥è¡¥KBæ— æ³•æä¾›å¼€æ”¾åŸŸQAæ‰€éœ€çš„å…¨éƒ¨çŸ¥è¯†ï¼Œå› æ­¤è€ƒè™‘åŠ å…¥ä¸€äº›éç»“æ„åŒ–æ–‡æœ¬çŸ¥è¯†ã€‘
- [x] ï¼ï¼ï¼ˆå¤šè·³MRCï¼‰Multi-hop Reading Comprehension through Question Decomposition and Rescoring
- [x] ï¼ï¼ï¼ˆMRCï¼‰Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension ã€åœ¨BERTä¸ŠåŠ å…¥çŸ¥è¯†åº“ã€‘
- [x] **ï¼ˆå¯¹è¯å¼QAï¼‰MCË†2: Multi-perspective Convolutional Cube for Conversational Machine Reading Comprehension**
- [x] MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension
- [x] ï¼ï¼ï¼ˆå¯¹è¯ï¼‰**Are Training Samples Correlated? Learning to Generate Dialogue Responses with Multiple References**ã€å»ºæ¨¡å¹¶åˆ©ç”¨å¤šä¸ªvalid responseä¹‹é—´çš„å…³ç³»ã€‘
- [x] ï¼ï¼ï¼ˆå¯¹è¯ï¼‰**Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading**ã€èåˆå¤–éƒ¨çŸ¥è¯†ï¼Œå¸®åŠ©å¯¹è¯ç³»ç»Ÿç”Ÿæˆresponseã€‚ã€‚åŒæ—¶æä¾›äº†ä¸€ä¸ªåŒ…æ‹¬å¤–éƒ¨çŸ¥è¯†çš„æ–°æ•°æ®é›†ã€‘



- [x] ï¼ï¼ï¼ˆå¯¹è¯ï¼‰Improving Multi-turn Dialogue Modelling with Utterance ReWriter

- [x] ï¼ˆå¯¹è¯ï¼‰Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study

- [x] ï¼ï¼ï¼ˆå¯¹è¯ç³»ç»Ÿï¼‰Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension

- [x] ï¼ˆå¯¹è¯ï¼Œresponseç”Ÿæˆï¼‰Incremental Transformer with Deliberation Decoder for Document Grounded Conversations

- [x] ï¼ˆå¯¹è¯ï¼Œresponseé€‰æ‹©ï¼‰Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection

- [x] ï¼ˆå¯¹è¯ç”Ÿæˆï¼‰ReCoSa: Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation

- [x] ï¼ˆresponse ç”Ÿæˆï¼‰Retrieval-Enhanced Adversarial Training for Neural Response Generation

- [x] ï¼ˆresponse ç”Ÿæˆï¼‰Learning to Abstract for Memory-augmented Conversational Response Generation

- [x] ï¼ˆresponse ç”Ÿæˆï¼‰Neural Response Generation with Meta-words

- [x] ï¼ˆå¯¹è¯ï¼Œå…ƒå­¦ä¹ ï¼‰Domain Adaptive Dialog Generation via Meta Learning

- [x] ï¼ï¼ï¼ˆä¸å¯å›ç­”çš„é—®é¢˜ç”Ÿæˆï¼‰Self-Attention Architectures for Answer-Agnostic Neural Question Generation

- [x] ï¼ˆå›¾ç½‘ç»œã€å¤šè·³æ¨ç†ï¼‰Dynamically Fused Graph Network for Multi-hop Reasoning

- [x] ï¼ï¼ï¼ˆå¤šæ­¥æ¨ç†ï¼‰Compositional Questions Do Not Necessitate Multi-hop Reasoning

- [x] ï¼ˆMRCï¼‰Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives 

- [x] ï¼ˆMRCï¼‰Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension

- [x] ï¼ï¼ï¼ˆé¢„è®­ç»ƒï¼Œè¿ç§»å­¦ä¹ ï¼Œè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼‰Large-Scale Transfer Learning for Natural Language Generation

- [x] Learning Compressed Sentence Representations for On-Device Text Processing

- [x] ï¼ˆBERTå¯è§£é‡Šæ€§ï¼‰What Does BERT Learn about the Structure of Language

- [x] Dual Supervised Learning for Natural Language Understanding and Generation

- [x] ï¼ˆæ¢¯åº¦åè½¬ï¼ŒåŸŸé€‚åº”ï¼‰Reversing Gradients in Adversarial Domain Adaptation for Question Deduplication and Textual Entailment Tasks

- [x] ï¼ˆçŸ¥è¯†è’¸é¦ï¼Œå¤šä»»åŠ¡å­¦ä¹ ï¼‰BAM! Born-Again Multi-Task Networks for Natural Language Understanding





# ï¼ˆå®Œæˆï¼‰ACL2020

- [x] ï¼ˆæ•ˆç‡ï¼‰DeFormer: Decomposing Pre-trained Transformers for Faster Question Answeringï¼ˆè§£è€¦ä½å±‚çš„BERTï¼‰

- [x] ï¼ˆæ•ˆç‡ï¼‰DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference

  > æœ¬æ–‡æå‡ºå‡è®¾ï¼šå¯¹æœ‰äº›æ ·æœ¬è€Œè¨€ï¼Œä¸éœ€è¦é€šè¿‡BERT/RoBERTçš„æ‰€æœ‰å±‚å°±å¯ä»¥å¾—åˆ°æ­£ç¡®çš„é¢„æµ‹ç»“æœï¼Œå³æœ‰äº›å±‚æ˜¯å†—ä½™çš„ã€‚åŸºäºæ­¤å‡è®¾ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•æ˜¯åœ¨é¢„è®­ç»ƒçš„BERT/RoBERTçš„åŸºç¡€ä¸Šï¼Œåœ¨æ¯å±‚Transformeråé¢åŠ ä¸€ä¸ªåˆ†ç±»å±‚ï¼Œç„¶åè¿›è¡Œä¸¤é˜¶æ®µçš„å‚æ•°å¾®è°ƒï¼Œæœ€ååœ¨æ¨æ–­é˜¶æ®µæ—¶ï¼Œå¦‚æœå“ªå±‚çš„åˆ†ç±»ç»“æœè¾¾åˆ°ç½®ä¿¡å€¼ï¼Œå°±æå‰é€€å‡ºã€‚å®éªŒéƒ¨åˆ†åœ¨GLUEçš„æ‰€æœ‰taskä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¯ä»¥åœ¨æ€§èƒ½ç¨é™çš„åŸºç¡€ä¸Šæ˜æ˜¾æé«˜æ•ˆç‡ã€‚

- [x] ï¼ˆæ•ˆç‡ã€è’¸é¦ï¼‰FastBERT: a Self-distilling BERT with Adaptive Inference Time

  > æ•´ä½“æ¨¡å‹ç»“æ„å’Œmotivationç±»ä¼¼DeeBERTï¼Œåªæ˜¯åœ¨å¯¹æ¯ä¸ªåˆ†ç±»å±‚çš„è®­ç»ƒæ–¹æ³•ä¸Šä¸å¤ªä¸€æ ·ï¼Œæœ¬æ–‡ä½¿ç”¨self-distillingçš„æ–¹å¼ï¼Œå¯ä»¥åˆ©ç”¨æ— é™çš„unlabelæ•°æ®ï¼Œæœ€åæ¥çœ‹æ•ˆæœä¼šæ¯”DeeBERTæ›´å¥½ã€‚å®éªŒåœ¨6ä¸ªä¸­æ–‡æ•°æ®é›†å’Œ6ä¸ªè‹±æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œå¯ä»¥åœ¨æ€§èƒ½å‡ ä¹ä¸é™çš„æƒ…å†µä¸‹3-5å€å¢åŠ æ•ˆç‡ã€‚

- [ ] ï¼ˆæ•ˆç‡ï¼‰HAT: Hardware-Aware Transformers for Efficient Natural Language Processing

- [ ] ï¼ˆæ•ˆç‡ã€æ¨¡å‹å‹ç¼©ï¼‰MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices

- [x] ï¼ˆæ•ˆç‡ã€ç¨€ç–è¡¨ç¤ºï¼‰Contextualized Sparse Representations for Real-Time Open-Domain Question Answering

  > å¯¹DenSPIçš„ç¨€ç–è¡¨ç¤ºéƒ¨åˆ†çš„æ”¹è¿›ï¼Œå¢åŠ å¯å­¦ä¹ çš„ç¨€ç–è¡¨ç¤ºï¼Œä½¿ç”¨æ ¸å‡½æ•°æ€æƒ³ã€‚

- [x] A Mixture of hâˆ’1 Heads is Better than h Heads

- [x] Highway Transformer: Self-Gating Enhanced Self-Attentive Networks

  > å¯¹Transformerç»“æ„è¿›è¡Œæ”¹è¿›ï¼ŒåŠ å…¥ç±»ä¼¼highwayçš„æ€æƒ³ï¼Œä»è€ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ï¼ŒåŒæ—¶å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚ä¸»è¦çš„motivationæ˜¯ï¼šTransformerä¸­multi-head attentionçš„ç»“æ„åªä¿ç•™äº†è¯ä¹‹é—´çš„å…³ç³»ï¼Œæ²¡æœ‰ä¿ç•™è¯æœ¬èº«çš„ç‰¹å¾ã€‚

- [ ] How Does Selective Mechanism Improve Self-Attention Networks?

- [ ] Combining Subword Representations into Word-level Representations in the Transformer Architecture

- [x] ï¼ˆå¯è§£é‡Šæ€§ï¼‰Roles and Utilization of Attention Heads in Transformer-based Neural Language Models

  > 1) we suggest an analysis method which helps understand where linguistic properties are learned and represented along attention heads in transformer architectures and 2) we show that using analysis results, attention heads can be maximally utilized for performance gains during the ï¬ne-tuning process on the downstream tasks and for capturing linguistic properties.

- [x] ï¼ˆé¢„è®­ç»ƒï¼‰BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

  > ä¸»è¦æ˜¯é’ˆå¯¹ç”Ÿæˆå¼ä»»åŠ¡ï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€QAã€å¯¹è¯ï¼‰ï¼ŒåŒæ—¶ä¿è¯åˆ†ç±»ä»»åŠ¡è¡¨ç°ä¹Ÿä¸å·®ã€‚
  >
  > è®ºæ–‡ä¸­ä¹Ÿå¯¹æ¯”äº†ä¸åŒé¢„è®­ç»ƒç›®æ ‡å‡½æ•°å’Œå™ªå£°å¹²æ‰°å‡½æ•°çš„å½±å“ã€‚

- [ ] BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance

- [ ] ï¼Ÿï¼ŸschuBERT: Optimizing Elements of BERT

- [ ] SenseBERT: Driving Some Sense into BERT

- [ ] ï¼ˆå¯è§£é‡Šæ€§ï¼‰Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT

- [x] ï¼ˆé¢„è®­ç»ƒï¼‰Span Selection Pre-training for Question Answering

- [x] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Attentive Pooling with Learnable Norms for Text Representation

  > we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation. we propose two methods to ensure the numerical stability of the model training: The ï¬rst one is scale limiting, which limits the scale of input representations to ensure their non-negativity and avoid potential exponential explosion. The second one is re-formulation, which decomposes the exponent operation into several safe atomic operations to avoid computing the real-valued powers of input features with less computational cost.
  >
  > è¯„ä¼°ä»»åŠ¡æ˜¯æ–‡æœ¬ï¼ˆæƒ…æ„Ÿï¼‰åˆ†ç±»ã€‚

- [x] ã€MRCã€‘Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension

  > We model documents at different levels of granularity to learn the hierarchical nature of the document. On the Natural Questions dataset, which contains two sub-tasks predicting a paragraph-level long answer and a token-level short answer, our method jointly trains the two sub-tasks to consider the dependencies of the twograined answers.

- [x] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning

  > we propose a new learning objective named language autoencoding (LAE) for obtaining fully contextualized language representations without repetition. To learn the proposed LAE, we develop a diagonal masking operation and an input isolation mechanism inside the T-TA.

- [x] QUASE: Question-Answer Driven Sentence Encoding

- [x] ReasoningOver Semantic-Level Graph for Fact Checking

  > The main contribution of this work is the graphbased reasoning approach for claim veriï¬cation.

- [x] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰SPECTER: Document-level Representation Learning using Citation-informed Transformersï¼ˆåˆ©ç”¨å­¦æœ¯æ–‡ç« ä¹‹é—´çš„å¼•ç”¨å…³ç³»æ›´å¥½çš„å­¦ä¹ å­¦æœ¯æ–‡ç« çš„documentè¡¨ç¤ºï¼‰

- [x] ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰Curriculum Learning for Natural Language Understanding

  > We explore and demonstrate the effectiveness of CL in the context of ï¬netuning LM on NLU tasks. We propose a novel CL framework that consists of a Difï¬culty Review method and a Curriculum Arrangement algorithm.

- [x] ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰How to Ask Good Questions? Try to Leverage Paraphrases

- [x] ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰Semantic Graphs for Generating Deep Questions

  > Deep Questions Generation: ç”Ÿæˆéœ€è¦å¤šæ–‡æœ¬ç‰‡æ®µæ¨ç†çš„å¤æ‚é—®é¢˜ã€‚
  >
  > we propose a novel framework which ï¬rst constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards, we fuse the document-level and graphlevel representations to perform joint training of content selection and question decoding.

- [x] ï¼ˆå•†å“æ£€ç´¢ï¼‰Learning Robust Models for e-Commerce Product Searchï¼ˆé€šè¿‡åŒ¹é…çš„query-item pairç”Ÿæˆä¸åŒ¹é…çš„queryï¼Œä»¥ä¾¿å­¦ä¹ æ›´é²æ£’çš„åˆ†ç±»ç•Œé¢ï¼Œæ›´å¥½åœ°è¯†åˆ«ä¸åŒ¹é…çš„query-item pairã€‚ï¼‰

- [x] ï¼ˆFAQï¼‰Unsupervised FAQ Retrieval with Question Generation and BERT

  > We presented a fully unsupervised method for FAQ retrieval. The method is based on an initial retrieval of FAQ candidates followed by three rerankers. The ï¬rst one is based on an IR passage retrieval approach, and the others two are independent BERT models that are ï¬ne-tuned to predict query-to-answer and query-to-question matching.

- [x] The Cascade Transformer: an Application for Efficient Answer Sentence Selection

  >This work introduces CT, a variant of the traditional transformer model designed to improve inference throughput.  Our approach leverages classiï¬ers placed at different encoding stages to prune candidates in a batch and improve model throughput.

- [x] ï¼ˆçŸ­æ–‡æœ¬åŒ¹é…ã€å›¾ç½‘ç»œï¼‰Neural Graph Matching Networks for Chinese Short Text Matching

  > we propose a neural graph matching model for Chinese short text matching.

- [x] ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection

  > we proposed a ï¬‚exible framework for combining topic models with BERT.

- [x] ï¼ˆè‡ªç„¶è¯­è¨€ç”Ÿæˆï¼‰A Generative Model for Joint Natural Language Understanding and Generation



# ACL2021

- [x] ã€DensePhrasesã€‘Learning Dense Representations of Phrases at Scale
  
- [x] ã€GARã€‘Generation-augmented retrieval for open-domain question answeringï¼ˆè®­ç»ƒç”Ÿæˆæ¨¡å‹ï¼Œè¾“å…¥æ—¶queryï¼Œè¾“å‡ºæ˜¯answer/answeræ‰€åœ¨çš„å¥å­/answeræ‰€åœ¨çš„passageçš„titleã€‚æ ¹æ®queryç”Ÿæˆæ–°çš„contextæ¥æ‰©å±•queryï¼Œå†ç”¨BM25ç­‰æ–¹æ³•è¿›è¡Œæ£€ç´¢ï¼‰
  
- [x] Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval
  
  > our method mimics the real queries by an iterative K-means clustering algorithm.
  
- [x] ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer

  > we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to ï¬ne-tune BERT in an unsupervised and effective way.

- [x] PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval

  > we propose a novel approach that leverages both query-centric and PAssage-centric sImilarity Relations (called PAIR) for dense passage retrieval: 1) introducing formal formulations of the two kinds of similarity relations, 2) generating high-quality pseudo labeled data via knowledge distillation, 3) designing an effective two-stage training procedure.

- [x] ã€APEã€‘Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints

  > by replacing the encoder of generative ODQA models with our proposed adaptive passage encoder, we can train an effective adaptive computation policy without tuning the base model.

- [ ] Neural Machine Translation with Monolingual Translation Memoryï¼ˆOutstanding papersï¼‰

- [ ] Vocabulary Learning via Optimal Transport for Neural Machine Translationï¼ˆbest paperï¼‰

- [ ] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations

- [ ] End-to-End Training of Neural Retrievers for Open-Domain Question Answering

  > End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering

- [ ] Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision

- [ ] Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval

- [ ] LeeBERT: Learned Early Exit for BERT with cross-level optimization

- [ ] EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets

- [ ] Probing Toxic Content in Large Pre-Trained Language Models

- [ ] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning

- [ ] Optimizing Deeper Transformers on Small Datasets

- [ ] xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering

- [ ] A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations

- [ ] Self-Guided Contrastive Learning for BERT Sentence Representations

- [ ] Pre-training Universal Language Representation

- [ ] UnitedQA: A Hybrid Approach for Open Domain Question Answering

- [ ] LexFit: Lexical Fine-Tuning of Pretrained Language Models

- [ ] Selecting Informative Contexts Improves Language Model Fine-tuning

- [ ] Language Model Augmented Relevance Score

- [ ] CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding

- [ ] The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes

- [ ] Efficient Passage Retrieval with Hashing for Open-domain Question Answering

- [x] Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation

- [ ] Contrastive Fine-tuning Improves Robustness for Neural Rankers

- [ ] Weakly Supervised Pre-Training for Multi-Hop Retriever

- [ ] K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters

- [x] Reader-Guided Passage Reranking for Open-Domain Question Answering

  >we propose a simple and effective passage reranking method, named ReaderguIDEd Reranker (R IDER), which does not involve training and reranks the retrieved passages solely based on the top predictions of the reader before reranking.
  
- [x] The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes

  > We have proven and shown empirically that the probability for false positives in dense information retrieval depends on the index size and on the dimensionality of the used representations.



# EMNLP2018

- [x] Phrase indexed question answering: A new challenge for scalable document comprehension.ï¼ˆå­¦ä¹ ç‹¬ç«‹çš„document phraseè¡¨ç¤ºå’Œquestionè¡¨ç¤ºï¼‰
- [x] Ranking paragraphs for improving answer recall in open-domain question answering.ï¼ˆå¯¹top ranked documentsè¿›è¡Œæ®µè½æ’åºï¼Œé€‰æ‹©topMä¸ªæ®µè½è¿›è¡ŒRCã€‚ä»¥ä¾¿ç»è¿‡retrievalä¹‹åå¯ä»¥æ¥è§¦æ›´å¤šçš„æ®µè½ï¼Œæé«˜answerçš„å¬å›ç‡ï¼‰
- [x]  Adaptive Document Retrieval for Deep Question Answering.ï¼ˆæ¢ç©¶äº†retrieverå’Œreaderä¹‹é—´çš„å…³ç³»ï¼Œå¯¹äºä¸åŒçš„questionï¼Œå–ä¸åŒtopæ•°ç›®çš„documentç”¨äºRCï¼‰





# EMNLP2019

- [x] ï¼ˆå¯¹è¯ï¼‰Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots

- [x] ï¼ˆå¯¹è¯ï¼‰Multi-Granularity Representations of Dialog

- [x] ï¼ˆæ–‡æœ¬åˆ†ç±»ã€è¡¨ç¤ºå­¦ä¹ ï¼‰Enhancing Local Feature Extraction with Global Representation for Neural Text Classification

- [x] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations

- [x] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

- [x] ~~ï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰Parameter-free Sentence Embedding via Orthogonal Basis~~

- [x] ï¼ˆæœºå™¨ç¿»è¯‘ï¼‰Hierarchical Modeling of Global Context for Document-Level Neural Machine Translation

- [x] ã€OSOA-DFNã€‘ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰Original Semantics-Oriented Attention and Deep Fusion Network for Sentence Matching

- [x] ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰Aggregating Bidirectional Encoder Representations Using MatchLSTM for Sequence Matching

  > Given a sentence pair, we extract the output representations of it from BERT. Then we extend BERT with a MatchLSTM layer to get further interaction of the sentence pair for sequence matching tasks.

- [x] ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling

  > we start with the Pairwise Word Interaction Model, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs.

- [x] ã€ADINã€‘ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰Asynchronous Deep Interaction Network for Natural Language Inference

  > This model is stacked with multiple inference sub-layers to implement the multi-step reasoning, and each sub-layer consists of two local inference modules in an asymmetrical manner to simulate the asynchronous and interpretable reasoning process.

- [x] ã€HCANã€‘ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling

  > We propose the HCAN model with a relevance matching module to capture weighted n-gram matching signals and a semantic matching module for contextaware representation learning.
  >
  > experiments show that relevance matching alone performs reasonably well for many NLP tasks, while semantic matching alone is not effective for IR tasks. We show that relevance matching and semantic matching are complementary, and HCAN combines the best of both worlds.

- [x] ï¼ˆQAçš„é‡‡æ ·ç­–ç•¥ï¼‰Improving Answer Selection and Answer Triggering using Hard Negatives

  > We have shown that selection of hard negatives is a powerful tool for answer selection.

- [x] ã€GSAMNã€‘ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰A Gated Self-attention Memory Network for Answer Selection

  > propose a new gated self-attention memory network for Answer selection.

- [x] ï¼ˆæ–‡æœ¬åŒ¹é…ï¼‰MICRON: Multigranular Interaction for Contextualizing RepresentatiON in Non-factoid Question Answering

  > we propose MICRON, allowing to match ï¬‚exible n-grams and to combine with word-based query term weighting, achieving the state of the art among baselines with reported performances on WikiPassageQA and InsuranceQA.


- [x] ï¼ˆå¯¹ä¸Šä¸‹æ–‡è¯å‘é‡çš„åˆ†æï¼‰How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings

  > 1ï¼‰In all layers of all three models, the contextualized word representations of all words are not isotropicï¼ˆå³å„å‘å¼‚æ€§çš„ï¼‰: they are not uniformly distributed with respect to direction. Instead, they are anisotropic, occupying a narrow cone in the vector space.
  >
  > 2ï¼‰While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-speciï¬c representations.
  >
  > 3ï¼‰In ELMo, representations of words in the same sentence grow more similar to each other as context-speciï¬city increases in upper layers; in BERT, they become more dissimilar to each other in upper layers but are still more similar than randomly sampled words are on average; in GPT-2, however, words in the same sentence are no more similar to each other than two randomly chosen words.
  >
  > 4ï¼‰In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a wordâ€™s contextualized representations can be explained by a static embedding for that word, providing some justiï¬cation for the success of contextualized representations.

- [x] ï¼ˆå¯¹transformerçš„æ”¹è¿›ï¼‰Tree Transformer: Integrating Tree Structures into Self-Attention

  > This paper proposes Tree Transformer, a ï¬rst attempt of integrating tree structures into Transformer by constraining the attention heads to attend within constituents. The tree structures are automatically induced from the raw texts by our proposed Constituent Attention module.

- [x] ï¼ˆå¯¹bertçš„æ”¹è¿›ï¼‰Fine-tune BERT with Sparse Self-Attention Mechanism

  > we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into selfattention mechanism to enhance the ï¬ne-tuning performance of BERT.
  >
  > sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when ï¬ne-tuning with BERT.

- [x] ï¼ˆberté¢„è®­ç»ƒåŠ å¾®è°ƒä¸ºä»€ä¹ˆæ¯”ä»0å¼€å§‹è®­ç»ƒæ›´å¥½çš„å¯è§£é‡Šæ€§ï¼‰Visualizing and Understanding the Effectiveness of BERT

  > we propose to visualize loss landscapes and optimization trajectories of ï¬ne-tuning BERT on speciï¬c datasets.
  >
  > å‘ç°ï¼š1ï¼‰visualization results indicate that BERT pretraining reaches a good initial point across downstream tasksã€‚2ï¼‰loss landscapes of ï¬ne-tuning partially explain the good generalization capability of BERTã€‚3ï¼‰we demonstrate that the lower (i.e., close to input) layers of BERT are more invariant across tasks than the higher layersã€‚

- [x] ã€Birchã€‘ï¼ˆé•¿æ–‡æœ¬çš„ad-hocæ£€ç´¢ï¼‰Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval

  > ç”¨BERT sentence-levelçš„å¾—åˆ†å¾—åˆ°document-levelçš„å¾—åˆ†ã€‚
  >
  > Our results demonstrate two surprising ï¬ndings: ï¬rst, that relevance models can be transferred quite straightforwardly across domains by BERT, and second, that effective document retrieval requires only â€œpaying attentionâ€ to a small number of â€œtop sentencesâ€ in each document.

- [x] ï¼ˆå¤–éƒ¨çŸ¥è¯†ï¼Œç”Ÿæˆå¼QAï¼‰Incorporating External Knowledge into Machine Reading for Generative Question Answering

- [x] ï¼ï¼**PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text**

- [x] ï¼ï¼**Ranking and Sampling in Open-Domain Question Answering**

- [x] ï¼ï¼Revealing the Importance of Semantic Retrieval for Machine Reading at Scale

- [x] ï¼ˆå¤šè·³é—®ç­”ï¼‰Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering

  > We introduced ROCC, a simple unsupervised approach for selecting justiï¬cation sentences for question answering, which balances relevance, overlap of selected sentences, and coverage of the question and answer. We coupled this method with a state-of-the-art BERT-based supervised question answering system, and achieved a new state-of-the-art on the MultiRC and ARC datasets among approaches that do not use external resources during training.

- [x] ï¼ˆå¼€æ”¾åŸŸå¤šè·³QAï¼‰**Answering Complex Open-domain Questions Through Iterative Query Generation**

  > In the very ï¬rst hop of reasoning, GOLDEN Retriever is presented the original question q, from which it generates a search query q1 that retrieves supporting document d1. Then for each of the subsequent reasoning steps (k = 2, . . . , S), GOLDEN Retriever generates a query q_k from the question and the available context, (q, d_1 , . . . , d_kâˆ’1 ). This formulation allows the model to generate queries based on information revealed in the supporting facts.

- [x]  ï¼ˆå¼€æ”¾åŸŸQAï¼‰Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answeringï¼ˆæå‡ºè¦è·¨æ®µè½è¿›è¡Œglobal normalizationï¼Œå¹¶ä½¿ç”¨åŸºäºBERTçš„passage rerankerï¼‰



# ï¼ˆå®Œæˆï¼‰EMNLP2020

- [ ] *~~AmbigQA: Answering Ambiguous Open-domain Questions~~*

  > ç ”ç©¶å¼€æ”¾åŸŸé—®ç­”ä¸­é—®é¢˜çš„æ­§ä¹‰æ€§ï¼Œæ‰¾åˆ°æ­§ä¹‰æ€§é—®é¢˜å¯¹åº”çš„å¤šä¸ªç­”æ¡ˆï¼Œå¹¶é’ˆå¯¹æ¯ä¸€ä¸ªç­”æ¡ˆå¯¹é—®é¢˜è¿›è¡Œé‡å†™ä»¥å®ç°æ¶ˆæ­§ã€‚
  >
  > æå‡ºäº†ä¸€ä¸ªå¯¹åº”çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä»¥åŠbaselineã€‚

- [x] ã€**å¥å­è¡¨ç¤ºå­¦ä¹ **ã€‘*An Unsupervised Sentence Embedding Method by Mutual Information Maximization*

  > <img src="../images/image-20201010154450654.png" alt="image-20201010154450654" style="zoom:33%;" />
  >
  > å¥å­è¾“å…¥åˆ° BERT åè¢«ç¼–ç ï¼Œå…¶è¾“å‡ºçš„ token embeddings é€šè¿‡å¤šä¸ªä¸åŒ kernel size çš„ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œå¾—åˆ°å¤šä¸ª n-gram ç‰¹å¾ã€‚æˆ‘ä»¬æŠŠæ¯ä¸€ä¸ª n-gram ç‰¹å¾å½“æˆå±€éƒ¨è¡¨å¾ï¼ˆLocal representationï¼‰ï¼Œ å°†å¹³å‡æ± åŒ–åçš„å±€éƒ¨è¡¨å¾ç§°ä¸ºå…¨å±€è¡¨å¾ï¼ˆGlobal representationï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªåŸºäºäº’ä¿¡æ¯çš„æŸå¤±å‡½æ•°æ¥å­¦ä¹ æœ€ç»ˆçš„å¥å‘é‡ã€‚è¯¥æŸå¤±å‡½æ•°çš„å‡ºå‘ç‚¹æ˜¯æœ€å¤§åŒ–å¥å­çš„å…¨å±€è¡¨å¾ï¼ˆå¥å‘é‡ï¼‰ä¸å±€éƒ¨è¡¨å¾ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯å€¼ï¼Œå› ä¸ºå¯¹äºä¸€ä¸ªå¥½çš„å…¨å±€å¥å‘é‡ï¼Œå®ƒä¸æ‰€å¯¹åº”çš„å±€éƒ¨è¡¨å¾ä¹‹é—´çš„ MI åº”è¯¥æ˜¯å¾ˆé«˜çš„ï¼Œ ç›¸åï¼Œå®ƒä¸å…¶ä»–å¥å­çš„å±€éƒ¨è¡¨å¾é—´çš„ MI åº”è¯¥æ˜¯å¾ˆä½çš„ã€‚
  >
  > è¿™æ ·çš„ä»»åŠ¡ç±»ä¼¼ contrastive learningï¼Œå¯ä»¥é¼“åŠ±ç¼–ç å™¨æ›´å¥½åœ°æ•æ‰å¥å­çš„å±€éƒ¨è¡¨å¾ï¼Œå¹¶ä¸”æ›´å¥½åœ°åŒºåˆ†ä¸åŒå¥å­ä¹‹é—´çš„è¡¨å¾ã€‚

- [x] ã€**æ¨¡å‹çš„å¯è§£é‡Šæ€§**ã€‘*Analyzing Individual Neurons in Pre-trained Language Models*

  > åˆ†ææ¨¡å‹ï¼šELMoï¼ŒT-ELMoï¼ŒBERTï¼ŒXLNet
  >
  > åˆ†ææ–¹å¼ï¼šä»neuronè€Œä¸æ˜¯layerçš„è§’åº¦è¿›è¡Œæ¢æµ‹ã€‚
  >
  > ä¸€äº›åˆ†æç»“è®ºï¼š1ï¼‰it is possible to extract a very small subset of neurons to predict a linguistic task with the same accuracy as using the entire network. Low level tasks such as predicting morphology require fewer neurons compared to high level tasks such as predicting syntax. 2ï¼‰ELMo generally needed fewer neurons while T-ELMo required more neurons compared to the other models to achieve oracle performance. è¯´æ˜knowledge of lexical semantics in T-ELMo is distributed in more neuronsã€‚3ï¼‰lexical tasks such as learning morphology (POS tagging) and word semantics (SEM tagging) are dominantly captured by the neurons at lower layers, whereas the more complicated task of modeling syntax (CCG supertagging) is taken care of at the ï¬nal layer. An exception to this overall pattern is the BERT model. Top neurons in BERT spread across all the layers, unlike other models where top neurons (for a particular task) are contributed by fewer layers. 4ï¼‰BERT is the most distributed model with respect to all properties while XLNet exhibits focus with the most disjoint set of neurons and layers designated for different linguistic properties. 5ï¼‰Some phenomena (e.g. Verbs) are distributed across many neurons while others (e.g. Interjections) are localized in a fewer neuronsã€‚

- [x] *Analyzing Redundancy in Pretrained Transformer Models*

  > ç»“è®ºï¼š1ï¼‰ç›¸é‚»å±‚çš„å†—ä½™åº¦é«˜ï¼Œé™¤äº†æœ€åä¸¤å±‚ã€‚2ï¼‰85%çš„ç¥ç»å…ƒå¯ä»¥è¢«removeï¼Œè€Œä¸”æ²¡æœ‰æ€§èƒ½çš„æŸå¤±ã€‚å¤§éƒ¨åˆ†å†—ä½™çš„ç¥ç»å…ƒæ¥è‡ªäºåŒä¸€å±‚æˆ–è€…ç›¸é‚»å±‚ã€‚3ï¼‰high layer-level redundancy for sequence labeling tasks, The amount of redundancy is substantially lower for sequence classiï¬cation tasks. all the sequence classiï¬cation tasks are learned at higher layers and none of the lower layers were found to be redundant. 4ï¼‰XLNet is more redundant than BERTã€‚5ï¼‰Complex core language tasks require more neuronsã€‚Less task-speciï¬c redundancy for core linguistic tasks compared to higher-level tasks

- [x] *Context-Aware Answer Extraction in Question Answering**.* ï¼ˆæ²¡æ‰¾åˆ°ï¼‰

- [x] *Dense Passage Retrieval for Open-Domain Question Answering**.*

- [x] *Masking as an Efficient Alternative to Finetuning for Pretrained Language Models**.* 

  > å®éªŒæ–¹æ³•ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œä¸æ˜¯é‡‡ç”¨å¾®è°ƒæ‰€æœ‰å‚æ•°çš„æ–¹æ³•ï¼Œè€Œæ˜¯é€‰æ‹©ä¸€ç»„weightsï¼Œmaskæ‰å…¶ä»–çš„å‚æ•°ï¼Œæœ€åå¯ä»¥å–å¾—å’Œå¾®è°ƒå·®ä¸å¤šçš„æ€§èƒ½ã€‚
  > å®éªŒç»“æœï¼š1ï¼‰Intrinsic evaluations show that masked language models extract valid representations for downstream tasksã€‚2ï¼‰the representations are generalizableã€‚3ï¼‰the minima obtained by ï¬netuning and masking can be easily connected by a line segmentã€‚

- [x] Modularized Transfomer-based Ranking Framework.

  > This paper proposes MORES, a modular Transformer ranking framework that decouples ranking into Document Representation, Query Representation, and Interaction.

- [x] *Multi-Step Inference for Reasoning Over Paragraphs*

  > æå‡ºæ¨ç†æ¨¡å‹ï¼Œåˆ†ä¸ºselect+chain+predictä¸‰ä¸ªæ¨¡å—ã€‚

- [x] *MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale**.*

  > ç»“è®ºï¼š1ï¼‰neither domain similarity nor training data size are suitable for predicting the best modelsã€‚

- [x] *Multilevel Text Alignment with Cross-Document Attention*

  > åœ¨hierarchical attention çš„åŒå¡”æ¨¡å‹ä¸­åŠ å…¥cross-attentionã€‚
  >
  > æå‡ºä¸€ä¸ªdoc-docå’Œsentence-docå¯¹é½çš„benchmarkã€‚

- [x] *On the Sentence Embeddings from BERT for Semantic Textual Similarity**.*

  > BERT-flow

- [ ] ~~*Probing Pretrained Language Models for Lexical Semantics*~~(çœ‹ä¸æ‡‚)

- [x] *Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting*

  > åœ¨å¾®è°ƒé˜¶æ®µä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„æ€æƒ³ï¼Œè”åˆå­¦ä¹ é¢„è®­ç»ƒä»»åŠ¡ï¼Œä½†æ˜¯éœ€è¦Pretraining Simulation mechanismè§£å†³é¢„è®­ç»ƒä»»åŠ¡ä¸å¯å¾—çš„å›°éš¾ï¼Œè€Œä¸”å¤šä»»åŠ¡å­¦ä¹ æ—¶è¦ä½¿ç”¨Objective Shifting mechanismé€æ¸åé‡ä¸‹æ¸¸ä»»åŠ¡ã€‚

- [x] *Ad-hoc Document Retrieval using Weak-Supervision with BERT and GPT2**.* 

- [ ] *Towards Better Context-aware Lexical Semantics: Adjusting Contextualized Representations through Static Anchors* (æ‰¾ä¸åˆ°)

- [x] FastFormers: Highly Efficient Transformer Models for Natural Language Understanding

  > We showed that utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efï¬ciency.

- [x] A Little Bit Is Worse Than None: Ranking with Limited Training Data

- [x] Early Exiting BERT for Efficient Document Ranking

  > We propose asymmetric early exiting BERT for document ranking, an effective method to improve model efï¬ciency in IR tasks.

- [ ] ~~Donâ€™t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering~~

- [x] Exploring the Boundaries of Low-Resource BERT Distillation

- [ ] ETC: Encoding Long and Structured Inputs in Transformers

  > we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.
  >
  > The key ideas are a new globallocal attention mechanism, coupled with relative position encodings and a CPC pre-training task.
  
- [x] Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains

- [x] Beyond [CLS] through Ranking by Generation

  > we revisit the generative framework for information retrieval. The global pretrained generator is ï¬ne-tuned on the task of query generation conditioned on document content as the context.

- [ ] 



# EMNLP2021

- [ ] ~~Dealing with Typos for BERT-based Passage Retrieval and Ranking~~

- [x] When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions

  > we propose a joint retriever-reader model called JEEVES where the retriever is implicitly supervised only using QA labels via a novel word weighting mechanism.

- [x] Matching-oriented Product Quantization For Ad-hoc Retrieval

  > we propose MoPQ to optimize PQâ€™s ad-hoc retrieval accuracy. We propose MCL as our new training objective, where the model can be learned to maximize the querykey matching probability to achieve the optimal retrieval accuracy. We further leverage DCS for contrastive sample argumentation, which ensures the effective minimization of MCL.

- [x] Unsupervised Contextualized Document Representation

  > we propose a new approach (SCDV+BERT(ctxd), which leverages a combination of clustering techniques for word-sense disambiguation and combines it with the expressibility of SCDV partition averaging method for creating better document representation.

- [x] Phrase Retrieval Learns Passage Retrieval, Too

  > we follow the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including passages and documents.

- [x] **Reï¬ning BERT Embeddings for Document Hashing via Mutual Information Maximization**

  > we modify existing generative hashing methods to accommodate the BERT embeddings and then use the trained model to generate hash codesã€‚
  > åˆ©ç”¨äº†äº’ä¿¡æ¯æœ€å¤§åŒ–å’Œå¯¹æ¯”å­¦ä¹ ã€‚

- [x] RoR: Read-over-Read for Long Document Machine Reading Comprehension

  > we propose RoR, a read-over-read pipeline, which is able to expand the reading ï¬eld from chunk-level to documentlevel.

- [x] Simple Entity-Centric Questions Challenge Dense Retrievers

  > We ï¬rst construct EntityQuestions, a set of simple, entityrich questions based on facts from Wikidata and observe that dense retrievers drastically underperform sparse methods.
  >
  > We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to ï¬x the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders.

- [x] **Adaptive Information Seeking for Open-Domain Question Answering**(æœ±è¿æ˜Œ)

  > It models the open-domain QA task as a POMDP, where the environment contains a large corpus and the agent is asked to sequentially select retrieval function and reformulate query to collect the evidence.

- [x] RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking

  >This paper has presented a novel joint training approach for dense passage retrieval and passage re-ranking. To implement the joint training, we have made two important technical contributions, namely dynamic listwise distillation and hybrid data augmentation

- [x] ~~LMSOC: An Approach for Socially Sensitive Pretraining~~

  > We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models.
  
- [ ] Joint Passage Ranking for Diverse Multi-Answer Retrieval

- [ ] Answering Open-Domain Questions of Varying Reasoning Steps from Textï¼ˆå¼€æ”¾åŸŸé—®ç­”ï¼Œå¤šæ­¥æ¨ç†ï¼‰

- [x] SimCSE: Simple Contrastive Learning of Sentence Embeddings

  > We present an unsupervised approach which predicts input sentence itself with dropout noise and a supervised approach utilizing NLI datasets.

- [ ] Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuningï¼ˆå¾®è°ƒï¼‰

- [ ] The Power of Scale for Parameter-Efficient Prompt Tuning ï¼ˆPromptï¼‰

- [ ] Dynamic Knowledge Distillation for Pre-trained Language Models

- [ ] More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledgeï¼ˆçŸ¥è¯†å¢å¼ºï¼‰

- [ ] Universal Sentence Representation Learning with Conditional Masked Language Modelï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰

- [ ] Pairwise Supervised Contrastive Learning of Sentence Representationsï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰

- [ ] TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learningï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰

- [ ] WhiteningBERT: An Easy Unsupervised Sentence Embedding Approachï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰

- [ ] Implicit Sentiment Analysis with Event-centered Text Representationï¼ˆäº‹ä»¶ä¸ºä¸­å¿ƒçš„æ–‡æœ¬è¡¨ç¤ºï¼‰

- [ ] Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encodersï¼ˆå¥å­è¡¨ç¤ºå­¦ä¹ ï¼‰

- [ ] Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotationï¼ˆOpenQAçš„æ— ç›‘ç£æ£€ç´¢ï¼‰

- [ ] FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text modelsï¼ˆå°‘æ ·æœ¬å­¦ä¹ forQAï¼‰

- [ ] Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval

- [x] Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval

- [ ] Importance Estimation from Multiple Perspectives for Keyphrase Extraction

- [ ] Do Long-Range Language Models Actually Use Long-Range Context?

  > æœ¬æ–‡æ¢ç´¢çš„é—®é¢˜ï¼šHow do â€œlong-rangeâ€ Transformerï¼ˆèƒ½å¤„ç†4000+æˆ–8000+çš„Transformerï¼‰ LMs make use of the long-range context?
  >
  > åˆ†ææ–¹æ³•å‚è€ƒäº†ï¼šSharp Nearby, Fuzzy Far Away: How Neural Language Models Use Contextï¼ˆæ¢ç©¶LSTMçš„ï¼‰
  >
  > 

- [ ] Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classificationï¼ˆNegativeçš„é—®é¢˜ï¼‰

- [ ] Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalizationï¼ˆä¼ªè´Ÿä¾‹çš„é—®é¢˜ï¼‰

- [ ] Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrievalï¼ˆdense retrievalï¼‰

- [ ] Improving Embedding-based Large-scale Retrieval via Label Enhancement

- [ ] Dense Hierarchical Retrieval for Open-domain Question Answering

- [ ] Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation

- [ ] 

- [ ] 





# ï¼ˆå®Œæˆï¼‰ICLR2018

ï¼ˆ2.3%çš„å£å¤´å±•ç¤ºï¼Œ31.4%çš„posteræ¥å—ï¼Œ9%çš„workshopï¼‰

- [x] ï¼ï¼ï¼ï¼ï¼ˆDIINï¼‰Natural Language Inference over Interaction Space

- [x] ï¼ï¼ï¼ï¼ï¼ˆIRï¼Œå¤šä»»åŠ¡å­¦ä¹ ï¼‰Multi-Task Learning for Document Ranking and Query Suggestion 

- [x] ï¼ï¼ï¼ï¼ï¼ˆè¡¨è¾¾å­¦ä¹ ï¼‰An efficient framework for learning sentence representations

- [x] ï¼ï¼ï¼ˆè¡¨è¾¾å­¦ä¹ ï¼Œå¤šä»»åŠ¡å­¦ä¹ ï¼‰Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning

- [x] ï¼ï¼ï¼ï¼ï¼ˆåºåˆ—å»ºæ¨¡ï¼‰Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling

- [x] ï¼ï¼ï¼ï¼ï¼ˆå¼€æ”¾åŸŸQAï¼‰Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering

- [x] ï¼ï¼ï¼ï¼ˆQAï¼Œå¼ºåŒ–å­¦ä¹ ï¼‰Ask the Right Questions: Active Question Reformulation with Reinforcement Learning

- [x] ï¼ï¼ï¼ˆMRCï¼‰QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension

- [x] ï¼ï¼ï¼ˆMRCï¼‰DCN+: Mixed Objective And Deep Residual Coattention for Question Answering

- [x] ï¼ï¼ï¼ˆMRCï¼‰FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension

  

- [x] ï¼ˆMRCï¼‰Multi-Mention Learning for Reading Comprehension with Neural Cascades 

- [x] ï¼ˆæ‘˜è¦ï¼‰Generating Wikipedia by Summarizing Long Sequences 

- [x] ï¼ˆè¡¨è¾¾å­¦ä¹ ï¼‰A New Method of Region Embedding for Text Classification 

  

- [ ] ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰breaking_the_softmax_bottleneckï¼ša_high_rank_rnn_language_model

- [ ] ï¼ˆè¯­è¨€æ¨¡å‹ï¼‰Neural Language Modeling by Jointly Learning Syntax and Lexicon

- [ ] ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼ŒGANï¼‰MaskGAN: Better Text Generation via Filling in the _______ 

- [ ] ï¼ˆæ‘˜è¦ï¼‰A Deep Reinforced Model for Abstractive Summarization

- [ ] ï¼ˆLSTMçš„å¯è§£é‡Šæ€§ï¼Œæƒ…æ„Ÿåˆ†æï¼‰Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs

- [ ] ï¼ˆè¿ç§»å­¦ä¹ ï¼‰Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain Adaptation

- [ ] ï¼ˆå›¾å·ç§¯+self-attentionï¼‰Graph Attention Networks

- [ ] ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰Routing Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning 

- [ ] All-but-the-Top: Simple and Effective Postprocessing for Word Representations

- [ ] ï¼ï¼ï¼ˆä¸€ä¸ªé€»è¾‘è•´å«çš„æ•°æ®é›†ï¼‰Can Neural Networks Understand Logical Entailment? 

- [ ] Spherical CNNs

- [ ] ï¼ˆå¤šä»»åŠ¡å­¦ä¹ ï¼‰Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering

- [ ] ï¼ˆtransformer-basedï¼‰Non-Autoregressive Neural Machine Translation



# ï¼ˆå®Œæˆï¼‰ICLR2019

- [x] ï¼ˆé¢å‘ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿï¼‰Global-to-local Memory Pointer Networks for Task-Oriented Dialogue
- [x] æ–¹æ³•è¿‡äºæ•°å­¦ï¼ˆå¯¹è¯ç³»ç»Ÿçš„é—®é¢˜ç”Ÿæˆï¼‰Large-Scale Answerer in Questioner's Mind for Visual Dialog Question Generation
- [x] ï¼ˆå¯¹è¯ç³»ç»Ÿï¼‰Wizard of Wikipedia: Knowledge-Powered Conversational Agents
- [x] ï¼ˆNLUï¼‰GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding 
- [x] ï¼ˆå¤šæ–‡æ¡£QAï¼‰Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering
- [x] ï¼ˆå¼€æ”¾åŸŸQAï¼‰Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering
- [x] è®ºæ–‡å†™å¾—å¾ˆä¸æ¸…æ™°ï¼ˆQAï¼‰Generative Question Answering: Learning to Answer the Whole Question
- [x] ï¼ˆå¯¹è¯å¼QAï¼‰flowQA




- [ ] ï¼ˆå¯¹è¯ç³»ç»Ÿï¼‰Detecting Egregious Responses in Neural Sequence-to-sequence Models 
- [ ] ï¼ˆä¿¡æ¯æ£€ç´¢ï¼‰textTOvec: DEEP CONTEXTUALIZED NEURAL AUTOREGRESSIVE TOPIC MODELS OF LANGUAGE WITH DISTRIBUTED COMPOSITIONAL PRIOR

- [ ] Posterior Attention Models for Sequence to Sequence Learning

- [ ] Universal Transformers 
- [ ] Trellis Networks for Sequence Modeling
- [ ] 



# ï¼ˆå®Œæˆï¼‰ICLR2020

ç¬¬ä¸€æ¬¡é˜…è¯»åˆ—è¡¨ï¼š

- [x] ï¼ï¼ï¼**NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension**ã€æå‡ºä¸€ä¸ªæ¯”è¾ƒå¥½ç”¨çš„ answerability åˆ¤å®šæ¨¡å—ï¼Œå¯å€Ÿé‰´åˆ°qqåŒ¹é…ä»»åŠ¡ä¸Šã€‘

- [x] ï¼ï¼ï¼ï¼ˆå¯¹è¯ã€å¤–éƒ¨çŸ¥è¯†ï¼‰**Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue**ã€å¯¹è¯ä»»åŠ¡ï¼Œå¦‚ä½•æ›´å¥½åœ°é€‰æ‹©çŸ¥è¯†ã€‘

- [x] **TinyBERT: Distilling BERT for Natural Language Understanding**

- [x] **StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding**

- [x] **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators**ã€å¯¹berté¢„è®­ç»ƒä»»åŠ¡çš„æ”¹è¿›ã€‘

- [x] Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension

- [x] ï¼ˆå’Œæˆ‘çš„ç ”ç©¶å¥½åƒå¾ˆåƒï¼‰Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring

- [x] ï¼ˆå¯¹BERTçš„æ”¹è¿›ï¼‰ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

- [x] ï¼ˆå¯¹BERTçš„æ”¹è¿›ï¼‰RoBERTa: A Robustly Optimized BERT Pretraining Approach

- [x] ï¼ˆå¯¹è¯ç”Ÿæˆï¼Œä½èµ„æºï¼‰Low-Resource Knowledge-Grounded Dialogue Generation ã€è®ºå›ä¸Šå†™çš„å¾ˆæ¸…æ™°ï¼Œæ–¹æ³•ç®€æ˜ï¼Œä¸ç”¨å†çœ‹äº†ã€‘

- [x] Learning Robust Representations via Multi-View Information Bottleneck

  > åœ¨ç›‘ç£å­¦ä¹ çš„è®¾ç½®ä¸‹ï¼Œä¿¡æ¯ç“¶é¢ˆç†è®ºï¼ˆinformation bottleneck principleï¼‰å¯ä»¥åªä¿ç•™å¯¹é¢„æµ‹labelæœ‰ç”¨çš„ä¿¡æ¯ã€‚
  >
  > åœ¨æ— ç›‘ç£å­¦ä¹ çš„è®¾ç½®ä¸‹ï¼Œä¿¡æ¯æœ€å¤§åŒ–ï¼ˆInfoMaxï¼‰æ˜¯å°½é‡ä¿ç•™æ‰€æœ‰çš„ä¿¡æ¯ã€‚
  >
  > In this paper, we extend the information bottleneck method to the unsupervised multi-view setting. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniquesã€‚



ç¬¬äºŒæ¬¡é˜…è¯»åˆ—è¡¨ï¼š

- [x] **ï¼ï¼ï¼ï¼ï¼ˆå¤šè·³QAï¼‰Transformer-XH: Multi-hop question answering with eXtra Hop attention**
- [x] **ï¼ï¼ï¼ï¼ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation**
- [x] ï¼ˆQAã€æ¨ç†ï¼‰**Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering**
- [x] ï¼ï¼ï¼ï¼ˆBERTã€æœºå™¨ç¿»è¯‘ï¼‰**Incorporating BERT into Neural Machine Translation**ã€ä¸ç›´æ¥ä½¿ç”¨BERTåˆå§‹åŒ–æ¨¡å‹ï¼Œæœ¬æ–‡æ˜¯æŠŠBERTçš„è¾“å‡ºèå…¥åˆ°SEQ2SEQæ¨¡å‹çš„æ¯ä¸€å±‚ï¼Œç”¨åœ¨æ¯ä¸ªself-attentionä¹‹ä¸­ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºdrop-netâ€”â€”éšæœºä¸¢å¼ƒBERTè¡¨ç¤ºæˆ–æ¨¡å‹æœ¬èº«çš„è¡¨ç¤ºï¼Œå……åˆ†åˆ©ç”¨ä¸¤ä¸ªæ–¹é¢çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¿˜ä¸é”™ã€‚ã€‘
- [x] ï¼ï¼ï¼ï¼ˆäº’ä¿¡æ¯æœ€å¤§åŒ–ã€è¡¨è¾¾å­¦ä¹ ï¼‰**A Mutual Information Maximization Perspective of Language Representation Learning**



- [x] **ï¼ˆåŸºäºçŸ¥è¯†çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼‰Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model** ã€è®ºå›ä¸Šå†™çš„å¾ˆæ¸…æ¥šã€‘
- [ ] ï¼ˆæ–°çš„é˜…è¯»ç†è§£æ•°æ®é›†ï¼‰ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning
- [ ] ï¼ï¼ï¼ˆå¯¹transformerçš„ç†è§£ï¼‰Are Transformers universal approximators of sequence-to-sequence functions?
- [ ] ï¼ï¼ï¼ˆTransformerçš„å¯è§£é‡Šæ€§ï¼‰Robustness Verification for Transformers 
- [ ] ï¼ï¼ï¼ˆTransformerï¼‰On the Relationship between Self-Attention and Convolutional Layersã€å¥½åƒä¸å°‘æ•°å­¦ æ¨ç†ã€‘
- [x] **ï¼ˆé¢„è®­ç»ƒçš„æ£€ç´¢æ¨¡å‹ï¼‰GOING BEYOND TOKEN-LEVEL PRE-TRAINING FOR EMBEDDING-BASED LARGE-SCALE RETRIEVAL** ã€è®ºå›ä¸Šå†™çš„å¾ˆæ¸…æ¥šã€‘
- [ ] ï¼ˆå­¦ä¹ word embeddingï¼‰DeFINE: Deep Factorized Input Word Embeddings for Neural Sequence Modeling
- [ ] ï¼ˆMRCã€æ¨ç†ï¼‰**Neural Module Networks for Reasoning over Text** 

- [ ] ï¼ï¼Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models

- [ ] ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹æŠ—å­¦ä¹ ï¼‰Self-Adversarial Learning with Comparative Discrimination for Text Generation



# ï¼ˆå®Œæˆï¼‰ICLR2021

- [x] Universal Sentence Representations Learning with Conditional Masked Language Model 

  > A novel pre-training technique CMLM for unsupervised sentence representation learning on unlabeled corpora (either in monolingual and multilingual). CMLMé€šè¿‡ä»¥ç›¸é‚»å¥å­çš„ç¼–ç å‘é‡ä¸ºæ¡ä»¶ï¼Œå°†å¥å­è¡¨ç¤ºå­¦ä¹ æ•´åˆåˆ°MLMè®­ç»ƒä¸­ã€‚ 

- [x] Synthesizer: Rethinking Self-Attention for Transformer Models 

  > This paper seeks to develop a deeper understanding of the role that the dot product self-attention mechanism plays in Transformer models.
  >
  > This paper proposes SYNTHESIZER, a new model that learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products.

- [x] Multi-Head Attention: Collaborate Instead of Concatenate

  > As the multiple heads are inherently solving similar tasks, they can collaborate instead of being independent. Collaborative MHA introduces weight sharing across the key/query projections and decreases the number of parameters and FLOPS.

- [x] Deepening Hidden Representations from Pre-trained Language Models 

  > We argue that only taking single layerâ€™s output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the ï¬nal layer.

- [x] Contextual Knowledge Distillation for Transformer Compression

  > we present a new knowledge distillation strategy for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. WR is proposed to capture the knowledge of relationships between word representations and LTR deï¬nes how each word representation changes as it passes through the network layers.

- [ ] A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks

- [x] JAKET: Joint Pre-training of Knowledge Graph and Language Understanding

  > These Pre-trained language models models struggle to grasp world knowledge about entities and relations. we propose JAKET, a Joint pre-trAining framework for KnowledgE graph and Text. Under our framework, the knowledge module and language module both provide essential information for each other.

- [x] Optimizing Transformers with Approximate Computing for Faster, Smaller and more Accurate NLP Models 

  > We observe that for a given downstream task, some parts of the pre-trained Transformer are more signiï¬cant to obtain good accuracy, while other parts are less important or unimportant. In order to exploit this observation in a principled manner, we introduce a framework to introduce approximations while ï¬ne-tuning a pre-trained Transformer network, optimizing for either size, latency, or accuracy of the ï¬nal network.

- [ ] Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth 

  > In this paper, we study these core questions, through detailed analysis of a family of ResNet models with varying depths and widths trained on CIFAR-10, CIFAR-100 and ImageNet.

- [x] ã€MDRã€‘**Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval** 

  > We propose a simple and efï¬cient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.
  >
  > Our method iteratively encodes the question and previously retrieved documents as a query vector and retrieves the next relevant documents using efï¬cient MIPS methods.
  >
  > It improves DPR by sampling negatives from a memory bank (Wu et al., 2018) â€” in which the representations of negative candidates are frozen so more candidates can be stored.

- [x] Cluster-Former: Clustering-based Sparse Transformer for Question Answering

  > self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically
  >
  > we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. Cluster-Former consists of two types of encoding layer. The ï¬rst one (noted as SlidingWindow Layer) focuses on extracting local information within a sliding window. The other one (noted as Cluster-Former Layer) learns to encode global information beyond the initial chunked sequences. we stack these two types of layer interchangeably to capture both global and local context efï¬ciently.

- [x] Block Skim Transformer for Efficient Question Answering

  > We then propose Block Skim Transformer (BST), a plug-and-play module to the transformer-based models, to accelerate transformer-based models on QA tasks. 
  >
  > By handling the attention weight matrices as feature maps, the CNN-based Block Skim module extracts information from the attention mechanism to make a skim decision. With the predicted block mask, BST skips irrelevant context blocks, which do not enter subsequent layersâ€™ computation.

- [x] Distilling Knowledge from Reader to Retriever for Question Answering

  > In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever.
  >
  > Relevance-guided supervision for openqa with colbert

- [ ] When Do Curricula Work?

- [ ] Rethinking Embedding Coupling in Pre-trained Language Models

- [x] SEED: Self-supervised Distillation For Visual Representation

  > we ï¬nd that existing techniques like contrastive learning do not work well on small networks.
  >
  > Instead of directly learning from unlabeled data, we proposed SEED as a novel self-supervised learning paradigm, which learns representation by self-supervised distillation from a bigger SSL pre-trained model.

- [x] Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval 

  > In this paper, we ï¬rst theoretically show the learning bottleneck of dense retrieval is the domination of uninformative negatives sampled locally in batch, which yield diminishing gradient norms, large stochastic gradient variances, and slow learning convergence. 
  >
  > We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), a learning mechanism that selects hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.

- [ ] PMI-Masking: Principled masking of correlated spans

  > ä¸åªæ˜¯maskä¸€ä¸ªsubtokenï¼Œè€Œä¸”maskä¸€ä¸ªspanã€‚

- [ ] Decomposing Mutual Information for Representation Learning

- [ ] Semantic Hashing with Locality Sensitive Embeddings

  > we propose a method for learning continuous representations in which the optimized similarity is the angular similarity.  

- [x] Memory Representation in Transformer 

  > Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model.
  >
  > In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer.

- [x] Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model

  > In this paper, we argue that randomly sampled masks in MLM would lead to undesirably large gradient variance. To reduce the variance due to the sampling of masks, we proposed a fully-explored masking strategy, where a text sequence is divided into multiple non-overlapping segments. During training, all tokens in one segment are masked out, and the model is asked to predict them with the other segments as the context. It was demonstrated theoretically that the gradients obtained with such a novel masking strategy have a smaller variance, thus enabling more efï¬cient pre-training.
  >
  > ç°æœ‰çš„BERTç­‰æ¨¡å‹å¾€å¾€é‡‡ç”¨masked language modelè¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä½†æ˜¯å…¶å¾€å¾€é‡‡ç”¨éšæœºçš„æ–¹æ³•ç¡®å®šmaskçš„wordæˆ–è€…spanï¼›æœ¬æ–‡æå‡ºä¸åˆé€‚çš„maskä¼šå¯¼è‡´æ¢¯åº¦æ–¹å·®å˜å¤§ï¼Œå¹¶å½±å“æ¨¡å‹çš„æ•ˆæœï¼Œå¹¶åˆ†æåŸå› åœ¨äºåŒæ—¶maskçš„wordä¹‹é—´å…·æœ‰ä¸€å®šçš„ç›¸ä¼¼åº¦ï¼›æ•…æœ¬æ–‡æå‡ºä¸€ç§ç‰¹æ®Šçš„maskæœºåˆ¶ï¼Œå…¶è€ƒè™‘å¢å¤§è¢«maskçš„wordä¹‹é—´çš„å·®å¼‚ï¼Œè¿›è€Œå‰Šå¼±æ¢¯åº¦æ–¹å·®å¤§å¸¦æ¥çš„å½±å“ã€‚

- [x] Deep Retrieval: An End-to-End Structure Model for Large-Scale Recommendationsï¼ˆå¥½åƒæ˜¯æ²¡ä¸­ï¼Œåæ¥å‘äº†CIKM2021ï¼‰

  > we have proposed Deep Retrieval, an end-to-end learnable structure model for largescale recommender systems. DR uses an EM-style algorithm to learn the model parameters and paths of items jointly. Experiments have shown that DR performs well compared with brute-force baselines in two public recommendation datasets.
  
- [x] Contrastive Learning with Hard Negative Samples

  > We propose an importance sampling approach that åˆ©ç”¨éš¾è´Ÿæ ·æœ¬çš„åŒæ—¶è§£å†³ä¼ªè´Ÿæ ·æœ¬çš„é—®é¢˜ã€‚

- [x] Autoregressive entity retrieval

  > we propose GENRE, the ï¬rst system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context.

- [ ] 

# ICLR2022

- [x] Contrastive Pre-training for Zero-Shot Information Retrievalï¼ˆå¾—åˆ†4åˆ†ï¼Œè´¨é‡ä¸æ€ä¹ˆæ ·ï¼‰

  > We hypothesise that dense retrievalçš„æ³›åŒ–æ€§ä¸å¥½ stems from a mismatch between conventional NLP pretraining objectives and the retrieval task. 
  >
  > In this work, we consider contrastive learningï¼ˆrandom cropping + cross-batch Negï¼‰ as a more natural pre-training technique for retrieval and show that it leads to models that are competitive with BM25 on many domains or applications, even without training on supervised data.

- [x] In defense of dual-encoders for neural rankingï¼ˆå¾—åˆ†5åˆ†ï¼Œè´¨é‡ä¸€èˆ¬ï¼Œä½†æ˜¯åˆ†æå®éªŒæœ‰ä¸€å®šå‚è€ƒä»·å€¼ï¼‰

  > åˆ†æäº†cross-encoderå’Œdual-encoderé¢„æµ‹çš„scoreçš„åˆ†å¸ƒçš„ä¸åŒã€‚

- [ ] ~~Compositional Attentionï¼š Disentangling Search and Retrieval~~

  > æ”¹è¿›äº†Transformerçš„attentionæœºåˆ¶ï¼Œpropose a novel attention mechanism, called Compositional Attention, that replaces the standard head structureã€‚

- [ ] ~~Seq2Tok: Deep Sequence Tokenizer for Retrieval~~

  > This paper proposes Seq2Tok, a deep sequence tokenizer that converts continuous-valued sequences(æŒ‡çš„æ˜¯éŸ³é¢‘ï¼Œè§†é¢‘ç­‰) to discrete tokens that are easier to retrieve via sequence queries.

- [ ] ~~ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity~~ï¼ˆå¤šæ¨¡æ€çš„ï¼‰

- [ ] 



# CIKM2018

- [x] From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing
- [x] Retrieve-and-read: Multi-task learning of information retrieval and reading comprehension.
- [ ] 



# CIKM2019

- [ ] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Beyond word2vec: Distance-graph Tensor Factorization for Word and Document Embeddings
- [ ] ï¼ˆdialogueï¼Œresponseé€‰æ‹©ï¼Œæ•æ‰å¤šç²’åº¦ä¿¡æ¯ï¼‰Multi-Turn Response Selection in Retrieval-Based Chatbots with Iterated Attentive Convolution Matching Network
- [ ] ï¼ˆå¯¹è¯ï¼‰A Hybrid Retrieval-Generation Neural Conversation Model
- [ ] ï¼ˆå¯¹è¯ï¼‰Attentive History Selection for Conversational Question Answering
- [ ] ï¼ˆçŸ¥è¯†å›¾è°±QAã€ç”¨æˆ·åé¦ˆï¼‰An Interactive Mechanism to Improve Question Answering Systems via Feedback
- [ ] ï¼ˆæ„å»ºé€‚åˆè¯­ä¹‰æ£€ç´¢çš„ç¨ å¯†å‘é‡æœç´¢å¼•æ“ï¼‰GRIP: Multi-Store Capacity-Optimized High-Performance Nearest Neighbor Search for Vector Search Engine ï¼ˆä¸ç”¨çœ‹ï¼‰

------ çŸ­æ–‡

- [ ] ~~Machine Reading Comprehension: Matching and Orders~~
- [ ] ~~Cluster-Based Focused Retrieval~~
- [ ] ï¼ˆdialogueï¼Œresponseé€‰æ‹©ï¼‰Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] ï¼ˆlearn2rankçš„è®­ç»ƒç­–ç•¥ã€é²æ£’æ€§ï¼‰Analysis of Adaptive Training for Learning to Rank in Information Retrieval



# ï¼CIKM2020

- [x] ã€SMITHã€‘ï¼ˆé•¿æ–‡æœ¬åŒ¹é…ï¼‰Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching

  > ä¸¤å±‚æ¬¡çš„ dual Transformeræ¶æ„ï¼Œç¬¬ä¸€å±‚å»ºæ¨¡å¥å­å—å†…æ–‡æœ¬ï¼Œç¬¬äºŒå±‚ç”±å¥å­å—å»ºæ¨¡æ–‡æœ¬è¡¨è¾¾ã€‚
  >
  > æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒçš„æ–¹æ³•ï¼Œå…¶ä¸­é¢„è®­ç»ƒä»»åŠ¡åœ¨BERT MLMçš„åŸºç¡€ä¸ŠåŠ å…¥sentence block mask language modelã€‚

- [x] Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems

- [x] Dual Head-wise Coattention Network for Machine Comprehension with Multiple-Choice Questions

- [ ] Robust Retrievability based Document Selection for Relevance Feedback with Automatically Generated Query Variantsï¼ˆæ‰¾ä¸åˆ°ï¼‰

- [ ] Transformer Model Compression via Joint Structured Pruning and Knowledge Distillationï¼ˆæ‰¾ä¸åˆ°ï¼‰

- [ ] TABLE: A Task-Adaptive BERT-based ListwisE Ranking Model for Document Retrievalï¼ˆæ‰¾ä¸åˆ°ï¼‰

- [x] CGTR: Convolution Graph Topology Representation for Document Ranking

  > we propose a GCN based method to encode global structure in contextual embeddings to improve ad-hoc document retrieval.

- [x] ã€DMINã€‘Deep Multi-Interest Network for Click-through Rate Prediction

  > Distant Supervision in BERT-based Adhoc Document Retrieval
  >
  > Multi-Interest Network with Dynamic Routing for Recommendation at Tmall

- [x] Distant supervision in BERT-based Ad-hoc Document Retrieval

  > We find that direct transfer of relevance labels from documents to passage introduces label noise and it can not leverage information from large amount of unlabelled documents.
  >
  > We show that our distantly supervised models(Qa-DocRank and QA-Full-DOCRANK) perform better than the baselines. Extracting relevant passages from the unlabelled documents (QA-Full-DOCRANK) helps in solving the scarcity problem.
  >
  > Qa-DocRankï¼šå…ˆç”¨ä¸€ä¸ªQA_modelå¯¹passageè¿›è¡Œæ‰“åˆ†ï¼Œæ ¹æ®è®¾å®šé˜ˆå€¼å¾—åˆ°query-passageçš„ç›¸å…³æ€§labelï¼Œå†ç”¨è¿™äº›labeled pairå¾®è°ƒBERTæ¨¡å‹ã€‚æ¨æ–­æ—¶æ˜¯å…ˆå¾—åˆ°æ¯ä¸ªpassageçš„å¾—åˆ†ï¼Œå†æ ¹æ®FirstP/MaxP/SumP/AvgPç­‰ç­–ç•¥å¾—åˆ°æœ€ç»ˆå¾—åˆ†ã€‚
  >
  > QA-Full-DOCRANKï¼šåœ¨Qa-DocRankçš„åŸºç¡€ä¸Šï¼Œç”¨ä¸€ä¸ªQA_modelå¯¹æ— æ ‡ç­¾çš„documentçš„passageè¿›è¡Œæ‰“åˆ†ï¼Œæ ¹æ®è®¾å®šé˜ˆå€¼å¾—åˆ°query-passageçš„ç›¸å…³æ€§labelï¼Œå†ç”¨è¿™äº›labeled pairå¾®è°ƒBERTæ¨¡å‹ã€‚

# CIKM2021

- [x] Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance

  > è”åˆä¼˜åŒ–dense retrieverå’ŒOPQç´¢å¼•ï¼š1ï¼‰ranking-oriented loss, 2ï¼‰PQ centroid optimization, 3ï¼‰end-to-end negative samplingã€‚

- [x] ã€DSAHã€‘**Deep Self-Adaptive Hashing for Image Retrieval**

  > we propose a novel deep unsupervised hashing method DSAH to yield binary codesã€‚
  >
  > DSAH provides two innovative designs: 1ï¼‰we adopt AND technique to refine the pre-computed similarity matrix with the learned representation and adaptively capture the implicit semantic structure behind the data distribution. 2ï¼‰we employ PIC to distinguish the different importance of data pairs and assign an adjustable weight to each data pair.
  >
  > ï¼ˆ1ï¼‰æ— ç›‘ç£ï¼ŒæŒ–æ˜å¹¶åˆ©ç”¨å¯èƒ½çš„æ­£æ ·æœ¬ã€‚ï¼ˆ2ï¼‰æ¯ä¸ªæ ·æœ¬è®¡ç®—lossæ—¶é‡‡ç”¨ä¸åŒçš„weight

- [x] Query Embedding Pruning for Dense Retrieval

  > We proposed query embedding pruning, and demonstrated that a subset of the original query embeddings can be used for effective retrieval while reducing the number of document requiring to be exactly scored.
  > æ”¹è¿›ColBERTã€‚

- [x] On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval

  > åŠ¨æœºåŒQuery Embedding Pruning for Dense Retrievalã€‚
  >
  > we investigate the use of ANN scores for ranking the candidate documents, in order to decrease the number of candidate documents being fully scored. æå‡ºå››ç§ç­–ç•¥ï¼š1ï¼‰Kprimeï¼Œ2ï¼‰Countï¼Œ3ï¼‰Sum Simï¼Œ4ï¼‰Max Simã€‚

- [ ] S~~AUCE: Truncated Sparse Document Signature Bit-Vectors for Fast Web-Scale Corpus Expansion~~ï¼ˆä¸ç›¸å…³ï¼‰

- [x] ã€HARPã€‘Pre-training for Ad-hoc Retrieval: Hyperlink is Also You Need

  > in this work, we propose to leverage the correlations and supervised signals brought by hyperlinks and anchor texts, and design four novel pretraining objectives to learn the correlations of query and documents for ad-hoc retrieval. å››ä¸ªæ–°çš„é¢„è®­ç»ƒç›®æ ‡å’ŒMLMç›®æ ‡ä¸€èµ·è¿›è¡Œé¢„è®­ç»ƒã€‚

- [x] Binary Code based Hash Embedding for Web-scale Applications

  > we propose a binary code based hash embedding method which allows the size of the embedding table to be reduced in arbitrary scale without compromising too much performance.

- [ ] Learning Sparse Binary Code for Maximum Inner Product Search

- [x] Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback

  > ANCE-PRF uses a BERT encoder that consumes the query and the top retrieved documents from a dense retrieval model, ANCE, and it learns to produce better query embeddings directly from relevance labels.

- [x] Query-driven Segment Selection for Ranking Long Documents

  > we propose query-driven segment selection from long documents to build training data. The segment selector provides relevant samples with more accurate labels and non-relevant samples which are harder to be predicted.
  >
  > we use an iterative approach that includes complete and incomplete data, which is similar to the EM algorithm.

- [x] **Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse Retrieval Strategy Selection**

  > we investigate strategies for selecting a first-stage retrieval strategy based on the query. Selecting an appropriate strategy provides a trade-off between efficiency and effectiveness.

- [x] BERT-QPP: Contextualized Pre-trained Transformers for Query Performance Prediction

  > Our work proposes to fine-tune a pre-trained transformer to learn to estimate the performance of a query in light of the set of documents that have been retrieved for that query.

- [x] **DML: Dynamic Multi-Granularity Learning for BERT-Based Document Reranking**

  > we focus on BERT-based document reranking and propose Dynamic Multi-Granularity Learning (DML). 1. By introducing Gaussian distribution into traditional loss functions, the weights of different documents can change dynamically according to the prediction probability to avoid the impact of unlabeled positive documents. 2. we optimize models from multiple granularity, i.e., documentgranularity and instance-granularity.

- [x] Self-supervised Fine-tuning for Efficient Passage Re-ranking

  > The MLM and ranking tasks are jointly executed during the fine-tuning stage as multitask learning.è¿™æ ·çš„å¾®è°ƒæ–¹å¼ä½¿ç”¨æ¯”è¾ƒå°‘é‡çš„æ ‡æ³¨æ•°æ®å³å¯ï¼Œå› ä¸ºMLMä»»åŠ¡çš„å¼•å…¥èµ·åˆ°äº†æ•°æ®å¢å¼ºçš„ä½œç”¨ã€‚
  > ä½†æ˜¯éœ€è¦ä½¿ç”¨weighted-masking methodï¼šThe importance scores for the terms and sentences in the passages were used to select the mask tokens and core sentences in the model.

- [ ] MedRetriever: Target-Driven Interpretable Health Risk Prediction via Retrieving Unstructured Medical Textï¼ˆæ¨¡å‹é¢„æµ‹ç»“æœçš„å¯è§£é‡Šæ€§ï¼Ÿï¼Ÿï¼‰

- [ ] Adaptive Posterior Knowledge Selection for Improving Knowledge-Grounded Dialogue Generationï¼ˆçŸ¥è¯†ååŠ©çš„å¯¹è¯ç”Ÿæˆï¼Ÿï¼Ÿï¼Ÿï¼‰

- [ ] Evaluating Relevance Judgments with Pairwise Discriminative Power   ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ

- [ ] ï¼ï¼HORNET: Enriching Pre-trained Language Representations with Heterogeneous Knowledge Sources ï¼ˆçŸ¥è¯†å¢å¼ºçš„é¢„è®­ç»ƒï¼‰

- [ ] ï¼ï¼ï¼K-AID: Enhancing Pre-trained Language Models with Domain Knowledge for Question Answeringï¼ˆçŸ¥è¯†å¢å¼ºçš„é¢„è®­ç»ƒï¼‰

- [ ] ï¼ï¼ï¼Self-supervised Learning for Large-scale Item Recommendationsï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰

- [x] Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching



# ï¼WSDM2018

- [x] ï¼ˆåŒ¹é…ï¼Œè¿ç§»å­¦ä¹ ï¼‰Modelling Domain Relationships for Transfer Learning on Chatbot-based Question Answering Systems
- [x] ï¼ï¼ï¼ˆConvKNRMï¼‰Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search
- [x] ï¼ˆHyperQAï¼‰Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering
- [ ] ï¼ˆXXXï¼‰Query Driven Algorithm Selection in Early Stage Retrieval
- [x] ï¼ï¼ï¼ˆCo-PACRRï¼‰ERM-PACRR: A Neural IR model with Enhanced Relevance Matching
- [x] Neural Ranking Models with Multiple Document Fields



# ï¼WSDM2019

- [ ] ï¼ˆxxxx,rankingï¼‰WassRank: Listwise Document Ranking Using Optimal Transport Theory
- [ ] ï¼ˆxxxx,rankingï¼‰Joint Optimization of Cascade Ranking Models
- [x] ï¼ˆåŒ¹é…ï¼Œè¿ç§»å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ ï¼‰Learning to Selectively Transfer: Reinforced Transfer Learning for Deep Text Matching
- [x] ï¼ˆQAï¼‰Learning to Transform, Combine, and Reason in Open-Domain Question Answering
- [x] ï¼ˆå¯¹è¯ï¼‰Multi-Representation Fusion Network for Multi-Turn Response Selection in Retrieval-Based Chatbots
- [ ] ï¼ˆXXX,è¡¨è¾¾å­¦ä¹ ï¼‰SHNE: Representation Learning for Semantic-Associated Heterogeneous Networks.
- [x] ï¼ˆç”µå•†ï¼‰Weakly Supervised Co-Training Query Rewriting and Semantic Matching for E-Commerce
- [x] ï¼ˆæ¨èï¼‰Gated Attentive-Autoencoder for Content-Aware Recommendation
- [x] ï¼ˆæ¨èï¼‰Product-Aware Answer Generation in E-Commerce Question-Answering



# ï¼WSDM2020

- [x] ã€TMKDã€‘Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System

  > we propose a novel Two-stage Multi-teacher Knowledge Distillation approach for model compression:1) a Q&A multi-teacher distillation task is proposed for student model pre-training,2) a multi-teacher paradigm is designed to jointly learn from multiple teacher models (m-o-1) for more generalized knowledge distillation on downstream specific tasks.



# WSDM2022

- [ ] ~~Web Search via an Efficient and Effective Brain-Machine Interface~~

  > we build an efficient and effective communication system between human beings and search engines based on electroencephalogram (EEG) signals, called Brain Machine Search Interface (BMSI) system.

- [x] Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval

  > we present RepCONC, which learns discrete representations by modeling quantization as constrained clustering in the joint learning process.

- [ ] 

# 





#ECIR2020 

Patch-based Identification of Lexical Semantic Relations

Relevance Ranking based on Query-Aware Context Analysis

**BERT for Evidence Retrieval and Claim Verification**



# ICTIR2020

**Approximate Nearest Neighbor Search and Lightweight Dense Vector Reranking in Multi-Stage Retrieval Architectures**



# ICTIR2021

- [x] More Robust Dense Retrieval with Contrastive Dual Learning

  > document retrieval lossåªèƒ½ä¿è¯document embeddingåˆ†å¸ƒçš„â€œalignmentâ€ and â€œuniformityâ€ï¼Œä½†query embeddingçš„åˆ†å¸ƒä¸èƒ½ä¿è¯ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªdual learning taskã€‚

- [x] ã€ColBERT-PRFã€‘Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval

  > ColBERT-PRF approach extracts representative feedback embeddings using a clustering technique. It then identifies discriminative embeddings among the representative embeddings and appends them to the query representation.

- [x] A Modern Perspective on Query Likelihood with Deep Generative Retrieval Models

  > We adopt several current neural generative models in our framework and introduce a novel generative ranker (T-PGN), which combines the encoding capacity of Transformers with the Pointer Generator Network model.





# ICML2018

# ICML2019





# IJCAI2018

# IJCAI2019





# NIPS2018

# NIPS2019

# NIPS2020

- [ ] Coresets for Robust Training of Deep Neural Networks against Noisy Labels

- [ ] Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples

- [ ] Part-dependent Label Noise: Towards Instance-dependent Label Noise

- [ ] Exploiting weakly supervised visual patterns to learn from partial annotations

- [ ] Curriculum Learning by Dynamic Instance Hardness

- [ ] Supervised Contrastive Learning

- [ ] Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning

- [ ] Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning

- [ ] **A Variational Approach for Learning from Positive and Unlabeled Data**

- [ ] **Learning from Positive and Unlabeled Data with Arbitrary Positive Shift**

- [ ] Robust Correction of Sampling Bias using Cumulative Distribution Functions

- [ ] Early-Learning Regularization Prevents Memorization of Noisy Labels

- [ ] BERT Loses Patience: Fast and Robust Inference with Early Exit

- [ ] ã€RAGã€‘Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

  > During the training phase, RAG takes an input and encodes it to a high-dimensional vector. This vector then gets used as a query to select the most relevant passages from an external database using Maximum Inner Product Search (MIPS) [Johnson et al., 2017, Guo et al., 2020]. Afterward, The input and the selected set of documents get fed into a generator that produces the ï¬nal answer.
  >
  > All three components are initialized with pre-trained transformers. 

- [ ] **Partial Optimal Transport with applications on Positive-Unlabeled Learning**

- [ ] Big Bird: Transformers for Longer Sequences

- [ ] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing

- [ ] Pre-training via Paraphrasing

- [ ] DynaBERT: Dynamic BERT with Adaptive Width and Depth

- [ ] Cross-lingual Retrieval for Iterative Self-Supervised Training

- [x] CogLTX: Applying BERT to Long Texts

- [ ] Hard Negative Mixing for Contrastive Learning

- [x] Debiased Contrastive Learning

- [ ] 





# AAAI2018

# AAAI2019

# AAAI2020

- [x] ã€NLIã€PIã€‘Multi-level Head-wise Match and Aggregation in Transformer for Textual Sequence Matching

  > Multi-level Head-wise Match.

- [x] ã€QAã€‘Knowledge and Cross-Pair Pattern Guided Semantic Matching for Question Answering

  > we propose a novel system named KCG for AS, which considers both knowledge and pattern conditions. KCG applies both intra-pair and cross-pair learning. cross-pair learningä½¿ç”¨GCNç½‘ç»œï¼Œintra-pair learningä½¿ç”¨cross attentionã€‚æœ€ç»ˆæ˜¯cross-pair å’Œ intra-pairå¾—åˆ†çš„åŠ æƒå’Œã€‚
  >
  > WikiQAã€TrecQA

- [ ] Scalable attentive sentence-pair modeling via distilled sentence embedding

 

# AAAI2021

æ£€ç´¢

- [ ] ï¼ï¼Weakly Supervised Deep Hyperspherical Quantization for Image Retrievalï¼ˆæ‰¾ä¸åˆ°ï¼‰

- [ ] ï¼ï¼Dual Compositional Learning in Interactive Image Retrievalï¼ˆæ‰¾ä¸åˆ°ï¼‰

- [x] Learning to Truncate Ranked Lists for Information Retrieval

- [x] HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions

  > In this paper, we propose the HopRetriever to collect reasoning evidence over Wikipedia for multi-hop question answering. Both the structured knowledge indicated by hyperlinks and the unstructured knowledge presented as introductory documents in Wikipedia, are involved and leveraged together in HopRetriever to help the evidence collection.

è’¸é¦ï¼š

- [x] LRC-BERT: Latent-Representation Contrastive Knowledge Distillation for Natural Language Understanding

  > 1ï¼‰Firstly, the COS-NCE based on contrastive learning is proposed to distill the output of the intermediate layer from the angle distance, which is not considered by the existing knowledge distillation methods. 2ï¼‰Then, we introduce a gradient perturbation-based training architecture in the training phase to increase the robustness of the model, which is the ï¬rst attempt in knowledge distillation. 3ï¼‰Finally, to better capture the distribution characteristics of the intermediate layer, we design a two-stage training method for the total distillation loss.

- [x] ALP-KD: Attention-Based Layer Projection for Knowledge Distillation

  > we discussed the importance of distilling from intermediate layers and proposed an attention-based technique to combine teacher layers without skipping them.

- [x] Reinforced Multi-Teacher Selection for Knowledge Distillation

  > We propose a novel RL-based approach, which dynamically assigns weights to teacher models at instance level to better adapt to the strengths of teacher models as well as the capability of student models.

é¢„è®­ç»ƒ

- [ ] TransTailor: Pruning the Pre-Trained Model for Improved Transfer Learning

- [ ] ï¼ï¼ ~~U-BERT: Pre-Training User Representations for Improved Recommendation~~

- [x] ã€UEDã€‘ **A Unified Pretraining Framework for Passage Ranking and Expansion**

  > we propose a general pretraining framework to enhance both the retrieval and re-ranking stages with a uniï¬ed encoder-decoder network.

- [ ] ~~ï¼ï¼**Towards Semantics-Enhanced Pre-Training: Can Lexicon Definitions Help Learning Sentence Meanings~~**

å¯è§£é‡Šæ€§ï¼š

- [ ] The Heads Hypothesis: A Unifying Statistical Approach towards Understanding Multi-Headed Attention in BERT

- [x] Self-Attention Attribution: Interpreting Information Interactions Inside Transformer

  > use the attribution scores to derive the interaction graphs, which visualizes the information ï¬‚ow of Transformer. Moreover, we use the proposed method to identify the most important attention heads, which leads to a new head pruning algorithm. Finally, we show that ATTATTR can also be employed to construct adversarial triggers to implement non-targeted attacks.

è¡¨ç¤ºå­¦ä¹ ï¼š

- [ ] LRSC: Learning Representations for Subspace Clustering
- [ ] Uncertainty-Aware Multi-View Representation Learning
- [ ] Multi-View information-Bottleneck Representation Learning

å…¶ä»–ï¼š

- [ ] Robust Model Compression Using Deep Hypotheses
- [ ] Evidence Inference Networks for Interpretable Claim Verification
- [ ] Retrospective Reader for Machine Reading Comprehension
- [ ] Natural Language Inference in Context - Investigating Contextual Reasoning over Long Texts
- [ ] Distributed Ranking with Communications: Approximation Analysis and Applications
- [ ] Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders
- [ ] ã€matchingã€‘Learning an Effective Context-Response Matching Model with Self-Supervised Tasks for Retrieval-Based Dialogues
- [ ] ã€matchingã€‘Making the Relation Matters: Relation of Relation Learning Network for Sentence Semantic Matching
- [ ] ã€matchingã€‘Graph-Based Tri-Attention Network for Answer Ranking in CQA
- [ ]  ã€matchingã€‘A Few Queries Go a Long Way: Information-Distortion Tradeoffs in Matching
- [ ] ã€matchingã€‘ A Graph-Based Relevance Matching Model for Ad-Hoc Retrieval
- [ ] Semantics-Aware Inferential Network for Natural Language Understanding



# NAACL2018

- [x] Contextualized word representations for reading comprehension.ï¼ˆNAACL2018ï¼‰

# NAACL2019





# WWW2019

- [ ] ï¼ˆIRï¼‰Adversarial Sampling and Training for Semi-Supervised Information Retrieval
- [ ] What We Vote for? Answer Selection from User Expertise View in Community Question Answering
- [ ] ï¼ˆåŒ¹é…æ¨¡å‹ï¼‰Semantic Text Matching for Long-Form Documents
- [ ] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Learning Graph Pooling and Hybrid Convolutional Operations for Text Representations
- [ ] ï¼ˆé—®ç­”ï¼‰HAR: A Hierarchical Attention Retrieval Model for Healthcare Question Answering
- [ ] ï¼ˆåŒ¹é…æ¨¡å‹ï¼‰Learning Fast Matching Models from Weak Annotations
- [ ] ï¼ˆè¡¨ç¤ºå­¦ä¹ ï¼‰Semantic Hilbert Space for Text Representation Learning
- [ ] ï¼ˆé—®ç­”ï¼‰Focusing Attention Network for Answer Ranking
- [ ] ï¼ˆé—®é¢˜ç”Ÿæˆï¼‰Inferring Search Queries from Web Documents via a Graph-Augmented Sequence to Attention Network



# WWW2020

- [x] Leveraging Passage-level Cumulative Gain for Document Ranking

  > æ‰¾äººæ ‡ä¸€æ‰¹PCGæ•°æ®ï¼Œç„¶åæå‡ºä¸€ä¸ªæ¨¡å‹é¢„æµ‹ä¸€ä¸ªdocçš„PCGåºåˆ—ï¼Œè¿›è€Œå°±å¾—åˆ°äº†docçš„å…¨å±€å¾—åˆ†ã€‚we proposed a BERT-based sequential model PCGM for modeling PCG, which uses an LSTM after a pre-trained BERT with a gain embedding layer and a gain mask mechanism.

- [x] Selective Weak Supervision for Neural Information Retrieval

  > ReInfoSelect leverages the widely available anchor data to weakly supervise neural rankers.
  >
  > To handle the noises in anchor data, ReInfoSelect uses policy gradient to connect the demandâ€”the needs of training signals from neural rankerâ€”and the supplyanchor-document pairs, to select more effective weak supervision signals.



# WWW2021

- [x] Generalizing Discriminative Retrieval Models using Generative Tasks

  > ç”¨ç”Ÿæˆä»»åŠ¡ä½œä¸ºè¾…åŠ©ä»»åŠ¡è®­ç»ƒåˆ¤åˆ«æ£€ç´¢æ¨¡å‹ï¼Œä¸ä»…å¯ä»¥ä½¿å¾—æ£€ç´¢æ€§èƒ½æ›´å¥½ï¼Œè¿˜èƒ½æé«˜æ¨¡å‹çš„è¿ç§»æ€§ã€‚

- [ ] 

  



# SIGKDD

# ICDM

# COLING

# CoNLL







# æ¨¡å‹å‹ç¼©

##### å‰ªæ

- [ ] Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.ï¼ˆ2019ï¼‰
- [ ] Are sixteen heads really better than one?ï¼ˆNIPS2019ï¼‰

##### Quantization æ–¹æ³•

- [ ] Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.ï¼ˆICLR2015ï¼‰
- [ ] Deep learning with limited numerical precision.ï¼ˆICML2015ï¼‰
- [ ] Training and inference with integers in deep neural networks.ï¼ˆICLR2018ï¼‰
- [ ] Efficient Weights Quantization of Convolutional Neural Networks Using Kernel Density Estimation based Non-uniform Quantizer.ï¼ˆ2019ï¼‰

##### binarized networks

- [ ] Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1.ï¼ˆ2016ï¼‰

##### è’¸é¦

- [ ] ã€çŸ¥è¯†è’¸é¦ã€‘Model compressionï¼ˆ2006ï¼ŒBuciluaï¼Œé¦–æ¬¡æå‡ºé€šè¿‡çŸ¥è¯†è’¸é¦å‹ç¼©æ¨¡å‹çš„æ€æƒ³ï¼Œä½†æ˜¯æ²¡æœ‰å®é™…çš„å·¥ä½œé˜è¿°ï¼‰
- [ ] ã€çŸ¥è¯†è’¸é¦ã€‘Do deep nets really need to be deep?ï¼ˆ2014ï¼‰
- [ ] ã€çŸ¥è¯†è’¸é¦ã€‘Distilling the knowledge in a neural network.ï¼ˆ2015ï¼ŒHintonï¼Œç¬¬ä¸€æ¬¡æ­£å¼å®šä¹‰Distillationï¼Œå¹¶æå‡ºç›¸åº”çš„è®­ç»ƒæ–¹æ³•ï¼‰

--------------------------------------------------

- [ ] ã€ç»¼è¿°ã€‘Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks.ï¼ˆ2020ï¼‰
- [ ] ã€ç»¼è¿°ã€‘Knowledge Distillation: A Survey. ï¼ˆ2020ï¼‰

---------------------

- [x] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.ï¼ˆNIPS2019ï¼Œå°†BERT-baseé€šè¿‡é¢„è®­ç»ƒé˜¶æ®µè’¸é¦åˆ°6å±‚ã€‚loss=MLM + é¦–å±‚embeddingçš„cosine embedding loss + é¢„æµ‹å±‚è¾“å‡ºç±»åˆ«åˆ†å¸ƒè’¸é¦lossã€‚å®éªŒæ˜¯GLUEã€SQuADï¼‰

- [x] TinyBERT: Distilling BERT for Natural Language Understanding.ï¼ˆ2019ï¼Œå°†BERT-baseé€šè¿‡é¢„è®­ç»ƒé˜¶æ®µè’¸é¦åˆ°å°‘å±‚ã€‚loss=embeddingå±‚è’¸é¦ + trm attention matrixè’¸é¦loss + trm hidden stateè’¸é¦loss + é¢„æµ‹å±‚è¾“å‡ºç±»åˆ«åˆ†å¸ƒè’¸é¦lossã€‚å®éªŒæ˜¯GLUEï¼‰

- [ ] Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformersï¼ˆ2020ï¼‰

- [ ] Well-read students learn better: The impact of student initialization on knowledge distillation.ï¼ˆ2019ï¼‰

- [x] ã€è’¸é¦æˆdual encoderã€‘**Distilling Task-Specific Knowledge from BERT into Simple Neural Networks**.ï¼ˆ2019ï¼Œdistilling the **BERT** model into a two-tower model with **one-layer BiLSTM** as encodersã€‚loss=é¢„æµ‹å±‚è¾“å‡ºè’¸é¦loss + ground truth lossã€‚ä½¿ç”¨æ•°æ®å¢å¼ºå¾—åˆ°æ— æ ‡ç­¾æ•°æ®é›†è¿›ä¸€æ­¥å¸®åŠ©çŸ¥è¯†è’¸é¦ã€‚å®éªŒæ˜¯SST-2ã€QQPã€MNLIï¼‰

- [x] ã€è’¸é¦æˆdual encoderã€‘**Scalable attentive sentence-pair modeling via distilled sentence embedding**.ï¼ˆAAAI2020ï¼ŒGiven a cross-attentive teacher model (e.g. a fine-tuned **BERT-large**), we train a sentence embedding based student modelï¼ˆ**BERT-large**ï¼‰ to reconstruct the sentence-pair scores obtained by the teacher modelã€‚loss=é¢„æµ‹å±‚è¾“å‡ºè’¸é¦loss + ground truth lossã€‚å®éªŒæ˜¯GLUEä¸­çš„5ä¸ªsentence-pairä»»åŠ¡ï¼‰

- [x] ã€TMKDã€‘Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering Systemï¼ˆWSDM2020ï¼Œé¢„è®­ç»ƒè’¸é¦+å¾®è°ƒè’¸é¦ï¼Œm-to-1æ¨¡å¼ï¼Œstudentæ¨¡å‹æ˜¯BiLSTMæˆ–è€…BERTã€‚å®éªŒæ˜¯QAã€MNLIã€SNLIã€RTEï¼‰  

- [x] ã€è’¸é¦æˆdual encoderã€‘**Distilling Knowledge for fast retrieval-based chat-bots**ï¼ˆSIGIR2020ã€‚student modelæ˜¯BiLSTMæˆ–è€…BERTã€‚loss=é¢„æµ‹å±‚è¾“å‡ºè’¸é¦loss + ground truth lossã€‚ï¼‰

- [x] ã€è’¸é¦æˆdual encoderã€‘**DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling**ï¼ˆ2020ï¼‰

- [x] Understanding bert rankers under distillation.

  > åœ¨MSMARCOä¸Šç”±bert rerankerè’¸é¦ä¸€ä¸ªä½å±‚çš„bert rerankerã€‚æå‡ºLM distilledå’ŒRanker Distilledã€‚

- [ ] Distilling Knowledge Learned in BERT for Text Generationï¼ˆACL2020ï¼‰

- [ ] TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processingï¼ˆACL2020ï¼‰

- [ ] DE-RRD: A Knowledge Distillation Framework for Recommender System

- [x] DistilSum: Distilling the Knowledge for Extractive Summarization

- [ ] Distilling Structured Knowledge into Embeddings for Explainable and Accurate Recommendationï¼ˆ

- [ ] ~~Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation~~

  > ä½œè€…æå‡ºäº†åŸºäºçŸ¥è¯†è’¸é¦çš„å¤šè¯­è¨€sentence embeddingè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥**å°†å·²æœ‰çš„å•è¯­è¨€æ¨¡å‹æ‰©å±•ä¸ºå¤šè¯­è¨€æ¨¡å‹**ã€‚

- [x] *Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data*

  > Our method involves two phases of distillation. The ï¬rst phase is at the data level as it constructs (i.e., distills) postresponse pairs by matching sentences retrieved from a set of unpaired data. The second phase is at the model-level as it distills a teacher model using the augmented data.

- [ ] *Lifelong Language Knowledge Distillation*

- [ ] Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers. 

  > we proposed a novel model to distill from intermediate layers as well as ï¬nal predictions.
  
- [ ] ~~Contrastive Representation Distillation~~

- [ ] Patient knowledge distillation for bert model compressionï¼ˆsun, å±‚ä¸å±‚çš„è’¸é¦ï¼‰

  > <img src="../images/image-20201221112254679.png" alt="image-20201221112254679" style="zoom:50%;" />
  >
  > <img src="../images/image-20201221112327072.png" alt="image-20201221112327072" style="zoom:33%;" />
  >
  > <img src="../images/image-20201221112349177.png" alt="image-20201221112349177" style="zoom:33%;" />
  >
  > <img src="../images/image-20201221112405716.png" alt="image-20201221112405716" style="zoom:33%;" />



# é•¿æœ¬æ–‡

- [x] ETC: Encoding Long and Structured Inputs in Transformers

- [ ] Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization.

- [ ] Generating long sequences with sparse transformers.

- [ ] Big bird: Transformers for longer sequences.

  

å¤„ç†é•¿æ–‡æœ¬çš„ä¸€èˆ¬æ–¹å¼ï¼š

1. sliding window

- [x] Multi-passage bert: A globally normalized bert model for open-domain question answering.

2. æ”¹è¿›Transformerç»“æ„

- [ ] Reformer: The efï¬cient transformer.
- [ ] Compressive transformers for long-range sequence modelling.
- [ ] Transformer-xl: Attentive language models beyond a ï¬xed-length context.
- [ ] Adaptive attention span in transformers.
- [ ] Longformer: The long-document transformer.

3. å‹ç¼©è¾“å…¥æ–‡æœ¬

- [x] CogLTX: Applying BERT to Long Texts

- [x] Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching

- [ ] Text summarization with pretrained encoders.ï¼ˆæ–‡æœ¬æ‘˜è¦çš„SOTAæ–¹æ³•ï¼‰

  

# bias é—®é¢˜

- [x] Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systemsï¼ˆæ¨èç³»ç»Ÿï¼‰

- [x] Debiased Contrastive Learning

  

- [x] Learning to rank with **selection bias** in **personal search**ï¼ˆ2016ï¼Œæå‡ºInverse propensity weightingï¼Œä½¿ç”¨result randomizationè¿›è¡Œexamination propensityï¼‰

- [x] Unbiased learning-to-rank with biased feedback.ï¼ˆ2017ï¼Œæå‡ºInverse propensity weightingï¼Œä½¿ç”¨swap-interventionè¿›è¡Œexamination propensityï¼‰
- [x] Unbiased Learning to Rank with Unbiased Propensity Estimationï¼ˆ2018ï¼Œè‰¾ï¼Œéƒ­ï¼Œä½¿ç”¨å¯¹å¶å­¦ä¹ ç®—æ³•è¿›è¡Œexamination propensityï¼‰
- [x] Position Bias Estimation for Unbiased Learning to Rank in Personal Search.ï¼ˆ2018ï¼Œæå‡ºRegression EMï¼‰
- [ ] Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithmï¼ˆ2019ï¼Œa variation of DLA with pairwise ranking lossesï¼‰
- [x] Unbiased Learning to Rank: Online or Offline?ï¼ˆ2020ï¼Œè‰¾ï¼‰





























